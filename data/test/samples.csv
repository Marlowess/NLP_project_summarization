id,text,gold
https://openreview.net/forum?id=r1GKzP5xx,"I think this build upon previous works, in the attempt of doing something similar to batch norm specific for RNNs. To me the experiments are not yet very convincing, I think is not clear this works better than e.g. Layer Norm or not significantly so. I'm not convinced on how significant the speed up is either, I can appreciate is faster, but it doesn't feel like order of magnitude faster. The theoretical analysis also doesn't provide any new insights. ----------------All in all I think is good incremental work, but maybe is not yet significant enough for ICLR.","Paper proposes a modification of batch normalization. After the revisions the paper is a much better read. However it still needs more diverse experiments to show the success of the method.    Pros:  - interesting idea with interesting analysis of the gradient norms  - claims to need less computation    Cons:  - Experiments are not very convincing and only focus on only a small set of lm tasks.  - The argument for computation gain is not convincing and no real experimental evidence is presented. The case is made that in speech domain, with long sequences this should help, but it is not supported.    With more experimental evidence the paper should be a nice contribution."
https://openreview.net/forum?id=rJe-Pr9le,"The term strategy is a bit ambiguous. Could you please explain more in formal terms what is strategy?--------Is r the discounted Return at time t, or the reward at time t?--------Could the author compare the method to TD learning?--------The paper is vague and using many RL terms with different meanings without clarifying those diversions.--------""So, the output for a given state-actions pair is always same"". Q function by definition is the value of (state, action). So as long as the policy is deterministic the output would be always same too. How's this different from Q learning?--------The model description doesn't specify what is the policy, and it's only being mentioned in data generation part.--------Why is it a model based approach?--------The learning curves are only for 19 iterations, which does not give any useful information. The final results are clearly nothing comparable to previous works. The model is only being tested on three games.----------------The paper is vague and using informal language or sometimes misusing the common RL terms. The experiments are very small scale and even in that scenario performing very bad. It's not clear, why it's a model-based approach. ","The authors have proposed a new method for deep RL that uses model-based evaluation of states and actions and reward/life loss predictions. The evaluation, on just 3 ATari games with no comparisons to state of the art methods, is insufficient, and the method seems ad-hoc and unclear. Design choices are not clearly described or justified. The paper gives no insight as to how the different aspects of the approach relate or contribute to the overall results."
https://openreview.net/forum?id=HyY4Owjll,"The authors propose two approaches to combine multiple weak generative models into a stronger one using principles from boosting. The approach is simple and elegant and basically creates an unnormalized product of experts model, where the individual experts are trained greedily to optimize the overall joint model. Unfortunately, this approach results in a joint model that has some undesirable properties: a unknown normalisation constant for the joint model and therefore an intractable log-likelihood on the test set; and it makes drawing exact samples from the joint model intractable. These problems can unfortunately not be fixed by using different base learners, but are a direct result of the product of experts formulation of boosting.--------  --------The experiments on 2 dimensional toy data illustrate that the proposed procedure works in principle and that the boosting formulation produces better results than individual weak learners and better results than e.g. bagging. But the experiments on MNIST are less convincing: Without an undisputable measure like e.g. log-likelihood it is hard to draw conclusions from the samples in Figure 2; and visually they look weak compared to even simple models like e.g. NADE.----------------I think the paper could be improved significantly by adding a quantitative analysis: investigating the effect of combining undirected (e.g. RBM), undirected (e.g. VAE) and autoregressive (e.g. NADE) models and by measuring the improvement over the number of base learners. But this would require a method to estimate the partition function Z or estimating some proxy.","The idea of boosting has recently seen a revival, and the ideas presented here are stimulating. After discussion, the reviewers agreed that the latest updates and clarifications have improved the paper, but overall they still felt that the paper is not quite ready, especially in making the case for when taking this approach is desirable, which was the common thread of concern for everyone. For this reason, this paper is not yet ready for acceptance at this year's conference."
https://openreview.net/forum?id=H1_QSDqxl,"The paper presents a nice idea of directly finding rules such as brother(father) => uncle in knowledge bases, by directly searching in embedding space. The idea is to interpret the successive application of relationships as the multiplication of the relation-dependent matrices in non-negative RESCAL. ----------------The experimental section provides an evaluation of the rules that are found by the algorithm. Nonetheless, the work seems only at its first stages for now, and many questions are left open:----------------1) while the approach to find rules seems very general, the reason why it should work is unclear. What properties of the embedding space or of the initial algorithm are required for this approach to find meaningful rules? Can we apply the same principles to other algorithms than non-negative RESCAL?----------------2) there is no real evaluation in terms of link prediction. How can we use these rules in conjunction with the original algorithm to improve link prediction? What performance gains can be expected? Can these rules find links that would not be found be the original algorithm in the first place?----------------3) scaling: for now the number of parameters of the rule miner is (#relationships)^(max. path length + 1). How does this method scale on standard benchmarks such as FB15k where there is more than a 1000 of relationships?","The reviewers concur that the paper is well written and the topic is interesting, but that the authors have not put sufficient effort into motivating their approach and evaluating it. The baselines seem too simple, the evaluation is incomplete. It is furthermore disappointing that the authors not only did not respond to the reviews, but did not respond to the pre-review questions. There is little in this review process that would support the paper being accepted, and therefore I concur with the reviewers' opinion and support rejection."
https://openreview.net/forum?id=S1xh5sYgx,"Summary: The paper presents a smaller CNN architecture called SqueezeNet for embedded deployment. The paper explores CNN macroarchitecture and microarchitecture to develop SqueezeNet, which is composed of fire modules.----------------Pros: --------Achieves x50 less memory usage than AlexNet while keeping similar accuracy.----------------Cons & Questions:--------Complex by-pass has less accuracy than simple by-pass. And simple by-pass is like ResNet bottlenecks and complex by-pass is like inception modules in GoogLeNet. Can we say that these two valiants of SqueezeNet are adaptation of concepts seen in GoogLeNet and ResNet? If so, then shouldn’t be there a SqueezeNet like model that achieves similar accuracy compared with GoogLeNet and ResNet?","The paper proposes a ConvNet architecture (""SqueezeNet"") and a building block (""Fire module"") aimed at reducing the model size while maintaining the AlexNet level of accuracy. The novelty of the submission is very limited as very similar design choices have already been used for model complexity reduction in Inception and ResNet. Because of this, we recommend rejection and invite the authors to further develop their method."
https://openreview.net/forum?id=Byx5BTilg,"The topic is very interesting, but the paper is not convincing. Specifically, the experiment part is weak. The study should include datasets that are familiar to the community as well as the ones ""that are not often addressed by deep learning"". The comparison to other approaches is not comprehensive.",The reviewers unanimously recommend rejection.
https://openreview.net/forum?id=ry54RWtxx,"This paper takes a first step towards learning to statically analyze source code. It develops a simple toy programming language that includes loops and branching. The aim is to determine whether all variables in the program are defined before they are used. The paper tries a variety of off-the-shelf sequence classification models and develops a new model that makes use of a ``differentiable set'' to keep track of which variables have been defined so far. Result show that an LSTM model can achieve 98% accuracy, and the differentiable set model can achieve 99.3% accuracy with sequence-level supervision and 99.7% accuracy with strong token-level supervision. An additional result is used whereby an LSTM language model is trained over correct code, and then low probability (where a threshold to determine low is tuned by hand) tokens are highlighted as sources of possible error.----------------One further question is if the authors could clarify what reasoning patterns are needed to solve these problems. Does the model need to, e.g., statically determine whether an `if` condition can ever evaluate to true in order to solve these tasks? Or is it just as simple as checking whether a variable appears on a LHS before it appears on a RHS later in the textual representation of the program?----------------Strengths:--------- Learning a static analyzer is an interesting concept, and I think there is good potential for this line of work--------- The ability to determine whether variables are defined before they are used is certainly a prerequisite for more complicated static analysis.--------- The experimental setup seems reasonable--------- The differentiable set seems like a useful (albeit simple) modelling tool----------------Weaknesses:--------- The setup is very toy, and it's not clear to me that this makes much progress towards the challenges that would arise if one were trying to learn a static analyzer --------- The models are mostly very simple. The one novelty on the modelling front (the differentiable set) provides a small win on this task, but it's not clear if it is a useful general construct or not.----------------Overall:--------I think it's an interesting start, and I'm eager to see how this line of work progresses. In my opinion, it's a bit too early to accept this work to ICLR, but I'd be excited about seeing what happens as the authors try to push the system to learn to analyze more properties of code, and as they push towards scenarios where the learned static analyzer would be useful, perhaps leveraging strengths of machine learning that are not available to standard programming languages analyses.","There is a general consensus that, though the idea is interesting, the work is not mature enough for a conference publication (e.g., the problem is too toy, not clear that really solves any, even artificial problem, better than existing techniques)."
https://openreview.net/forum?id=Hy8X3aKee,The paper compare three representation learning algorithms over symbolized sequences. Experiments are executed on several prediction tasks. The approach is potentially very important but the proposed algorithm is rather trivial. Besides detailed analysis on hyper parameters are not described. ,"The main strengths and weaknesses pointed out by the reviewers were:    Strengths  -Domain is interesting, problem is important (R2, R1)  -Discretization of continuous domain may enable leveraging of advanced tools developed for discrete domains, e.g. NLP (R1)    Weaknesses  -Issues with experiments: models under-capacity, omission of obvious baselines (R1, R3)  -Unclear conclusion: is quantization and embedding superior to working with the raw data? (R1)  -Fair amount of relevant work omitted (R1, R3)    While the authors engaged in the pre-review discussion, they did not respond to the official reviews. Therefore I have decided to align with the reviewers who are in consensus that the paper does not meet the acceptance bar."
https://openreview.net/forum?id=SkwSJ99ex,"This paper proposes to reduce model size and evaluation time of deep CNN models on mobile devices by converting multiple layers into single layer and then retraining the converted model. The paper showed that the computation time can be reduced by 3x to 5x with only 0.4% accuracy loss on a specific model.----------------Reducing model sizes and speeding up model evaluation are important in many applications. I have several concerns:----------------1. There are many techniques that can reduce model sizes. For example, it has been shown by several groups that using the teacher-student approach, people can achieve the same and sometimes even better accuracy than the teacher (big model) using a much smaller model. However, this paper does not compare any one of them.--------2. The technique proposed in this paper is limited in its applicability since it's designed specifically for the models discussed in the paper. --------3. Replacing several layers with single layer is a relatively standard procedure. For example, the mean variance normalization layer and batch normalization layer can all be absorbed without retraining or losing accuracy.----------------BTW, the DNN low-rank approximation technique was first proposed in speech recognition. e.g., ----------------Xue, J., Li, J. and Gong, Y., 2013, August. Restructuring of deep neural network acoustic models with singular value decomposition. In INTERSPEECH (pp. 2365-2369).",The proposed method doesn't have enough novelty to be accepted to ICLR.
https://openreview.net/forum?id=Hk3mPK5gg,"This is a solid paper that applies A3C to Doom, enhancing it with a collection of tricks so as to win one of the VizDoom competitions. I think it is fair to expect the competition aspect to overshadow the more scientific approach of justifying every design decision in isolation, but in fact the authors do a decent job at the latter.----------------Two of my concerns have remained unanswered (see AnonReviewer2, below). ----------------In addition, the citation list is rather thin, for example reward shaping has a rich literature, as do incrementally more difficult task setups, dating back at least to Mark Ring’s work in the 1990s. There has also been a lot of complementary work on other FPS games. I’m not asking that the authors do any direct comparisons, but to give the reader a sense of context in which to place this.","This paper provides a number of performance enhancements inspired by domain knowledge. Taken together, these produce a compelling system that has shown itself to be the best-in-class as per the related competition.  Experts agree that the authors do a good job at justifying the majority of the design decisions.    pros:  - insights into the SOTA Doom player    cons:  - lack of pure technical novelty: the various elements have existed previously    This paper comes down to a matter of taste in terms of appreciation of SOTA systems or technical novelty.  With the code being released, I believe that this work will have impact as a benchmark, and as a guidebook  as to how features can be combined for SOTA performance on FPS-style scenarios."
https://openreview.net/forum?id=HyxQzBceg,"Summary:--------The paper “Deep Variational Information Bottleneck” explores the optimization of neural networks for variational approximations of the information bottleneck (IB; Tishby et al., 1999). On the example of MNIST, the authors show that this may be used for regularization or to improve robustness against adversarial attacks.----------------Review:--------The IB is potentially very useful for important applications (regularization, adversarial robustness, and privacy are mentioned in the paper). Combining the IB with recent advances in deep learning to make it more widely applicable is an excellent idea. But given that the theoretical contribution is a fairly straight-forward application of well-known ideas, I would have liked to see a stronger experimental section.----------------Since the proposed approach allows us to scale IB, a better demonstration of this would have been on a larger problem than MNIST. It is also not clear whether the proposed approach will still work well to regularize more interesting networks with many layers.----------------Why is dropout not included in the quantitative comparison of robustness to adversarial examples (Figure 4)?----------------How was the number of samples (12) chosen?----------------What are the error bars in Figure 1 (a)?----------------On page 7 the authors claim “the posterior covariance becomes larger” as beta “decreases” (increases?). Is this really the case? It’s hard to judge based on Figure 1, since the figures are differently scaled.----------------It might be worth comparing to variational fair autoencoders (Louizos et al., 2016), which also try to learn representations minimizing the information shared with an aspect of the input.----------------The paper is well written and easy to follow.",This paper discussses applying an information bottleneck to deep networks using a variational lower bound and reparameterization trick. The paper is well written and the examples are compelling. The paper can be improved with more convincing results on MNIST.
https://openreview.net/forum?id=ryWKREqxx,"The paper aims to consolidate some recent literature in simple types of ""reading comprehension"" tasks involving matching questions to answers to be found in a passage, and then to explore the types of structure learned by these models and propose modifications. These reading comprehension datasets such as CNN/Daily Mail are on the simpler side because they do not generally involve chains of reasoning over multiple pieces of supporting evidence as can be found in datasets like MCTest. Many models have been proposed for this task, and the paper breaks down these models into ""aggregation readers"" and ""explicit reference readers."" The authors show that the aggregation readers organize their hidden states into a predicate structure which allows them to mimic the explicit reference readers. The authors then experiment with adding linguistic features, including reference features, to the existing models to improve performance.----------------I appreciate the re-naming and re-writing of the paper to make it more clear that the aggregation readers are specifically learning a predicate structure, as well as the inclusion of results about dimensionality of the symbol space. Further, I think the effort to organize and categorize several different reading comprehension models into broader classes is useful, as the field has been producing many such models and the landscape is unclear. ----------------The concerns with this paper are that the predicate structure demonstrated is fairly simple, and it is not clear that it provides insight towards the development of better models in the future, since the ""explicit reference readers"" need not learn it, and the CNN/Daily Mail dataset has very little headroom left as demonstrated by Chen et al. 2016. The desire for ""dramatic improvements in performance"" mentioned in the discussion section probably cannot be achieved on these datasets. More complex datasets would probably involve multi-hop inference which this paper does not discuss. Further, the message of the paper is a bit scattered and hard to parse, and could benefit from a bit more focus.----------------I think that with the explosion of various competing neural network models for NLP tasks, contributions like this one which attempt to organize and analyze the landscape are valuable, but that this paper might be better suited for an NLP conference or journal such as TACL.","The reviewers mostly agree that this paper offers valuable insights about a family of problems in automatic ""reading"" of text, as well as current solutions. The paper seems to fail to generate excitement because it doesn't really point the way forward. I disagree with some reviewers about the work's suitability for ICLR (as opposed to an ACL venue) since ML researchers are also thinking about these tasks now. The consensus is that the paper will have greater impact (wherever it is published) with a clearer message."
https://openreview.net/forum?id=Sk2iistgg,"This paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The authors proposed an iterative minimization approach in order to obtain a local optimum of a relaxed problem. --------The paper contains errors and the experimental evaluation is not convincing. Only old techniques are compared against in very toy datasets. ----------------The authors claim state-of-the-art, however, the oil dataset is not a real benchmark, and the comparisons are to very old approaches. --------The experimental evaluation should demonstrate robustness to more complex noise and outliers, as this was one of the motivations in the introduction.----------------The authors do not address the out-of-sample problem. This is a problem of kernel-based methods vs LVMs, and thus should be address here.------------------------The paper contains errors:----------------- The last paragraph of section 1 says that this paper proposes a closed form solution to robust KPCA. This is simply wrong, as the proposed approach consists of iteratively solving iterativey a set of closed form updates  and Levenberg-Marquard optimizationd. This is not any more closed form!----------------- In the same paragraph (and later in the text) the authors claim that the proposed approach can be trivially generalized to incorporate other cost functions. This is not true, as in general there will be no more inner loop closed form updates and the authors will need to solve a much more complex optimization problem. ----------------- The third paragraph of section 2 claims that this paper presents a novel energy minimization framework to solve problems of the general form of eq. (2). However, this is not what the authors solve at the end. They solve a different problem that has been subject to at least two relaxations. It is not clear how solving for a local optima of this double relaxed problem is related to the original problem they want to solve. ----------------- The paper says that Geiger et al defined non linearities on a latent space of pre-defined dimensionality. This is wrong. This paper discovers the dimensionality of the latent space by means of a regularizer that encourages the singular values to be sparse. Thus, it does not have a fixed dimensionality, the latent space is just bounded to be smaller or equal than the dimensionality of the original space. ------------------------It is not clear to me why the author say for LVMs such as GPLVM that ""the latent space is learned a priority with clean training data"". One can use different noise models within the GP framework. Furthermore, the proposed approach assumes Gaussian noise (see eq. 6), which is also the trivial case for GP-based LVMs.  ------------------------It is not clear what the authors mean in the paper by ""pre-training"" or saying that techniques do not have a training phase. KPCA is trained via a closed-form update, but there is still training.  ","There is complete consensus among the reviewers that the KPCA formulation in this paper needs better motivation; that the paper has technical errors, and the experimental evaluation is not convincing. As such the paper is not up to ICLR standards. The authors are encouraged to revise the paper based on feedback from the reviews."
https://openreview.net/forum?id=HkpLeH9el,"The paper proposes a set of recommendations for the design of differentiable programming languages, based on what made gradient descent more successful in experiments.----------------I must say i’m no expert in program induction. While i understand there is value in exploring what the paper set out to explore -- making program learning easier -- i did not find the paper too engaging. First everything is built on top of Terpret, which isn’t yet publicly available. Also most of the discussion is very detailed on the programming language side and less so on the learning side. It is conceivable that it would be best received on a programming language conference. A comparison with alternatives not generating code would be valuable in my opinion, to motivate for the overall setup.----------------Pros: --------Useful, well executed, novel study.--------Cons:--------Low on learning-specific contributions, more into domain-related constraints. Not sure a great fit to ICLR.","Quality, Clarity: There is no consensus on this, with the readers having varying backgrounds, and one reviewer commenting that they found it to be unreadable.     Originality, Significance:   The reviews are mixed on this, with the high score (7) acknowledging a lack of expertise on program induction.  The paper is based on the published TerpreT system, and some think that it marginal and contradictory with respect to the TerpreT paper. In the rebuttal, point (3) from the authors points to the need to better understand gradient-based program search, even if it is not always better. This leaves me torn about a decision on this paper, although currently it does not have strong support from the most knowledgeable reviewers.  That said, due to the originality of this work, the PCs are inclined to invite this work to be presented as a workshop contribution."
https://openreview.net/forum?id=HyCRyS9gx,"This paper proposes an interesting idea for rapidly adapting generative models in the low data regime. The idea is to use similar techniques that are used in one-shot learning, specifically ideas from matching networks. To that end, the authors propose the generative matching networks model, which is effectively a variational auto-encoder that can be conditioned on an input dataset. Given a query point, the model matches the query point to points in the conditioning set using an attention model in an embedding space (this is similar to matching networks). The results on the Omniglot dataset show that this method is successfully able to rapidly adapt to new input distributions given few examples.----------------I think that the method is very interesting, however the major issue for me with this paper is a lack of clarity. I outline more details below, but overall I found the paper somewhat difficult to follow. There are a lot of details that I feel are scattered throughout, and I did not get a sense after reading this paper that I would be able to implement the method and replicate the results. My suggestion is to consolidate the major implementation details into a single section, and be explicit about the functional form of the different embedding functions and their variants.----------------I was a bit disappointed to see that weak supervision in the form of labels had to be used. How does the method perform in a completely unsupervised setting? This could be an interesting baseline.----------------There is a lack of definition of the different functions. Some basic insight into the functional forms of f, g, \phi, sim and R would be nice. Otherwise it is very unclear to me what’s going on.----------------Section 3.2: “only state of the recurrent controller was used for matching”, my reading of this section (after several passes) is that the pseudo-input is used in the place of a regular input. Is this correct? Otherwise, this sentence/section needs more clarification. I noticed upon further reading in section 4.2 that there are two versions of the model: one in which a pseudo input is used, and one in which a pseudo input is not used (the conditional version). What is the difference in functional form between these? That is, how do the formulas for the embeddings f and g change between these settings?----------------“since the result was fully contrastive we did not apply any further binarization” what does it mean for a result to be fully contrastive?----------------For clarity, the figures and table refer to the number of shots, but this is never defined. I assume this is T here. This should be made consistent.----------------Figure 2: why is the value of T only 9 in this case? What does it mean for it to be 0? It is stated earlier that T should go up to 20 (I assume #shot corresponds to T). It also looks like the results continue to improve with an increased number of steps, I would like to see the results for 5 and maybe 6 steps as well. Presumably there will come a point where you get diminishing returns.----------------Table 1: is the VAE a fair baseline? You mention that Ctest affects Pd() in the evaluation. The fact that the VAE does not have an associated Ctest implies that the two models are being evaluated with a different metric. Can the authors clarify this? It’s important that the comparison is apples-to-apples.----------------MNIST is much more common than Omniglot for evaluating generative models. Would it be possible to perform similar experiments on this dataset? That way it can be compared with many more models.----------------Further, why are the negative log-likelihood values monotonically decreasing in the number of shots? That is, is there ever a case where increasing the number of shots can hurt things? What happens at T=30? 40?----------------As a minor grammatical issue, the paper is missing determiners in several sentences. At one point, the model is referred to as “she” instead of “it”. “On figure 3” should be changed to “in figure 3” in the experiments section.","This work extends variational autoencoders to adapt to a new dataset containing a small number of examples. While this work is promising, two of the reviewers had serious concerns about clarity. A new version of the paper has been submitted, however I still find it too hard to follow and would find it hard to accurately describe what was done having read the main body of the paper."
https://openreview.net/forum?id=r1VdcHcxx,"The paper shows that BN, which does not work out of the box for RNNs, can be used with LSTM when the operator is applied to the hidden-to-hidden and the input-to-hidden contribution separately. Experiments are conducted to show that it leads to improved generalisation error and faster convergence.----------------The paper is well written and the idea well presented. ----------------i) The data sets and consequently the statistical assumptions used are limited (e.g. no continuous data, only autoregressive generative modelling).--------ii) The hyper parameters are nearly constant over the experiments. It is ruled out that they have not been picked in favor of one of the methods. E.g. just judging from the text, a different learning rate could have lead to equally fast convergence for vanilla LSTM. ----------------Concluding, the experiments are flawed and do not sufficiently support the claim. An exhaustive search of the hyper parameter space could rule that out. ","The reviewers believe this paper is of significant interest to the ICLR community, as it demonstrates how to get the popular batch normalization method to work in the recurrent setting. The fact that it has already been cited a variety of times also speaks to its interest within the community. The extensive experiments are convincing that the method works. One common criticism is that the authors don't address enough the added computational cost of the method in the text or empirically. Plots showing loss as a function of wall-clock time instead of training iteration would be more informative to readers deciding whether to use batch norm.     Pros:   - Gets batch normalization to work on recurrent networks (which had been elusive to many)  - The experiments are thorough and demonstrate the method reduces training time (as a function of training iterations)  - The paper is well written and accessible    Cons  - The contribution is relatively incremental (several tweaks to an existing method)  - The major disadvantage to the approach is the added computational cost, but this is conspicuously not addressed."
https://openreview.net/forum?id=B1ewdt9xe,"Paper Summary--------This paper proposes an unsupervised learning model in which the network--------predicts what its state would look like at the next time step (at input layer--------and potentially other layers).  When these states are observed, an error signal--------is computed by comparing the predictions and the observations. This error--------signal is fed back into the model. The authors show that this model is able to--------make good predictions on a toy dataset of rotating 3D faces as well as on--------natural videos. They also show that these features help perform supervised--------tasks.----------------Strengths--------- The model is an interesting embodiment of the idea of predictive coding--------  implemented using a end-to-end backpropable recurrent neural network architecture.--------- The idea of feeding forward an error signal is perhaps not used as widely as it could--------  be, and this work shows a compelling example of using it. --------- Strong empirical results and relevant comparisons show that the model works well.--------- The authors present a detailed ablative analysis of the proposed model.----------------Weaknesses--------- The model (esp. in Fig 1) is presented as a generalized predictive model--------  where next step predictions are made at each layer. However, as discovered by--------running the experiments, only the predictions at the input layer are the ones--------that actually matter and the optimal choice seems to be to turn off the error--------signal from the higher layers. While the authors intend to address this in future--------work, I think this point merits some more discussion in the current work, given--------the way this model is presented.--------- The network currently lacks stochasticity and does not model the future as a--------  multimodal distribution (However, this is mentioned as potential future work).----------------Quality--------The experiments are well-designed and a detailed analysis is provided--------in the appendix.----------------Clarity--------The paper is well-written and easy to follow.----------------Originality--------Some deep models have previously been proposed that use predictive coding.--------However, the proposed model is most probably novel in the way it feds back the--------error signal and implements the entire model as a single differentiable--------network.----------------Significance--------This paper will be of wide interest to the growing set of researchers working--------in unsupervised learning of time series. This helps draw attention to--------predictive coding as an important learning paradigm.----------------Overall--------Good paper with detailed and well-designed experiments. The idea of feeding--------forward the error signal is not being used as much as it could be in our--------community. This work helps to draw the community's attention to this idea.","This paper proposes an interesting architecture for predicting future frames of videos using end-to-end trained deep predictive coding.   The architecture is well presented and the paper is clearly written. The experiments are extensive and convincing, include ablation analyses, and show that this architecture performs well compared to other current methods.  Overall, this is an interesting, solid contribution."
https://openreview.net/forum?id=r1nTpv9eg,"This paper purports to investigate the ability of RL agents to perform ‘physics experiments’ in an environment, to infer physical properties about the objects in that environment. The problem is very well motivated; indeed, inferring the physical properties of objects is a crucial skill for intelligent agents, and there has been relatively little work in this direction, particularly in deep RL. The paper is also well-written.----------------As there are no architectural or theoretical contributions of the paper (and none are claimed), the main novelty comes in the task application – using a recurrent A3C model for two tasks that simulate an agent interacting with an environment to infer physical properties of objects. More specifically, two tasks are considered – moving blocks to determine their mass, and poking towers such that they fall to determine the number of rigid bodies they are composed of. These of course represent a very limited cross-section of the prerequisite abilities for an agent to understand physics. This in itself is not a bad thing, but since there is no comparison of different (simpler) RL agents on the tasks, it is difficult to determine if the tasks selected are challenging. As mentioned in the pre-review question, the ‘Which is Heavier’ task seems quite easy due to the actuator set-up, and the fact that the model simply must learn to take the difference between successive block positions (which are directly encoded as features in most experiments).  Thus, it is not particularly surprising that the RL agent can solve the proposed tasks. ----------------The main claim beyond solving two proposed tasks related to physics simulation is that “the agents learn different strategies for these tasks that balance the cost of gathering information against the cost of making mistakes”. The ‘cost of gathering information’ is implemented by multiplying the reward with a value of gamma < 1. This is somewhat interesting behaviour, but is hardly surprising given the problem setup.----------------One item the authors highlight is that their approach of learning about physical object properties through interaction is different from many previous approaches, which use visual cues. However, the authors also note that this in itself is not novel, and has been explored in other work (e.g. Agrawal et al. (2016)). I think it’s crucial for the authors to discuss these approaches in more detail (potentially along with removing some other, less relevant information from the related work section), and specifically highlight why the proposed tasks in this paper are interesting compared to, for example, learning to move objects towards certain end positions by poking them.----------------To discern the level of contribution of the paper, one must ask the following questions: ----------------1) how much do these two tasks contribute (above previous work) to the goal of having agents learn the properties of objects by interaction; and--------2) how much do the results of the RL agent on these tasks contribute to our understanding of agents that interact with their environment to learn physical properties of objects? ----------------It is difficult to know exactly, but due to the concerns outlined above, I am not convinced that the answers to (1) or (2) are “to a significant extent”. In particular, for (1), since the proposed agent is able to essentially solve both tasks, it is not clear that the tasks can be used to benchmark more advanced agents (e.g. it can’t be used as a set of bAbI-like tasks). ----------------Another possible concern, as pointed out by Reviewer 3, is that the description of the model is extremely concise. It would be nice to have, for example, a diagram illustrating the inputs and outputs to the model at each time step, to ease replication.----------------Overall, it is important to make progress towards agents that can learn to discover physical properties of their environment, and the paper contributes in this direction. However, the technical contributions of this paper are rather limited – thus, it is not clear to what extent the paper pushes forward research in this direction beyond previous work that is mentioned. It would be nice, for example, to have some discussion about the future of agents that learn physics from interaction (speculation on more difficult versions of the tasks in this paper), and how the proposed approach fits into that picture.  ---------------------------------------EDIT: score updated, see comments below","The following statement best summarizes the contribution: ""This paper shows that model free RL methods can learn how to gather information about physical properties of objects, even when this information is not available to a passive observer, and use this information to make decisions."" So this is not a paper about new theory or algorithms, but rather about solving the problem of acquiring knowledge about the physics of the world around us, which is important for many problems and helps explain human performance in many tasks. There are still some concerns about the depth-of-analysis of the paper, but on balance, it is seen as an unconventional but interesting paper. As per AnonReviewer6, the final version could still aim to better address ""What should other researchers focus on if they are trying to build agents that can understand physics intuitively (building off this work)?""  -- Area chair"
https://openreview.net/forum?id=ryCcJaqgl,"Updated review: the authors did an admirable job of responding to and incorporating reviewer feedback. In particular, they put a lot of effort into additional experiments, even incorporating a new and much stronger baseline (the ConvNet -> LSTM baseline requested by multiple reviewers). I still have two lingering concerns previously stated -- that each model's architecture (# hidden units, etc.) should be tuned independently and that a pure time series forecasting baselines (without the trend preprocessing) should be tried. I'm going to bump up my score from a clear rejection to a borderline.-------------------------------------This paper is concerned with time series prediction problems for which the prediction targets include the slope and duration of upcoming local trends. This setting is of great interest in several real world problem settings (e.g., financial markets) where decisions (e.g., buy or sell) are often driven by local changes and trends. The primary challenge in these problems is distinguishing true changes and trends (i.e., a downturn in share price) from noise. The authors tackle this with an interesting hybrid architecture (TreNet) with four parts: (1) preprocessing to extract trends, (2) an LSTM that accepts those trends as inputs to ostensibly capture long term dependencies, (3) a ConvNet that accepts a local window of raw data as its input at each time step, and (4) a higher ""feature fusion"" (i.e., dense) layer to combine the LSTM's and ConvNet's outputs. On three univariate time series data sets, the TreNet outperforms the competing baselines including those based on its constituent parts (LSTM + trend inputs, CNN).----------------Strengths:--------- A very interesting problem setting that can plausibly be argued to differ from other sequential modeling problems in deep learning (e.g., video classification). This is a nice example of fairly thoughtful task-driven machine learning.--------- Accepting the author's assumptions as true for the moment, the proposed architecture seems intuitive and well-designed.----------------Weaknesses:--------- Although this is an interesting problem setting (decisions driven by trends and changes), the authors did not make a strong argument for why they formulated the machine learning task as they did. Trend targets are not provided from ""on high"" (by data oracle) but extracted from raw data using a deterministic algorithm. Thus, one could just easily formulate this as plain time series forecasting problem in which we forecast the next 100 steps and then apply the trend extractor to convert those predictions into a trend. If the forecasts are accurate, so will be the extracted trends.--------- The proposed architecture, while interesting, is not justified, in particular the choice to feed the extracted trends and raw data into separate LSTM and ConvNet layers that are only combined at the end by a shallow MLP. An equally straightforward but more intuitive choice would have been to feed the output of the ConvNet into the LSTM, perhaps augmented by the trend input. Without a solid rationale, this unconventional choice comes across as arbitrary.--------- Following up on that point, the raw->ConvNet->LSTM and {raw->ConvNet,trends}->LSTM architectures are natural baselines for experiments.--------- The paper presupposes, rather than argues, the value of the extracted trends and durations as inputs. It is not unreasonable to think that, with enough training data, a sufficiently powerful ConvNet->LSTM architecture should be able to learn to detect these trends in raw data, if they are predictive.--------- Following up on that point, two other obvious baselines that were omitted: raw->LSTM and {raw->ConvNet,trends}->MLP. Basically, the authors propose a complex architecture without demonstrating the value of each part (trend extraction, LSTM, ConvNet, MLP). The baselines are unnecessarily weak.----------------One thing I am uncertain about in general: the validity of the practice of using the same LSTM and ConvNet architectures in both the baselines and the TreNet. This *sounds* like an apples-to-apples comparison, but in the world of hyperparameter tuning, it could in fact disadvantage either. It seems like a more thorough approach would be to optimize each architecture independently.----------------Regarding related work and baselines: I think it is fair to limit the scope of in-depth analysis and experiments to a set of reasonable, representative baselines, at least in a conference paper submitted to a deep learning conference. That said, the authors ignored a large body of work on financial time series modeling using probabilistic models and related techniques. This is another way to frame the above ""separate trends from noise"" problem: treat the observations as noisy. One semi-recent example: J. Hernandez-Lobato, J. Lloyds, and D. Hernandez-Lobato. Gaussian process conditional copulas with applications to financial time series. NIPS 2013.----------------I appreciate this research direction in general, but at the moment, I believe that the work described in this manuscript is not suitable for inclusion at ICLR. My policy for interactive review is to keep an open mind and willingness to change my score, but a large revision is unlikely. I would encourage the authors to instead use their time and energy -- and reviewer feedback -- in order to prepare for a future conference deadline (e.g., ICML).","I appreciate the authors putting a lot of effort into the rebuttal. But it seems that all the reviewers agree that the local trend features segmentation and computation is adhoc, and the support for accepting the paper is lukewarm.    As an additional data point, I would argue that the model is not end-to-end since it doesn't address the aspect of segmentation. Incorporating that into the model would have made it much more interesting and novel."
https://openreview.net/forum?id=BJbD_Pqlg,"The paper reports several connections between the image representations in state-of-the are object recognition networks and findings from human visual psychophysics:--------1) It shows that the mean L1 distance in the feature space of certain CNN layers is predictive of human noise-detection thresholds in natural images.--------2) It reports that for 3 different 2-AFC tasks for which there exists a condition that is hard and one that is easy for humans, the mutual information between decision label and quantised CNN activations is usually higher in the condition that is easier for humans.--------3) It reproduces the general bandpass nature of contrast/frequency detection sensitivity in humans. ----------------While these findings appear interesting, they are also rather anecdotal and some of them seem to be rather trivial (e.g. findings in 2). To make a convincing statement it would be important to explore what aspects of the CNN lead to the reported findings. One possible way of doing that could be to include good baseline models to compare against. As I mentioned before, one such baseline should be reasonable low-level vision model. Another interesting direction would be to compare the results for the same network at different training stages.----------------In that way one might be able to find out which parts of the reported results can be reproduced by simple low-level image processing systems,  which parts are due to the general deep network’s architecture and which parts arise from the powerful computational properties (object recognition performance) of the CNNs.----------------In conclusion, I believe that establishing correspondences between state-of-the art CNNs and human vision is a potentially fruitful approach. However to make a convincing point that found correspondences are non-trivial, it is crucial to show that non-trivial aspects of the CNN lead to the reported findings, which was not done. Therefore, the contribution of the paper is limited since I cannot judge whether the findings really tell me something about a unique relation between high-performing CNNs and the human visual system.----------------UPDATE:----------------Thank you very much for your extensive revision and inclusion of several of the suggested baselines. --------The results of the baseline models often raise more questions and make the interpretation of the results more complex, but I feel that this reflects the complexity of the topic and makes the work rather more worthwhile. ----------------One further suggestion: As the experiments with the snapshots of the CaffeNet shows, the direct relationship between CNN performance and prediction accuracy of biological vision known from Yamins et al. 2014 and Cadieu et al. 2014 does not necessarily hold in your experiments. I think this should be discussed somewhere in the paper.----------------All in all, I think that the paper now constitutes a decent contribution relating state-of-the art CNNs to human psychophysics and I would be happy for this work to be accepted.----------------I raise the my rating for this paper to 7.","I think the reviewers evaluated this paper very carefully and were well balanced. The reviewers all agree that the presented comparison between human vision and DNNs is interesting. At the same time, none of the reviewers would strongly defend the paper. As it stands, this work seems a little too premature for publication as the analysis does not go too much beyond what we already know. We encourage the authors to deepen the investigation and resubmit."
https://openreview.net/forum?id=B1hdzd5lg,"This paper proposes a new gating mechanism to combine word and character representations. The proposed model sets a new state-of-the-art on the CBT dataset; the new gating mechanism also improves over scalar gates without linguistic features on SQuAD and a twitter classification task. ----------------Intuitively, the vector-based gate working better than the scalar gate is unsurprising, as it is more similar to LSTM and GRU gates. The real contribution of the paper for me is that using features such as POS tags and NER help learn better gates. The visualization in Figure 3 and examples in Table 4 effectively confirm the utility of these features, very nice! ----------------In sum, while the proposed gate is nothing technically groundbreaking, the paper presents a very focused contribution that I think will be useful to the NLP community. Thus, I hope it is accepted.","Sorry to pop up late, but I've been looking over ICLR accepted papers, and I noticed this one. Something very similar, by combining word and character information at the feature level using sigmoid gating, has been done before, see https://aclweb.org/anthology/C/C16/C16-1030.pdf  It would be good to see a citation to prior work in the final version of this paper. "
https://openreview.net/forum?id=HkIQH7qel,"This paper presents an architecture for answer extraction task and evaluates on the SQUAD dataset. The proposed model builds fixed length representations of all spans in the answer document based on recurrent neural network. It outperforms a few baselines in exact match and F1 on SQUAD.----------------It is unfortunate that the blind test results are not obtained yet due to the copyright issue. There are quite a few other systems/submissions on the SQUAD leader board that were available for comparison.----------------Given that there's no result on the test set reported, the grid search for hyperparameters on the dev set directly is also a concern, even though the authors did cross validation experiments.","The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, most reviewers are not leaning sufficiently towards acceptance. In particular, it is unfortunate that authors can not evaluate their model on the leaderboard due to copyright issues. The role of standard datasets and benchmarks is to allow for meaningful comparisons. Evaluation on non-standard splits defeats this purpose. Fortunately, sounds like authors are working on getting their model evaluated on the leaderboard. Resolving that and incorporating reviewers' feedback will help make the paper stronger."
https://openreview.net/forum?id=ryh9pmcee,"This paper introduces an energy-based Generative Adversarial Network (GAN) and provides theoretical and empirical results modeling a number of image datasets (including large-scale versions of categories of ImageNet). As far as I know energy-based GANs (EBGAN) were introduced in Kim and Bengio (2016), but the proposed version makes a number of different design choices. ----------------First, it does away with the entropy regularization term that Kim and Bengio (2016) introduced to ensure that the GAN discriminator converged to an energy function proportional to the log density of the data (at optimum). This implies that the discriminator in the proposed scheme will become uniform at convergence as discussed in the theoretical section of the paper, however the introductory text seems to imply otherwise -- that one could recover a meaningful score function from the trained energy-function (discriminator). This should be clarified. ----------------Second, this version of the EBGAN setting includes two innovations: (1) the introduction of the hinge loss in the value function, and (2) the use of an auto-encoder parametrization for the energy function. These innovations are not empirically justified in any way - this is disappointing, as it would be really good to see empirical results supporting the arguments made in support of their introduction. ----------------The two significant contributions of this paper are the theoretical analysis of the energy-baesd GAN formalism (showing that the optimum corresponds to a Nash equilibrium) and the impressive empirical results on large images that set a new standard in what straight GAN-style models can achieve.----------------The theoretical results seem solid to me and make a nice contribution.----------------Regarding the quantitative results in Table 2, it seems not appropriate to bold the EBGAN line when it seems to be statistically indistinguishable form the Rasmus et al (2015) results. Though it is not mentioned here, the use of bold typically indicates the state-of-the-art. ----------------I think this paper could be be much stronger if the two novel contributions to the energy-based GAN setting were more thoroughly explored with ablation experiments. That being said, I think this paper has already become a contribution other are building on (including at least two other ICLR submissions) and so I think it should be accepted for publication at ICLR. ","The authors have proposed an energy-based rendition of probabilistic GANs, with the addition of auto-encoder and hinge loss to improve stability. Theoretical results involving the Nash equilibrium are also given. Solid paper, well-written. Novel contribution with good empirical and theoretical justification."
https://openreview.net/forum?id=r1br_2Kge,"Summary of the paper:----------------The paper introduces sketches that approximates linear and sparse polynomials on binary data. The paper shows that such sketches can be represented as a one layer neural network. Experiments are conducted on some language processing tasks. ----------------Novelty:----------------Approximation for linear functions and polynomial have been studied in previous work, hashing here is learned as a neural network which is novel. Although I have some reserves on the connexion between the theory presented and the learning part in the paper (Please see comments below).----------------The main concern is that the  single layer neural network that is  learned  is not restricted to the class of  hash functions studied, hence it is hard to know if we are really testing the hashing class or just a one layer neural network.----------------Clarity: ----------------The paper is  written in a clear way.----------------Comments :----------------- The link between neural network and the hashing stems from the implementation of the 'and operation' with a relu and a binary weight V and a bias 1-t. V encodes the support of the linear weight for instance in the linear model. --------When Learning V it is regularized with l_1 norm to encourage sparsity, nevertheless the weights are not restricted to be positive to be faithful to the theory introduced in the paper. When learning V by back propagation when can try projected gradient to restrict V values to be in [0,1]. On a synthetic example where the support of w is known it would be interesting to see if V recovers the support of w. ----------------- Similarly  for polynomial seems group sparsity is a good regularizer, difficulty being here that the groups are unknown. Many works have tackled that issue see for instance fbach/IEEE TSP shervashidze bach.pdf"" target=""_blank"" rel=""nofollow"">http://www.di.ens.fr/ fbach/IEEE TSP shervashidze bach.pdf----------------- The experiments are limited to small sets.","The reviewers present a detailed set of concerns regarding the paper. In particular, the paper lacks comparison to other sketching works. The sketches used in the paper are rudimentary and in practice, there are more sophisticated sketches employed.    Sketching has a different goal from function approximation. Sketching aims to reconstruct (with a certain error). The paper uses sketching in a naive manner, in the sense that the error due to the sketch is incorporated into the overall error. But presumably, networks can be compressed without achieving reconstruction on each instance, while maintaining a guarantee on the overall accuracy. I would find such a framework to be more interesting and practically relevant.    On the other hand, it is important for the ICLR community to be exposed to sketching frameworks. Hence, I would like to invite the paper to be presented in the workshop, in the hope that it can spur further ideas."
https://openreview.net/forum?id=HkvS3Mqxe,"This paper proposes two pruning methods to reduce the computation of deep neural network. In particular, whole feature maps and the kernel connections can be removed with not much decrease of classification accuracy. --------However, this paper also has the following problems. --------1) The method is somehow trivial, since the pruning masks are mainly chosen by simple random sampling. The novelty and scalability are both limited. --------2) Experiment results are mainly focused on the classification rate and the ideal complexity. As a paper on improving computation efficiency, it should include results on practical time consumption. It is very common that reducing numbers of operations may not lead to reduced computational time on a highly parallel platform (e.g., GPU). --------3) It is more important to improve the computational efficiency on large-scale models (e.g., ImageNet classification network) than on small models (e.g., MNIST, CIFAR network). However, results on large-scale network is missing.--------4) (*Logical validity of the proposed method*) For feature map pruning, what if just to train reduced-size network is trained from scratch without transfer any knowledge from the pretrained large network? Is it possible to get the same accuracy? If so, it will simply indicate the hyper-parameter is not optimal for the original network. Experimental results are necessary to clarify the necessity of feature map pruning. --------Note that I agree with that a smaller network may be more generalizable than a larger network. ------------------------------------------------------------------------------Comments to the authors's response:----------------Thanks for replying to my comments. ----------------1) I still believe that the proposed methods are trivial.--------2) It is nice to show GPU implementation. Compared to existing toolboxes (e.g., Torch, Caffe, TensorFlow), is the implementation of convolution efficient enough?--------3) Experiments on Cifar-100 are helpful (better than cifar-10), but it is not really large-scale, where speed-up is not so critical. ImageNet and Places datasets are examples of large-scale datasets.--------4) The author did not reply to the question wrt the validity of the proposed methods. This question is critical.   ","Unfortunately this paper is not competitive enough for ICLR. The paper is focusing on efficiency where for the results to be credible it is of utmost importance to present experiments on large scale data and state-of-the-art models.  I am afraid the reviewers were not able to take into account the latest drafts of the paper that were submitted in January, but those submissions came very late."
https://openreview.net/forum?id=HJDBUF5le,"The authors introduce a variant of the variational autoencoder (VAE) that models dataset-level latent variables. The idea is clearly motivated and well described. In my mind the greatest contribution of this paper is the movement beyond the relatively simple graphical model structure of the traditional VAEs and the introduction of more interesting structures to the deep learning community. ----------------Comments:----------------- It's not clear to me why this should be called a ""statistician"". Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network. One could consider a maximum likelihood approach, etc. In general it felt like the paper could be more clear, if it avoided coining new terms like ""statistic network"" and stuck to the more accurate ""approximate posterior"".----------------- The experiments are nice, and I appreciate the response to my question regarding ""one shot generation"". I still think that language needs to be clarified, specifically at the end of page 6. My understanding of Figure 5 is the following: Take an input set, compute the approximate posterior over the context vector, then generate from the forward model given samples from the approximate posterior. I would like clarification on the following: ----------------(a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior? ----------------(b) I agree that the samples are of high-quality, but that is not a quantified statement. The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities. To that end, one ""proper"" way of computing the ""one shot generation"" performance is to report log p(x | c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets. I suspect that log probability performance of these networks relative to a vanilla VAE without the context latent variable will be impressive. I still don't see a reason not to include that.",This is an interesting paper that adds nicely to the literature on VAEs and one-shot generalisation. This will be of interest to the community and will contribute positively to the conference.
https://openreview.net/forum?id=Hk1l9Xqxe,"This paper applies HDP-HMM to challenging bioacoustics segmentation problems including humpback whole sound and bird sound segmentation. Although the technique itself is not novel, the application of this data-driven method to bioacoustics segmentation is quite challenging, and may yield some scientific findings, and this is a valuable contribution to the bioacoustics field. My concern for this paper is that it does not have fair comparison of the other simple methods including BIC and AIC, and it is better to provide such comparisons. Especially, as the authors pointed out, the computational cost of HDP-HMM is a big issue, and the other simple methods may solve this issue.","The paper studies an unusual and (apparently) challenging application -- segmentation of whale and bird songs. Though the authors claim that the applications is much more challenging than previous applications of the proposed techniques (in speech processing), the evaluation is very questionable (as there is no gold standard), there is no convincing comparison with other (potentially simpler techniques). Overall, the reviewers believe the work is not mature enough to be accepted at ICLR.    + interesting dataset / task     - novelty is limited  - evaluation is weak  - writing is poor"
https://openreview.net/forum?id=BJuysoFeg,"Update: I thank the authors for their comments. I still think that the method needs more experimental evaluation: for now, it's restricted to the settings in which one can use pre-trained ImageNet model, but it's also important to show the effectiveness in scenarios where one needs to train everything from scratch. I would love to see a fair comparison of the state-of-the-art methods on toy datasets (e.g. see (Bousmalis et al., 2016), (Ganin & Lempitsky, 2015)). In my opinion, it's a crucial point that doesn't allow me to increase the rating.----------------This paper proposes a simple trick turning batch normalization into a domain adaptation technique. The authors show that per-batch means and variances normally computed as a part of the BN procedure are sufficient to discriminate the domain. This observation leads to an idea that adaptation for the target domain can be performed by replacing population statistics computed on the source dataset by the corresponding statistics from the target dataset.----------------Overall, I think the paper is more suitable for a workshop track rather than for the main conference track. My main concerns are the following:----------------1. Although the main idea is very simple, it feels like the paper is composed in such a way to make the main contribution less obvious (e.g. the idea could have been described in the abstract but the authors avoided doing so). ----------------2. (This one is from the pre-review questions) The authors are using much stronger base CNN which may account for the bulk of the reported improvement. In order to prove the effectiveness of the trick, the authors would need to conduct a fair comparison against competing methods. It would be highly desirable to conduct this comparison also in the case of a model trained from scratch (as opposed to reusing ImageNet-trained networks).","Although this paper has been rejected, I feel I need to say sth that matters if authors want to further develop their work.   The work tries to address a very important use case of BN when domain adaption is needed. I believe it is an important problem. I have some additional suggestions regarding its use cases.  1. Think about what would happen if no fine-tuning is used at all. Would this trick improve generalization performance on a different domain? 2. Instead of using accuracy, try more stable metrics based on ranking (e.g. AUC). I also suggest to report the variance of evaluation metrics under different random initialization. So we can observe how much gain is considered significant.  3. Think about what is the reason to use this trick from the simplest case: (a) if you are using BN for logistic regression, what will happen? (b) how about auto-encoder? "
https://openreview.net/forum?id=Hy-2G6ile,"Paper proposes Gated Muiltimodal Unit, a building block for connectionist models capable of handling multiple modalities.----------------(Figure 2) The bimodal case returns weighted activation by gains of gating units, do you do anything special to keep multi-modal case weighted as well? I.e. how the equation for h in section 3.1 would look like for multi-modal case. Also what’s the rationale for using tanh nonlinearity (over, say RELU), is it somehow experimentally optimised choice?----------------I would find interesting a discussion on a possibility of handling missing data in case one or more modalities are unavailable at test time. Is this possible in the current model to back-off to fewer modalities? Synthetic example may suggest that’s in fact possible. Those numbers, perhaps, could be added to table 2.----------------In the synthetic experiment, you should compare MGU with the fully-connected MLP model really, with similar complexity - that is - at least two hidden units (as GMU has two such for each modality) followed by logistic regression. At least in terms of capability of drawing decision boundary, those should be comparable.----------------I think, broader discussion shall be written on the related work associated with mixture of experts models (which is fact are very similar conceptually) as well as multiplicative RNN models [1]. Also, gating unit in LSTM can, in principle, play very similar role when multiple modalities are spliced in the input.----------------Overall, the paper is interesting, so is the associated (and to be released) dataset.----------------Minor comments/typos:----------------Sec. 3.3:  layers and a MLP (see Section 3.4) -> layers and an MLP----------------Apologies for unacceptably late review.----------------[1] Multiplicative LSTM for sequence modelling B Krause, L Lu, I Murray, S Renals","The authors propose a Gated Muiltimodal Unit to combine multi-modal information (visual and textual). They also collect a large dataset of movie summers and posters. Overall, the reviewers were quite positive, while AR4 points to related models and feels that the contribution in the current version is too weak for ICLR. The AC read the paper and the authors responses but tends to agree with AR4. The authors are encouraged to strengthen their work and resubmit to a future conference."
https://openreview.net/forum?id=HJrDIpiee,"This paper combines DRQN with eligibility traces, and also experiment with the Adam optimizer for optimizing the q-network. This direction is worth exploring, and the experiments demonstrate the benefit from using eligibility traces and Adam on two Atari games. The methods themselves are not novel. Thus, the primary contributions are (1) applying eligibility traces and Adam to DRQN and (2) the experimental evaluation. The paper is well-written and easy to understand.----------------The experiments provide quantitative results and detailed qualitative intuition for how and why the methods perform as they do. However, with only two Atari games in the results, it is difficult to tell how well it the method would perform more generally. Showing results on several more games and/or other domains would significantly improve the paper. Showing error bars from multiple random seeds would also improve the paper.","The reviewers agree that the paper is clear and well-written, but all reviewers raised significant concerns about the novelty of the work, since the proposed algorithm is a combination of well-known techniques in reinforcement learning. It is worth noting that the use of eligibility traces is not very heavily explored in the deep reinforcement learning literature, but since the contribution is primarily empirical rather than conceptual and algorithmic, there is a high bar for the rigorousness of the experiments. The reviewers generally did not find the evaluation to be compelling enough in this regard. Based on this evaluation, the paper is not ready for publication."
https://openreview.net/forum?id=Sk2iistgg,"This paper considers an alternate formulation of Kernel PCA with rank constraints incorporated as a regularization term in the objective. The writing is not clear. The focus keeps shifting from estimating “causal factors”, to nonlinear dimensionality reduction to Kernel PCA to ill-posed inverse problems. The problem reformulation of Kernel PCA uses somewhat standard tricks and it is not clear what are the advantages of the proposed approach over the existing methods as there is no theoretical analysis of the overall approach or empirical comparison with existing state-of-the-art.  ----------------- Not sure what the authors mean by “causal factors”. There is a reference to it in Abstract and in Problem formulation on page 3 without any definition/discussion.----------------- In KPCA, I am not sure why one is interested in step (iii) outlined on page 2 of finding a pre-image for each----------------- Authors outline two key disadvantages of the existing KPCA approach. The first one, that of low-dimensional manifold assumption not holding exactly, has received lots of attention in the machine learning literature. It is common to assume that the data lies near a low-dimensional manifold rather than on a low-dimensional manifold. Second disadvantage is somewhat unclear as finding “a data point (pre-image) corresponding to each projection in the input space” is not a standard step in KPCA. ----------------- On page 3, you never define , , . Clearly, they cannot be cartesian products. I have to assume that notation somehow implies N-tuples. ----------------- On page 3, Section 2,  and  are sets. What do you mean by ----------------- On page 5,  is never defined. ----------------- Experiments: None of the standard algorithms for matrix completion such as OptSpace or SVT were considered ----------------- Experiments: There is no comparison with alternate existing approaches for Non-rigid structure from motion.  ----------------- Proof of the main result Theorem 3.1: To get from (16) to (17) using the Holder inequality (as stated) one would end up with a term that involves sum of fourth powers of weights w_{ij}. Why would they equal to one using the orthonormal constraints? It would be useful to give more details here, as I don’t see how the argument goes through at this point. ","There is complete consensus among the reviewers that the KPCA formulation in this paper needs better motivation; that the paper has technical errors, and the experimental evaluation is not convincing. As such the paper is not up to ICLR standards. The authors are encouraged to revise the paper based on feedback from the reviews."
https://openreview.net/forum?id=BJlxmAKlg,The paper proposes an architecture called ReasoNet that reason over the relation. The paper addresses important tasks but there are many other related works. The comparison to other methods are not comprehensive. The Graph Reachability dataset is not a good example to use.,"This paper introduces a method for estimating the number of iterations of an attention mechanism in a neural machine reading module using REINFORCE and a custom baseline, which is estimated on the data. Estimating a baseline from data, multi-hop attention, and modelling latent variable models with REINFORCE are not new, so their composition, while sensible, is slightly incremental. There were some concerns from several of the reviewers with the impact the experiments have in terms of validating the model changes. Improvements on ""real"" tasks such as CNN/DailyMail did not impress reviewers, some of whom believe the benchmarks compared to were not representative of the best comparable architectures tried on these datasets (e.g. pointer networks, which definitely can be used). Overall, I do not find this paper strong enough to recommend acceptance to the main conference in its current state. With better evaluation, it could be a decently strong paper if the results come through."
https://openreview.net/forum?id=Hkg4TI9xl,"The authors present results on a number of different tasks where the goal is to determine whether a given test example is out-of-domain or likely to be mis-classified. This is accomplished by examining statistics for the softmax probability for the most likely class; although the score by itself is not a particularly good measure of confidence, the statistics for out-of-domain examples are different enough from in-domain examples to allow these to be identified with some certainty. ----------------My comments appear below:--------1. As the authors point out, the AUROC/AUPR criterion is threshold independent. As a result, it is not obvious whether the thresholds that would correspond to a certain operating point (say a true positive rate of 10%) would be similar across different data sets. In other words, it would be interesting to know how sensitive the thresholds are to different test sets (or different splits of the test set). This is important if we want to use the thresholds determined on a given held-out set during evaluation on unseen data (where we would need to select a threshold).----------------2. Performance is reported in terms of AUROC/AUPR and models are compared against a random baseline. I think it’s a little hard to look at the differences in AUC/AUPR to get a sense for how much better the proposed classifier is than the random baseline. It would be useful, for example, if the authors could also report how strongly statistically significant some of these differences are (although admittedly they look to be pretty large in most cases).----------------3. In the experiments on speech recognition presented in Section 3.3, I was not entirely clear on how the model was evaluated. In Table 9, for example, is an “example” the entire utterance or just a single (stacked?) speech frame. Assuming that each “example” is an utterance, are the softmax probabilities the probability of the entire phone sequence (obtained by multiplying the local probability estimates from a Viterbi decoding?)----------------4. I’m curious about the decision to ignore the blank symbol’s logit in Section 3.3. Why is this required?----------------5. As I mentioned in the pre-review question, at least in the speech recognition case, it would have been interesting to compare performance obtained using a simple generative baseline (e.g., GMM-HMM). I think that would serve as a good indication of the ability of the proposed model to detect out-of-domain examples over the baseline.","The paper presents an approach that uses the statistics of softmax outputs to identify misclassifications and/or outliers. The reviewers had mostly minor comments on the paper, which appear to have been appropriately addressed in the revised version of the paper."
https://openreview.net/forum?id=Sy7m72Ogg,"The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.----------------I have two main concerns. One is the lack of comparisons to similar recently proposed methods - ""Learning Step Size Controllers for Robust Neural Network Training"" by Daniel et al. and ""Learning to learn by gradient descent by gradient descent"" by Andrychowicz et al. The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are. Their work does use more prior knowledge as the authors stated, but why is this a bad thing?----------------My second concern is with the experiments. Some of the numbers reported for the other methods are surprisingly low. For example, why is RMSprop so bad in Table 2 and Table 3? These results suggest that the methods are not being tuned properly, which reinforces the need for comparisons on standard architectures with previously reported results. For example, if the baselines used a better architecture like a ResNet or, for simplicty, Network in Network from this list:--------http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html--------there wouldn't be a question of how well they are being tuned. Beating a previously published result on a well known architecture would be much more convincing.","The authors use actor-critic reinforcement learning to adjust the step size of a supervised learning algorithm. There are no comparisons made to other, similar approaches, and the baselines are suspiciously weak, making the proposed method difficult to justify."
https://openreview.net/forum?id=SkCILwqex,"The paper presents a new exciting layerwise origin-target synthesis method both for generating a large number of diverse adversarials as well as for understanding the robustness of various layers. The methodology is then used to visualize the amount of perturbation necessary for producing a change for higher level features.----------------The approach to match the features of another unrelated image is interesting and it goes beyond producing adversarials for classification. It can also generate adversarials for face-recognition and other models where the result is matched with some instance from a database.----------------Pro: The presented approach is definitely sound, interesting and original. --------Con: The analyses presented in this paper are relatively shallow and don't touch the most obvious questions. There is not much experimental quantitative evidence for the efficacy of this method compared with other approaches to produce adversarials. The visualization is not very exciting and it is hard to any draw any meaningful conclusions from them.----------------It would definitely improve the paper if it would present some interesting conclusions based on the new ideas.","This paper studies the effects of modifying intermediate representations arising in deep convolutional networks, with the purpose of visualizing the role of specific neurons, and also to construct adversarial examples. The paper presents experiments on MNIST as well as faces.     The reviewers agreed that, while this contribution presents an interesting framework, it lacks comparisons with existing methods, and the description of the method lacks sufficient rigor. In light of the discussions and the current state of the submission, the AC recommends rejection.     Since the final scores of the reviewers might suggest otherwise, please let me explain my recommendation.     The main contribution of this paper seems to be essentially a fast alternative to the method proposed in 'Adversarial Manipulation of Deep Representations', by Sabour et al, ICLR'16, although the lack of rigor and clarity in the presentation of section 3 makes this assessment uncertain. The most likely 'interpretation' of Eq (3) suggests that eta(x_o, x_t) = nabla_{x_o}( || f^(l)_w(x_t) - f^(l)_w(x_o) ||^2), which is simply one step of gradient descent of the method described in Sabour et al. One reviewer actually asked for clarification on this point on Dec. 26th, but the problem seems to be still present in the current manuscript.     More generally, visualization and adversarial methods based on backpropagation of some form of distance measured in feature space towards the pixel space are not new; they can be traced back to Simoncelli & Portilla '99.   Fast approximations based on simply stopping the gradient descent after one iteration do not constitute enough novelty.     Another instance of lack of clarity that has also been pointed out in this discussion but apparently not addressed in the final version is the so-called PASS measure. It is not defined anywhere in the text, and the authors should not expect the reader to know its definition beforehand.     Besides these issues, the paper does not contribute to the state-of-the-art of adversarial training nor feature visualization, mostly because its experiments are limited to mnist and face datasets. Since the main contribution of the paper is empirical, more emphasis should be made to present experiments on larger, more numerous datasets."
https://openreview.net/forum?id=H1Gq5Q9el,"In this paper, the authors propose to pretrain the encoder/decoder of seq2seq models on a large amount of unlabeled data using a LM objective. They obtain improvements using this technique on machine translation and abstractive summarization.----------------While the effectiveness of pretraining seq2seq models has been known among researchers and explored in a few papers (e.g. Zoph et al. 2016,  Dai and Le 2015), I believe this is the first paper to pretrain using a LM for both the encoder/decoder. The technique is simple, but the gains are large (e.g. +2.7 BLEU on NMT). In addition, the authors perform extensive ablation studies to analyze where the performance is coming from. Hence, I think this paper should be accepted.","This paper effectively demonstrates that the use of pretraining can improve the performance of seq2seq models for MT and summarization tasks. However, despite these empirical gains, the reviewers were not convinced enough by the novelty of the work itself and did not feel like there were technical contributions to make this a fit for ICLR.    Pros:  - All reviewers agree that the empirical gains in this paper are convincing and lead to BLEU improvements on a large scale translation and translation like tasks. Reviewer 4 also praises the detailed analysis that demonstrates that these gains come from the pretraining process itself.   - From an impact perspective, the reviewers found the approach clear and implementable.     Cons:  - Novelty criticisms are that the method is a ""compilation"" of past approaches (although at a larger scale) and therefore primarily experimental, and that the objectives given are ""highly empirical"" and not yet motivated by theory. The authors did respond, but the reviewer did not change their score.  - There are suggestions that this type of work would perhaps be more widely impactful in an NLP venue, where a BLEU improvement of this regard is a strong supporting piece on its own."
https://openreview.net/forum?id=Byk-VI9eg,"This work brings multiple discriminators into GAN. From the result, multiple discriminators is useful for stabilizing. ----------------The main problem of stabilizing seems is from gradient signal from discriminator, the authors motivation is using multiple discriminators to reduce this effect.----------------I think this work indicates the direction is promising, however I think the authors may consider to add more result vs approach which enforce discriminator gradient, such as GAN with DAE (Improving Generative Adversarial Networks with Denoising Feature Matching), to show advantages of multiple discriminators.","Using an ensemble in the discriminator portion of a GAN is a sensible idea, and it is well explored and described in this paper. Further clarification and exploration of how the multiple discriminators are combined (max versus averaging versus weighted averaging) would be good. The results are fairly strong, across a variety of datasets."
https://openreview.net/forum?id=r1G4z8cge,"The authors show that the idea of smoothing a highly non-convex loss function can make deep neural networks easier to train.----------------The paper is well-written, the idea is carefully analyzed, and the experiments are convincing, so we recommend acceptance. For a stronger recommendation, it would be valuable to perform more experiments. In particular, how does your smoothing technique compare to inserting probes in various layers of the network? Another interesting question would be how it performs on hard-to-optimize tasks such as algorithm learning. For example, in the ""Neural GPU Learns Algorithms"" paper the authors had to relax the weights of different layers of their RNN to make it optimize -- could this be avoided with your smoothing technique?","The paper presents a nice idea for using a sequence of progressively more expressive neural networks to train a model. Experiments are shown on CIFAR10, parity, language modeling to show that the methods performs well on these tasks.  However, as noted by the reviewers, the experiments do not do a convincing enough job. For example, the point of the model is to show that optimization can be made easier by their concept, however, results are presented on depths that are considered shallow these days. The results on PTB are also very far from SOTA. However, because of the novelty of the idea, and because of the authors ratings, I'm giving the paper a pass. I strongly encourage the authors to revise the paper accordingly for the camera ready version.    Pros:  - interesting new idea  - shows gains over simple baselines.  Cons:  - not a very easy read, I think the paper was unnecessarily dense exposition of a relatively simple idea.  - experiments are not very convincing for the specific type of problem being addressed."
https://openreview.net/forum?id=ryelgY5eg,"The author attacks the problem of shallow binary autoencoders using a minmax game approach. The algorithm, though simple, appears to be very effective. The paper is well written and has sound analyses. Although the work does not extend to deep networks immediately, its connections with other popular minmax approaches (eg GANs) could be fruitful in the future.",All reviewers (weakly) support the acceptance of the paper. I also think that binary neural networks model is an important direction to explore.
https://openreview.net/forum?id=Hk4_qw5xe,"This paper makes a valuable contribution to provide a more clear understanding of generative adversarial network (GAN) training procedure. ----------------With the new insight of the training dynamics of GAN, as well as its variant, the authors reveal the reason that why the gradient is either vanishing in original GAN or unstable in its variant. More importantly, they also provide a way to avoid such difficulties by introducing perturbation. I believe this paper will inspire more principled research in this direction. ----------------I am very interested in the perturbation trick to avoid the gradient instability and vanishment. In fact, this is quite related to dropout trick in where the perturbation can be viewed as Bernoulli distribution. It will be great if the connection can be discussed.  Besides the theoretical analysis, is there any empirical study to justify this trick? Could you please add some experiments like Fig 2 and 3 for the perturbated GAN for comparison? ","The paper provides a detailed analysis of the instability issues surrounding the training of GANs. They demonstrate how perturbations can help with improving stability. Given the popularity of GANs, this paper is expected to have a significant impact."
https://openreview.net/forum?id=H1VyHY9gg,"This paper proposes a data noising technique for language modeling. The main idea is to noise a word history by using a probabilistic distribution based on N-gram smoothing techniques. The paper is clearly written and shows that such simple techniques improve the performance in various tasks including language modeling and machine translation. May main concern is that the method is too simple and sounds ad hoc, e.g., there is no theoretical justification of why n-gram smoothing based data noising would be effective for recurrent neural network based language modeling.----------------Comments: --------- p. 3 “can be seen has a way” -> “can be seen as a way” (?)--------- p. 3. In general, the explanation about blank noising should be improved. Why does it avoid overfitting on specific contexts?--------- p. 4. It would be better to provide more detailed derivations for a general form of unigram and blank noising equations.--------- p. 5, Section 3.6: Is there any discussions about noising either/both input and output sequences with some numbers? This would be helpful information.","The reviewers are reasonably supportive of this paper. The ideas presented in the paper are nice and the results are encouraging. The authors should consider, for the final version of this work, providing comparisons to other approaches on the text8 corpus (or on the 1 Billion Words corpus, Chelba et al.)."
https://openreview.net/forum?id=S1LVSrcge,"TLDR: The authors present Variable Computation in Recurrent Neural Networks (VCRNN). VCRNN is similar in nature to Adaptive Computation Time (Graves et al., 2016). Imagine a vanilla RNN, at each timestep only a subset (i.e., ""variable computation"") of the state is updated. Experimental results are not convincing, there is limited comparison to other cited work and basic LSTM baseline.----------------=== Gating Mechanism ===--------At each timestep, VCRNN generates a m_t vector which can be seen as a gating mechanism.  Based off this m_t vector, a D-first (D-first as in literally the first D RNN states) subset of the vanilla RNN state is gated to be updated or not. Extra hyperparams epsilon and \bar{m} are needed -- authors did not give us a value or explain how this was selected or how sensitive and critical these hyperparms are.----------------This mechanism while novel, feels a bit clunky and awkward. It does not feel well principled that only the D-first states get updated, rather than a generalized solution where any subset of the state can be updated.----------------A short section in the text comparing to the soft-gating mechanisms of GRUs/LSTMs/Multiplicative RNNs (Wu et al., 2016) would be nice as well.----------------=== Variable Computation ===--------One of the arguments made is that their VCRNN model can save computation versus vanilla RNNs. While this may be technically true, in practice this is probably not the case. The size of the RNNs they compare to do not saturate any modern GPU cores. In theory computation might be saved, but in practice there will probably be no difference in wallclock time. The authors also did not report any wallclock numbers, which makes this argument hard to sell.----------------=== Evaluation ===--------This reviewer wished there was more citations to other work for comparison and a stronger baseline (than just a vanilla RNN). First, LSTMs are very simple and quite standard nowadays -- there is a lack of comparison to any basic stacked LSTM architecture in all the experiments.----------------The PTB BPC numbers are quite discouraging as well (compared to state-of-the-art). The VCRNN does not beat the basic vanilla RNN baseline. The authors also only cite/compare to a basic RNN architecture, however there has been many contributions since a basic RNN architecture that performs vastly better. Please see Chung et al., 2016 Table 1. Chung et al., 2016 also experimented w/ PTB BPC and they cite and compare to a large number of other (important) contributions.----------------One cool experiment the authors did is graph the per-character computation of VCRNN (i.e., see Figure 2). It shows after a space/word boundary, we use more computation! Cool! However, this makes me wonder what a GRU/LSTM does as well? What is the magnitude of the of the change in the state vector after a space in GRU/LSTM -- I suspect them to do something similar.----------------=== Minor ===--------* Please add Equations numbers to the paper, hard to refer to in a review and discussion!----------------References--------Chung et al., ""Hierarchical Multiscale Recurrent Neural Networks,"" in 2016.--------Graves et al., ""Adaptive Computation Time for Recurrent Neural Networks,"" in 2016.--------Wu et al., ""On Multiplicative Integration with Recurrent Neural Networks,"" in 2016.","This paper describes a new way of variable computation, which uses a different number of units depending on the input. This is different from other methods for variable computation that compute over multiple time steps. The idea is clearly presented and the results are shown on LSTMs and GRUs for language modeling and music prediction.    Pros:  - new idea  - convincing results in a head to head comparison between different set ups.    Cons:  - results are not nearly as good as the state of the art on the reported tasks.    The reviewers and I had several rounds of discussion on whether or not to accept the paper. One reviewer had significant reservations about the paper since the results were far from SOTA. However, since getting SOTA often requires a combination of several tricks, I felt that perhaps it would not be fair to require this, and gave them the benefit of doubt (especially because the other two reviewers did not think this was a dealbreaker). In my opinion, the authors did a fair enough job on the head to head comparison between their proposed VCRNN models and the underlying LSTMs and GRUs, which showed that the model did well enough."
https://openreview.net/forum?id=ryPx38qge,"The paper investigates a hybrid network consisting of a scattering network followed by a convolutional network. By using scattering layers, the number of parameters is reduced, and the first layers are guaranteed to be stable to deformations. Experiments show that the hybrid network achieves reasonable performance, and outperforms the network-in-network architecture in the small-data regime.----------------I have often heard researchers ask why it is necessary to re-learn low level features of convolutional networks every time they are trained. In theory, using fixed features could save parameters and training time. As far as I am aware, this paper is the first to investigate this question. In my view, the results show that using scattering features in the bottom layers does not work as well as learned CNN features. This is not completely obvious a priori, and so the results are interesting, but I disagree with the framing that the hybrid network is superior in terms of generalization.----------------For the low-data regime, the hybrid network sometimes gives better accuracy than NiN, but this is quite an old architecture and its capacity has not been tuned to the dataset size. For the full dataset, the hybrid network is clearly outperformed by fully learned models. If I understood correctly, the authors have not simply compared identical architectures with and without scattering as the first layers, which further complicates drawing a conclusion.----------------The authors claim the hybrid network has the theoretical advantage of stability. However, only the first layers of a hybrid network will be stable, while the learned ones can still create instability. Furthermore, if potentially unstable deep networks outperform stable scattering nets and partially-stable hybrid nets, we have to question the importance of stability as defined in the theory of scattering networks. ----------------In conclusion, I think the paper investigates a relevant question, but I am not convinced that the hybrid network really generalizes better than standard deep nets. Faster computation (at test time) could be useful e.g. in low power and mobile devices, but this aspect is not really fleshed out in the paper.----------------Minor comments:---------section 3.1.2: “learni”","The program committee appreciates the authors' response to concerns raised in the reviews. Reviewers are generally excited about the combination of predefined representations with CNN architectures, allowing the model to generalize better in the low data regime. This was an extremely borderline paper, and the PCs have determined that the paper would have needed to be further revised and should be rejected."
https://openreview.net/forum?id=S1OufnIlx,"The paper is well motivated and well written. The setting of the experiments is to investigate a particular case. While the results of experiments are interesting, such investigation is not likely to systematically improve our understanding of the adversarial example phenomenon. Overall, the contribution of the paper seems incremental. ----------------Pros:--------1. This paper proposes the iterative LL method, which is efficient in both computation and success rate in generating adversarial examples. This method could be useful when the number of classes in the dataset is huge.--------2. Some observations of the experiments are interesting. For example, overall photo transformation does not affect much the accuracy on clean image, but could destroy some adversarial methods. ----------------Cons:--------1. As noticed by the authors, some similar works exist in the literature. According to the authors, what differs this work from other existing works is that this paper tend to fool NN by making very small perturbations of the input. But based on the experiments and the demonstration (the real pictures), it is arguable that the perturbations in the experiments are still small.--------2. Some hypotheses proposed in the paper based on one-shot experiments seems too rushy.--------3. As mentioned above, the results of this paper seems not really improving the understanding of the adversarial example phenomenon.","This paper studies an interesting aspect of adversarial training with important practical applications: its robustness against transformations that correspond to physical world constraints. The paper demonstrates how to construct adversarial examples such that they can be used to attack a machine-learning device through a physical interface such as a camera device.     The reviewers agreed that this is a well-written paper which clearly describes its contributions and offers a detailed experimental section. The authors took into account the reviewer's concerns and the rebuttal phase offered interesting discussions.     In light of the reviews, the main critique of this work is its lack of significance, relative to existing works in adversarial examples. The authors, however, did a good job during the rebuttal phase to highlight the empirical nature of the work and the potential practical significance of their findings in the design of ML models. The AC concludes that the potential practical applications, while not significant enough to be part of the conference proceedings, are worthy to be disseminated in the workshop. I therefore recommend submitting this work to the workshop track."
https://openreview.net/forum?id=S1dIzvclg,"The authors of the paper set out to answer the question whether chaotic behaviour is a necessary ingredient for RNNs to perform well on some tasks. For that question's sake, they propose an architecture which is designed to not have chaos. The subsequent experiments validate the claim that chaos is not necessary.----------------This paper is refreshing. Instead of proposing another incremental improvement, the authors start out with a clear hypothesis and test it. This might set the base for future design principles of RNNs.----------------The only downside is that the experiments are only conducted on tasks which are known to be not that demanding from a dynamical systems perspective; it would have been nice if the authors had traversed the set of data sets more to find data where chaos is actually necessary.","The reviewers all enjoyed this paper and the analysis.    pros:  - novel new model  - interesting insights into the design of model, through analysis of trajectories of hidden states of RNNs.    cons:  - results are worse than LSTMs."
https://openreview.net/forum?id=HJWzXsKxx,"This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. This observation is novel (although not too surprising) to my knowledge, but I must state that I am not very familiar with research on fast RNN implmentations.----------------Minor note:--------The LSTM language model does not use a 'word2vec' layer. It is simply a linear embedding layer. Word2vec is the name of a specific model which is not directly to character level language models.----------------The paper presents the central observation clearly. However, it will be much more convincing if a well known dataset and experiment set up are used, such as Graves (2013) or Sutskever et al (2014), and actual training, validation and test performances are reported.----------------While the main observation is certainly interesting, I think it is not sufficient to be the subject of a full conference paper without implementation (or simulation) and benchmarking of the promised speedups on multiple tasks. For example, how would the gains be affected by various architecture choices?----------------At present this is an interesting technical report and I would like to see more detailed results in the future.","The main point of the paper was that sparsifying gradients does not hurt performance; however, this in itself is not enough for a publication in this venue. As noted by R1 and R2, showing how this can help in more energy efficient training would make for a good paper; without that aspect the paper only presents an observation that is not too surprising to the practitioners in this area.    Further, while the main point of the paper was relatively clear, the scientific presentation was not rigorous enough. All the reviewers point out that several details were missing (including test set performance, reporting of results on the different sets, etc).     Paper would be strengthened by a better exploration of the problem."
https://openreview.net/forum?id=Bk3F5Y9lx,"This paper is refreshing and elegant in its handling of ""over-sampling"" in VAE. Problem is that good reconstruction requires more nodes in the latent layers of the VAE. Not all of them can or should be sampled from at the ""creative"" regime of the VAE. Which ones to choose? The paper offers and sensible solution. Problem is that real-life data-sets like CIFAR have not being tried, so the reader is hard-pressed to choose between many other, just as natural, solutions. One can e.g. run in parallel a classifier and let it choose the best epitome, in the spirit of spatial transformers, ACE, reference [1]. The list can go on. We hope that the paper finds its way to the conference because it addresses an important problem in an elegant way, and papers like this are few and far between!----------------On a secondary note, regarding terminology: Pls avoid using ""the KL term"" as in section 2.1, there are so many ""KL terms"" related to VAE-s, it ultimately gets out of control. ""Generative error"" is a more descriptive term, because minimizing it is indispensable for the generative qualities of the net. The variational error for example is also a ""KL term"" (equation (3.4) in reference [1]), as is the upper bound commonly used in VAE-s (your formula (5) and its equivalent - the KL expression as in formula (3.8) in reference [1]). The latter expression is frequently used and is handy for, say, importance sampling, as in reference [2].----------------[1] https://arxiv.org/pdf/1508.06585v5.pdf--------[2] https://arxiv.org/pdf/1509.00519.pdf","This paper addresses issues faced when using VAEs and the pruning of latent variables. Improvements to the paper have been accounted for and improved the paper, but after considering the rebuttal and discussion, the reviewers still felt that more was needed, especially in terms of applicability across multiple different data sets. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings."
https://openreview.net/forum?id=HyET6tYex,"The authors explore whether the halting time distributions for various algorithms in various settings exhibit ""universality"", i.e. after rescaling to zero mean and unit variance, the distribution does not depend on stopping parameter, dimensionality and ensemble.----------------The idea of the described universality is very interesting. However I see several shortcomings in the paper:----------------In order to be of practical relevance, the actual stopping time might be more relevant than the scaled one. The discussion of exponential tailed halting time distributions is a good start, but I am not sure how often this might be actually helpful. Still, the findings in the paper might be interesting from a theoretical point of view.----------------Especially for ICLR, I think it would have been more interesting to look into comparisons between stochastic gradient descent, momentum, ADAM etc on different deep learning architectures. Over which of those parameters does universality hold?. How can different initializations influence the halting time distribution? I would expect a sensible initialization to cut of part of the right tail of the distribution.----------------Additionally, I found the paper quite hard to read. Here are some clarity issues:----------------- abstract: ""even when the input is changed drastically"": From the abstract I'm not sure what ""input"" refers to, here--------- I. Introduction: ""where the stopping condition is, essentially, the time to find the minimum"": this doesn't seem to make sense, a condition is not a time. I guess the authors wanted to say that the stopping condition is that the minimum has been reached?--------- I.1 the notions of dimension N, epsilon and ensemble E are introduced without any clarification what they are. From the later parts of the paper I got some ideas and examples, but here it is very hard to understand what these parameters should be (just some examples would be already helpful)--------- I.3 ""We use x^\ell for \ell \in Z=\{1, \dots, S\} where Z is a random sample from of training samples"" This formulation doesn't make sense. Either Z is a random sample, or Z={1, ..., S}.--------- II.1 it took me a long time to find the meaning of M. As this parameter seems to be crucial for universality in this case, it would be very helpful to point out more explicitly what it refers to.",My overall conclusion is that the paper needs more work to be sufficiently convincing. I think the reviews are sufficiently careful. My recommendation is based on the fact that none of the reviewers supports acceptance.
https://openreview.net/forum?id=HJTXaw9gx,"Approximating solutions to PDEs with NN approximators is very hard. In particular the HJB and HJI eqs have in general discontinuous and non-differentiable solutions making them particularly tricky (unless the underlying process is a diffusion in which case the Ito term makes everything smooth, but this paper doesn't do that). What's worse, there is no direct correlation between a small PDE residual and a well performing-policy [tsitsiklis? beard? todorov?, I forget]. There's been lots of work on this which is not properly cited. ----------------The 2D toy examples are inadequate. What reason is there to think this will scale to do anything useful? ----------------There are a bunch of typos (""Range-Kutta""?) .----------------More than anything, this paper is submitted to the wrong venue. There are no learned representations here. You're just using a NN. That's not what ICLR is about. Resubmit to ACC, ADPRL or CDC.----------------Sorry for terseness. Despite rough review, I absolutely love this direction of research. More than anything, you have to solve harder control problems for people to take notice...","The basic approach of this paper is to use a neural net to sequentially generate points that can be used as the basis points in a PDE solver. The idea is definitely an interesting one, and all three reviewers are in agreement that the approach does seem to have a lot of potential.    The main drawback of the paper, simply, is that it's unclear whether this result would be of sufficient interest for the ICLR audience. Ultimately, it seems as though the authors are simply training a neural network to generate this points, and the interesting contribution here comes from the application to PDE solving, not really from any advance in the NN/ML side itself. As such, it seems like the paper would be better appreciated (as a full conference paper or journal paper), within the control community, rather than ICLR. However, I do think that as an application, many at ICLR would be interested in seeing this work, even if its likely to have relatively low impact on the community. Thus, I think the best avenue for this paper is probably as a workshop post at ICLR, hopefully with further submission and exposure in the controls community.    Pros:  + Nice application of ML to a fun problem, generating sample points for PDE solutions  + Overall well-written and clearly presented    Cons:  - Unclear contribution to the actual ML side of things  - Probably better suited to controls conferences"
https://openreview.net/forum?id=SkqMSCHxe,"Authors propose a competitive learning architecture that learn different RNN predictors independently, akin to a committee of experts which are chosen with a hard switch at run-time. This work is applied to the task of predictive different driving behaviors from human drivers, and combines behaviors at test time, often switching behaviors within seconds. Prediction loss is lower than the similar but non-competitive architecture used as a baseline.--------It is not very clear how to interpret the results, what is the real impact of the model. If behaviors switch very often, can this really be seen as choosing the best driving mode for a given situation? Maybe the motivation needs to be rephrased a little to be more convincing?--------The competitive approach presented is interesting but not really novel, thus the impact of this paper for a conference such as ICLR may be limited.","The authors present a prediction framework that involves multiple 'competitive' RNNs, and they claim that they are predicting human intention. It is unclear if this method, which seems quite ad-hoc, is any different from a simple ensemble approach, and it is unclear that the model is predicting human intention. The experiments do not adequately demonstrate either."
https://openreview.net/forum?id=HkvS3Mqxe,"This paper proposes a simple randomized algorithm for selecting which weights in a ConvNet to prune in order to reduce theoretical FLOPs when evaluating a deep neural network. The paper provides a nice taxonomy or pruning granularity from coarse (layer-wise) to fine (intra-kernel). The pruning strategy is empirically driven and uses a validation set to select the best model from N randomly pruned models. Makes claims in the intro about this being ""one shot"" and ""near optimal"" that cannot be supported: it is ""N-shot"" in the sense that N networks are generated and tested and there is no evidence or theory that the found solution is ""near optimal.""----------------Pros:--------- Nice taxonomy of pruning levels--------- Comparison to the recent weight-sum pruning method----------------Cons:--------- Experimental evaluation does not touch upon recent models (ResNets) and large scale datasets (ImageNet)--------- Paper is somewhat hard to follow--------- Feature map pruning can obviously accelerate computation without specialized sparse implementations of convolution, but this is not the case for finer grained sparsity; since this paper considers fine-grained sparsity it should provide some evidence that introducing that sparsity can yield performance improvements----------------Another experimental downside is that the paper does not evaluate the impact of filter pruning on transfer learning. For example, there is not much direct interest in the tasks of MNIST, CIFAR10, or even ImageNet. Instead, a main interest in both academia and industry is the value of the learned representation for transferring to other tasks. One might expect pruning to harm transfer learning. It's possible that the while the main task has about the same performance, transfer learning is strongly hurt. This paper has missed an opportunity to explore that direction.----------------In summary, the proposed method is simple, which is good, but the experimental evaluation is somewhat incomplete and does not cover recent models and larger scale datasets.","Unfortunately this paper is not competitive enough for ICLR. The paper is focusing on efficiency where for the results to be credible it is of utmost importance to present experiments on large scale data and state-of-the-art models.  I am afraid the reviewers were not able to take into account the latest drafts of the paper that were submitted in January, but those submissions came very late."
https://openreview.net/forum?id=HkvS3Mqxe,"Summary: There are many different pruning techniques to reduce memory footprint of CNN models, and those techniques have different granularities (layer, maps, kernel or intra kernel), pruning ratio and sparsity of representation. The work proposes a method to choose the best pruning masks out to many trials. Tested on CIFAR-10, SVHN and MNIST.----------------Pros:--------Proposes a method to choose pruning mask out of N trials. --------Analysis on different pruning methods.----------------Cons & Questions:--------“The proposed strategy selects the best pruned network through N random pruning trials. This approach enables one to select pruning mask in one shot and is simpler than the multi-step technique.” How can one get the best pruning mask in one shot if you ran N random pruning trials? (answered)--------Missing tests of the approach with bigger CNN: like AlexNet, VGG, GoogLeNet or ResNet. (extended to VGG ok)--------Since reducing model size for embedded systems is the final goal, then showing how much memory space in MB is saved with the proposed technique compared with other approaches like Han et al. (2015) would be good.----------------Misc:--------Typo in figure 6 a) caption: “Featuer” (corrected)","Unfortunately this paper is not competitive enough for ICLR. The paper is focusing on efficiency where for the results to be credible it is of utmost importance to present experiments on large scale data and state-of-the-art models.  I am afraid the reviewers were not able to take into account the latest drafts of the paper that were submitted in January, but those submissions came very late."
https://openreview.net/forum?id=SJQNqLFgl,"The authors have grouped recent work in convolutional neural network design (specifically with respect to image classification) to identify core design principles guiding the field at large. The 14 principles they produce (along with associated references) include a number of useful and correct observations that would be an asset to anyone unfamiliar with the field. The authors explore a number of architectures on CIFAR-10 and CIFAR-100 guided by these principles.----------------The authors have collected a quality set of references on the subject and grouped them well which is valuable for young researchers. Clearly the authors explored a many of architectural changes as part of their experiments and publicly available code base is always nice.----------------Overall the writing seems to jump around a bit and the motivations behind some design principles feel lost in the confusion. For example, ""Design Pattern 4: Increase Symmetry argues for architectural symmetry as a sign of beauty and quality"" is presented as one of 14 core design principles without any further justification. Similarly ""Design Pattern 6: Over-train includes any training method where the network is trained on a harder problem than necessary to improve generalization performance of inference"" is presented in the middle of a paragraph with no supporting references or further explanation.----------------The experimental portion of this paper feels scattered with many different approaches being presented based on subsets of the design principles. In general, these approaches either are minor modifications of existing networks (different FractalNet pooling strategies) or are novel architectures that do not perform well. The exception being the Fractal-of-Fractal network which achieves slightly improved accuracy but also introduces many more network parameters (increased capacity) over the original FractalNet.------------------------Preliminary rating:--------It is a useful and perhaps noble task to collect and distill research from many sources to find patterns (and perhaps gaps) in the state of a field; however, some of the patterns presented do not seem well developed and include principles that are poorly explained. Furthermore, the innovative architectures motivated by the design principles either fall short or achieve slightly better accuracy by introducing many more parameters (Fractal-Of-Fractal networks). For a paper addressing the topic of higher level design trends, I would appreciate additional rigorous experimentation around each principle rather than novel architectures being presented.",The authors agree with the reviewers that this manuscript is not yet ready.
https://openreview.net/forum?id=BysZhEqee,"The proposed approach consists in a greedy layer wise initialization strategy for a deep MLP model, which is followed by global gradient-descent with dropout for fine-tuning. The initialization strategy uses a first randomly initialized sigmoid layer for dimensionality expansion followed by 2 sigmoid layers whose weights are initialized by Marginal Fisher Analysis (MFA) which learns a linear dimensionality reduction based on a neighborhood graph constructed using class label information (i.e. supervised dimensionality reduction). Output layer is a standard softmax layer.----------------The approach is thus to be added to a growing list of heuristic layer-wise initialization schemes.--------The particular choice of initialization strategy, while reasonable, is not sufficiently well motivated in the paper relative to alternatives, and thus feels rather arbitrary.--------The paper lacks clarity in the description of the approach:  MFA is poorly explained with undefined notations (in Eq. 4, what is A? It has not been properly defined); the precise use of alluded denoising in the model is also unclear (is there really training of an additional denoting objective, or just input corruption?).----------------The question of the (arguably mild) inconsistency of applying a linear dimensionality reduction algorithm, that is trained without any sigmoid, and then passing its learned representation through a sigmoid is not even raised. This, in addition to the fact that sigmoid hidden layers are no longer commonly used (why did you not also consider using RELUs?).----------------More importantly I suspect methodological problems with the experimental comparisons: the paper mentions using *default* values for learning-rate and momentum, and having (arbitrarily?) fixed epoch to 400 (no early stopping?) and L2 regularization to 1e-4 for some models. --------*All* hyper parameters should always be properly hyper-optimized using a validation set (or cross-validation) including early-stopping, and this separately for each model under comparison (ideally also including layer sizes). This is all the more important since you are considering smallish datasets, so that the various initialization strategies act mainly as different indirect regularization schemes: they thus need to be carefully tuned. This casts serious doubts as to the amount of hyper-parameter tuning (close to none?) that went into training the alternative models used for comparison. ----------------The Marginal Fisher Analysis dimensionality reduction initialization strategy may well offer advantages, but as it currently stands this paper doesn’t yet make a sufficiently convincing case for it, nor provide useful insights into the nature of the expected advantages.----------------I would also suggest, for image inputs such as CIFAR10, to use the qualitative tool of showing the filters (back projected to input space) learned by the different initialization schemes under consideration, as this could help visually gain insight as to what sets methods apart. ",The reviewers unanimously recommend rejection.
https://openreview.net/forum?id=HJy_5Mcll,"Paper summary: this work presents ENet, a new convnet architecture for semantic labeling which obtains comparable performance to the previously existing SegNet while being ~10x faster and using ~10x less memory. ------------------------Review summary: Albeit the results seem interesting, the paper lacks detailed experimental results, and is of limited interest for the ICLR audience.------------------------Pros:--------* 10x faster--------* 10x smaller--------* Design rationale described in detail------------------------Cons:--------* The quality of the reference baseline is low. For instance, cityscapes results are 58.3 IoU while state of the art is ~80 IoU. Thus the results are of limited interest.--------* The results that support the design rationale are not provided. It is important to provide the experimental evidence to support each claim.------------------------Quality: the work is interesting but feels incomplete. If your model is 10x faster and smaller, why not try build a model 10x longer to obtain improved results ? The paper focuses only on  nimbleness at the cost of quality (using a weak baseline). This limits the interest for the ICLR audience.------------------------Clarity: the overall text is somewhat clear, but the model description (section 3) could be more clear. ------------------------Originality: the work is a compendium of “practitioners wisdom” applied to a specific task. It has thus limited originality.------------------------Significance: I find the work that establishes a new “best practices all in one” quite interesting, but however these must shine in all aspects. Being fast at the cost of quality, will limit the impact of this work.------------------------Minor comments:--------* Overall the text is proper english but the sentences constructions is often unsound, specific examples below. --------* To improve the chances of acceptance, I invite the authors to also explore bigger models and show that the same “collected wisdom” can be used both to reach high speed and high quality (with the proper trade-off curve being shown). Aiming for only one end of the quality versus speed curve limits too much the paper.--------* Section 1: “mobile or battery powered … require rates > 10 fps“. 10 fps with which energy budget ? Should not this be  > 10 fps && < X Watt.--------* “Rules and ideas” -> rules seem too strong of a word, “guidelines” ?--------* “Is of utmost importance” -> “is of importance” (important is already important)--------* “Presents a trainable network … therefore we compare to … the large majority of inference the same way”; the sentence makes no sense to me, I do not see the logical link between before and after “therefore”--------* Scen-parsing -> scene-parsing--------* It is arguable if encoder and decoder can be called “separate”--------* “Unlike in Noh” why is that relevant ? Make explicit or remove--------* “Real-time” is vague, you mean X fps @ Y W ?--------* Other existing architectures -> Other architectures--------* Section 3, does not the BN layer include a bias term ? Can you get good results without any bias term ?--------* Table 1: why is the initial layer a downsampling one, since the results has half the size of the input ?--------* Section 4, non linear operations. What do you mean by “settle to recurring pattern” ?--------* Section 4, dimensionality changes. “Computationally expensive”, relative to what ?--------* Section 4, dimensionality changes. “This technique ... speeds-up ten times”, but does not provide the same results. Without an experimental validation changing an apple for an orange does not make the orange better than the apple.--------* Section 4, dimensionality changes. “Found one problem”, problem would imply something conceptually wrong. This is more an “issue” or an “miss-match” when using ResNet for semantic labelling.--------* Section 4, factorizing filters. I am unsure of why you call nx1 filter asymmetric. A filter could be 1xn yet be symmetric (e.g. -2 -1 0 1 2). Why not simply call them rectangular filters ?--------* Section 4, factorizing filters. Why would this change increase the variety ? I would have expected the opposite.--------* Section 4, regularization. Define “much better”.--------* Section 5.1; “640x360 is adequate for practical applications”; for _some_ applications.--------* Section 5.2, “very quickly” is vague and depends on the reader expectations, please be quantitative.--------* Section 5.2, Haver -> have--------* Section 5.2, in this work -> In this work--------* Section 5.2, unclear what you use the class weighting for. Is this for class balancing ?--------* Section 5.2, Cityscapes was -> Cityscapes is--------* Section 5.2, weighted by the average -> is each instance weighted relative the average object size.--------* Section 5.2, fastest model in the Cityscapes -> fastest model in the public Cityscapes",Three knowledgable reviewers recommend rejection and there was no rebuttal. The AC agrees with the reviewers.
https://openreview.net/forum?id=BJ5UeU9xx,"The authors of this work propose an interesting approach to visualizing the predictions made by a deep neural network. The manuscript is well written is provides good insight into the problem. I also appreciate the application to medical images, as simply illustrating the point on ImageNet isn't interesting enough. I do have some questions and comments.--------1.  As the authors correctly point out in 3.1, approximating the conditional probability of a feature x_i by the marginal distribution p(x_i) is not realistic. They advocate for translation invariance, i.e. the position of the pixel in the image shouldn't affect the probability, and suggest that the pixels appearance depends on the small neighborhood around it. However, it is well known that global context makes an big impact on the semantics of pixels. In ""Objects in Contexts"", authors show that a given neighborhood of pixels can take different semantic meanings based on the global context in the image. In the context of deep neural networks, works such as ""ParseNet"" also illustrate the importance of global context on the spatial label distribution. This does not necessarily invalidate this approach, but is a significant limitation. It would be great if the authors provided a modification to (4) and empirically verified the change.----------------2. Figure 7 shows the distribution over top 3 predictions before and after softmax. It is expected that even fairly uniform distributions will transform toward delta functions after softmax normalization. Is there an additional insight here?----------------4. Finally, in 4.1, the authors state that it takes 30 minutes to analyze a single image with GooLeNet on a GPU? Why is this so computationally expensive? Such complexity seems to make the algorithm impractical and analyzing datasets of statistical relevance seems prohibitive. ","Reviewers felt the paper was clearly written with necessary details given, leading to a paper that was pleasant to read. The differences with prior art raised by one of the reviewers were adequately addressed by the authors in a revision. The paper presents results on both ImageNet and medical imagery, an aspect of the paper that was appreciated by reviewers."
https://openreview.net/forum?id=HyWG0H5ge,"It is interesting to derive such a bound and show it satisfies a regret bound along with empirical evidence on the CIFAR-10 for cross entropy loss and auto encoder for MSE loss. At least empirically, by comparing the observed training loss and taylor loss, the better a particular optimizer performs (training loss statement, not a validation or observed test statement) the smaller the difference between these two. Also shown is the regret loss is satisfied at different scales of the network, by layer, neuron and whole network. ----------------The Taylor approximation can be used to investigate activation configurations of the network, and used to connect this to difficulty in optimizing  at kinks in the loss surface, along with an empirical study of exploration of activation surface of the SGD/Adam/RMSprop optimizers, the more exploration the better the resulting training loss.----------------Not that it impacts the paper but the weaker performance of the SGD could be related to the fixed learning rate, if we anneal this learning rate, which should improve performance, does this translate to more exploration and tightening between the actual loss and the Taylor loss? ----------------- It might be useful to use a cross validation set for some of the empirical studies, in the end we would like to say something about generalization of the resulting network----------------- Is there a reason the subscript on the Jacobian changes to a_l in the <Gl,V> definition?","The paper presents a theoretical analysis of the convergence of the training error. The presented result is rather general and can potentially apply to many neural networks.     Reviewers pointed out several important concerns regarding the mathematical rigor and precision of the claims. The authors' response partially addressed concerns about the scope of the claims and unclear arguments of the proofs.     We invite the authors to submit a revision of the paper, where all statements and proofs are mathematically clear, to the workshop track."
https://openreview.net/forum?id=r1GKzP5xx,"The authors show how the hidden states of an LSTM can be normalised in order to preserve means and variances. The method’s gradient behaviour is analysed. Experimental results seem to indicate that the method compares well with similar approaches.----------------Points----------------1) The writing is sloppy in parts. See at the end of the review for a non-exhaustive list.----------------2) The experimental results show marginal improvements, of which the the statistical significance is impossible to asses. (Not completely the author’s fault for PTB, as they partially rely on results published by others.) Weight normalisation seems to be a viable alternative in the: the performance and runtime are similar. The implementation complexity of weight norm is, however, arguably much lower. More effort could have been put in by the authors to clear that up. In the current state, practitioners as well as researchers will have to put in more effort to judge whether the proposed method is really worth it for them to replicate.----------------3) Section 4 is nice, and I applaud the authors for doing such an analysis.------------------------List of typos etc.----------------- maintain -> maintain--------- requisits -> requisites--------- a LSTM -> an LSTM--------- ""The gradients of ot and ft are equivalent to equation 25.” Gradients cannot be equivalent to an equation.--------- “beacause""-> because--------- One of the γx > γh at the end of page 5 is wrong.","Paper proposes a modification of batch normalization. After the revisions the paper is a much better read. However it still needs more diverse experiments to show the success of the method.    Pros:  - interesting idea with interesting analysis of the gradient norms  - claims to need less computation    Cons:  - Experiments are not very convincing and only focus on only a small set of lm tasks.  - The argument for computation gain is not convincing and no real experimental evidence is presented. The case is made that in speech domain, with long sequences this should help, but it is not supported.    With more experimental evidence the paper should be a nice contribution."
https://openreview.net/forum?id=H178hw9ex,"I sincerely apologize for the late review!----------------The first part has a strong emphasis on the technical part. It could benefit from some high level arguments on what the method aims to achieve, what limitation is there to overcome. I may have misunderstood the contribution (in which case please correct me) that the main novel part of the paper is the suggestion to learn the group parameterizations instead of pre-fixing them. So instead of applying it to common spatial filters as in De Brabandere et al., it is applied to Steerable Frames?----------------The first contribution suggests that ""general frame bases are better suited to represent sensory input data than the commonly used pixel basis."". The experiments on Cifar10+ indicate that this is not true in general. Considering the basis as a hyper-parameter, expensive search has to be conducted to find that the Gauss-Frame gives better results. I assume this does not suggest that the Gauss-Frame is always better, at least there is weak evidence on a single network presented. Maybe the first contribution has to be re-stated. Further is the ""Pixel"" network representation corrected for the larger number of parameters. As someone who is interested in using this, what are the runtime considerations? ----------------I would strongly suggest to improve Fig.3. The Figure uses ""w"" several times in different notations and depictions. It mixes boxes, single symbols and illustrative figures. It took some time to decipher the Figure and its flow. ------------------------Summary: The paper is sufficiently clear, technical at many places and readability can be improved. E.g., the introduction of frames in the beginning lacks motivation and is rather unclear to someone new to this concept. The work falls in the general category of methods that impose knowledge about filter transformations into the network architecture. For me that has always two sides, the algorithmic and technical part (there are several ways to do this) and the practical side (should I do it)? This is a possible approach to this problem but after the paper I was a bit wondering what I have learned, I am certainly not inspired based on the content of the paper to integrate or build on this work. I am lacking insights into transformational parameters that are relevant for a problem. While the spatial transformer network paper was weaker on the technical elegance side, it provided exactly this: an insight into the feature transformation learned by the algorithm. I am missing this here, e.g., from Table 2  I learn that among four choices one works empirically better. What is destroyed by the x^py^p and Hermite frames that the ResNet is *not* able to recover from? You can construct network architectures that are the superset of both, so that inferior performance could be avoided. ----------------The algorithm is clear but it is similar to the Dynamic Filter Networks paper. And I am unfortunately not convinced about the usefulness of this particular formulation. I'd expect a stronger paper with more insights into transformations and comparisons to standard techniques, a clear delineation of when this is advised. ","This paper studies how to incorporate local invariance to geometric transformations into a CNN pipeline. It proposes steerable filter banks as the ground-bed to measure and produce such local invariance, building on previous work from the same authors as well as the Spatial Transformer Networks. Preliminary experiments on several tasks requiring different levels of local invariance are presented.     The reviewers had varying opinions about this work; all acknowledged the potential benefits of the approach, while some of them raised questions about the significance and usefulness of the approach. The authors were very responsive during the rebuttal phase and took into account all the feedback.     Based on the technical content of the paper and the reviewers opinion, the AC recommends rejection. Since this decision is not consensual among all reviewers, please let me explain it in more detail.    - The current manuscript does not provide a clear description of the new model in the context of the related works it builds upon (the so-called dynamic filter networks and the spatial transformer networks). The paper spends almost 4 pages with a technical exposition on steerable frames, covering basic material from signal processing. While this might indeed be a good introduction to readers not familiar with the concept of steerable filters, the fact is that it obfuscates the real contributions of the paper. which are not clearly stated. In fact, the model is presented between equations (10) and (11), but it is not clear from these equations what specifically differentiates the dsfn from the other two models -- the reader has to do some digging in order to uncover the differences (which are important).     Besides this clarity issue, the paper does not offer any insight as to how the 'Pose generating network' Psi is supposed to estimate the pose parameters. Which architecture? what is the underlying estimation problem it is trying to solve, and why do we expect this problem to be efficiently estimated with a neural network? when are the pose parameters uniquely determined? how does this network deal with the aperture effects (i.e. the situations where there is no unicity in determining a specific pose) ?  Currently, the reader has no access to these questions, which are to some extent at the core of the proposed technique."
https://openreview.net/forum?id=r1LXit5ee,"The paper presents a learning algorithm for micromanagement of battle scenarios in real-time strategy games. It focuses on a complex sub-problem of the full RTS problem. The assumptions and restrictions made (greedy MDP, distance-based action encoding, etc.) are clear and make sense for this problem.----------------The main contribution of this paper is the zero-order optimization algorithm and how it is used for structured exploration. This is a nice new application of zero-order optimization meets deep learning for RL, quite well-motivated using similar arguments as DPG. The results show clear wins over vanilla Q-learning and REINFORCE, which is not hard to believe. Although RTS is a very interesting and challenging domain (certainly worthy as a domain of focused research!), it would have been nice to see results on other domains, mainly because it seems that this algorithm could be more generally applicable than just RTS games. Also, evaluation on such a complex domain makes it difficult to predict what other kinds of domains would benefit from this zero-order approach. Maybe the authors could add some text to clarify/motivate this.----------------There are a few seemingly arbitrary choices that are justified only by ""it worked in practice"". For example, using only the sign of w / Psi_{theta}(s^k, a^k). Again later: ""Also we neglected the argmax operation that chooses the actions"". I suppose this and dividing by t could keep things nicely within or close to [-1,1] ? It might make sense to try truncating/normalizing w/Psi; it seems that much information must be lost when only taking the sign. Also lines such as ""We did not extensively experiment with the structure of the network, but we found the maxpooling and tanh nonlinearity to be particularly important"" and claiming the importance of adagrad over RMSprop without elaboration or providing any details feels somewhat unsatisfactory and leaves the reader wondering why.. e.g. could these only be true in the RTS setup in this paper?----------------The presentation of the paper can be improved, as some ideas are presented without any context making it unnecessarily confusing. For example, when defining f(\tilde{s}, c) at the top of page 5, the w vector is not explained at all, so the reader is left wondering where it comes from or what its use is. This is explained later, of course, but one sentence on its role here would help contextualize its purpose (maybe refer later to the section where it is described fully). Also page 7: ""because we neglected that a single u is sampled for an entire episode""; actually, no, you did mention this in the text above and it's clear from the pseudo-code too.----------------""perturbated"" -> ""perturbed""------------------- After response period: ----------------No rebuttal entered, therefore review remains unchanged.","The paper presents an approach to structured exploration in StarCraft micromanagement policies (essentially small tasks in the game). From an application standpoint, this is a notable advance, since the authors are tackling a challenging domain and in doing so develop a novel exploration algorithm that seems to work quite well here.    The main downside of the paper is that the proposed algorithm does seem fairly ad-hoc: certain approximations in the algorithm, like ignoring the argmax term in computing the backprop, are not really justified except in that it ""seems to work well in practice"" (a quote from the paper), so it's really unclear whether these represent general techniques or just a method that happens to work well on the target domain for poorly understood reasons.    Despite this, however, I think the strength of the application is sufficient here. Deep learning has been alternatively pushed forward by more algorithmic/mathematical advances and more applied advances, with many of the major breakthroughs coming from seemingly ad-hoc strategies applied to challenging problems. This paper falls in that later category: the ZO algorithm may or may not lead to something slightly more disciplined in the future, but for now the compelling results on StarCraft are I believe enough to warrant accepting the paper.    Pros:  + Substantial performance improvement (over basic techniques like Q-learning) on a challenging task  + Nice intuitive justification of a new exploration approach    Cons:  - Proposed algorithm seems rather ad-hoc, making some (admittedly not theoretically justified) approximations simply because they seem to work well in practice"
https://openreview.net/forum?id=SkqMSCHxe,"This paper introduces a neural network architecture and training procedure for predicting the speed of a vehicle several seconds into the future based on video and vehicle state input. The architecture allows several RNNs to compete to make the best predictions, with only the best prediction receiving back propagation training at each time step. Preliminary experimental results show that this scheme can yield reduced prediction error.---------------- It is not clear how the best-performing RNN is chosen for each time point at test time. That is, how is the “integrated prediction” obtained in Fig. 7? Is the prediction the one with minimum error over all of the output layers? If so, this means the prediction cannot be made until you already know the value to be predicted.----------------It seems possible that a larger generic RNN might be able to generate accurate predictions. If I understand correctly, the competitive architectures have many more parameters than the baseline. Is the improved performance here due to the competitive scheme, or just a larger model? ----------------A large amount of additional work is required to sustain the claim that this scheme is successfully extracting driver ‘intentions’. It would be interesting to see if the scheme, suitably extended, can automatically infer the intention to stop at a stop sign vs slowing but not stopping due to a car in front, say, or to pass a car vs simply changing lanes. Adding labels to the dataset may enable this comparison more clearly.----------------More generally, the intention of the driver seems more related to the goals they are pursuing at the moment; there is a fair amount of work in inverse reinforcement learning that examines this problem (some of it in the context of driving style as well).","The authors present a prediction framework that involves multiple 'competitive' RNNs, and they claim that they are predicting human intention. It is unclear if this method, which seems quite ad-hoc, is any different from a simple ensemble approach, and it is unclear that the model is predicting human intention. The experiments do not adequately demonstrate either."
https://openreview.net/forum?id=BJ46w6Ule,"The paper addresses the problem of learning compact binary data representations. I have a hard time understanding the setting and the writing of the paper is not making it any easier. For example I can't find a simple explanation of the problem and I am not familiar with these line of research. I read all the responses provided by authors to reviewer's questions and re-read the paper again and I still do not fully understand the setting and thus can't really evaluate the contributions of these work. The related work section does not exist and instead the analysis of the literature is somehow scattered across the paper. There are no derivations provided. Statements often miss references, e.g. the ones in the fourth paragraph of Section 3. This makes me conclude that the paper still requires significant work before it can be published.",This paper is about learning distributed representations. All reviewers agreed that the first draft was not clear enough for acceptance.    Reviewer time is limited and a paper that needed a complete overhaul after the reviews were written is not going to get the same consideration as a paper that was well-drafted from the beginning.    It's still the case that it's unclear from the paper how the learning updates or derived. The results are not visually impressive in themselves. It's also still the case that more is needed to demonstrate that this direction is promising compared to other approaches to representation learning.
https://openreview.net/forum?id=Hy-lMNqex,"The authors present TARTAN, a derivative of the previously published DNN accelerator architecture: “DaDianNao”. The key difference is that TARTAN’s compute units are bit-serial and unroll MAC operation over several cycles. This enables the units to better exploit any reduction in precision of the input activations for improvement in performance and energy efficiency.----------------Comments:----------------1. I second the earlier review requesting the authors to be present more details on the methodology used for estimating energy numbers for TARTAN. It is claimed that TARTAN gives only a 17% improvement in energy efficiency. However, I suspect that this small improvement is clearly within the margin of error ij energy estimation.  ----------------2. TARTAN is a derivative of DaDianNao, and it heavily relies the overall architecture of DaDianNao. The only novel aspect of this contribution is the introduction of the bit-serial compute unit, which (unfortunately) turns out to incur a severe area overhead (of nearly 3x over DaDianNao's compute units).----------------3. Nonetheless, the idea of bit-serial computation is certainly quite interesting. I am of the opinion that it would be better appreciated (and perhaps be even more relevant) in a circuit design / architecture focused venue.","This metareview is written based on the reviews of the two expert reviewers (scores of 5(5), 5(5)),  given the wide disparity of reviewer expertise, and the relevance of hardware expertise to this particular paper.    Quality, Clarity: The paper is clearly written. It does require hardware expertise to read and appreciate the ideas, methodology, and the results. There was a request to post a clear ""take home"" message for ML research, and the authors did post a clear statement in this regard.    Originality: The size of the contribution is the major point of contention for this paper. The expert reviewers believe it to be an incremental/small idea, particularly in relation to the prior work of the authors. The authors provide rebut that while the implementation builds on their previous work, it can be seen as a general concept that could be applied to other architectures, and that this work targets the important case of fully-connected architectures, vs the case of convolutional architectures that were the case where the prior work did well.    Overall: I have asked the expert reviewers for further input. In the absence of further information, updated scores, or some reasonably strong indication of excitement about the ideas from the expert reviewers, there is little choice but to currently lean towards ""reject"" in terms of this being an inspirational paper for ICLR."
https://openreview.net/forum?id=SJBr9Mcxl,"This paper makes three main methodological contributions:-------- - definition of Neural Feature (NF) as the pixel average of the top N images that highly activation a neuron-------- - ranking of neurons based on color selectivity-------- - ranking of neurons based on class selectivity----------------The main weaknesses of the paper are that none of the methodological contributions are very significant, and no singularly significant result arises from the application of the methods.----------------However, the main strengths of the paper are its assortment of moderately-sized interesting conclusions about the basic behavior of neural nets. For example, a few are:-------- - “Indexing on class selectivity neurons we found highly class selective neurons like digital-clock at conv2, cardoon at conv3 and ladybug at conv5, much before the fully connected layers.” As far as I know, this had not been previously reported.-------- - Color selective neurons are found even in higher layers. (25% color selectivity in conv5)-------- - “our main color axis emerge (black-white, blue-yellow, orange-cyan and cyan- magenta). Curiously, these two observations correlate with evidences in the human visual system (Shapley & Hawken (2011)).” Great observation!----------------Overall, I’d recommend the paper be accepted, because although it’s difficult to predict at this time, there’s a fair chance that one of the “smaller conclusions” would turn out to be important in hindsight a few years hence.------------------------Other small comments:-------- - The cite for “Learning to generate chairs…” is wrong (first two authors combined resulting in a confusing cite)---------------- - What exactly is the Color Selectivity Index computing? The Opponent Color Space isn’t well defined and it wasn’t previously familiar to me. Intuitively it seems to be selecting for units that respond to a constant color, but the highest color selectivity NF in Fig 5 i for a unit with two colors, not one. Finally, the very last unit (lowest color selectivity) is almost the same edge pattern, but with white -> black instead of blue -> orange. Why are these considered to be so drastically different? This should probably be more clearly described.---------------- - For the sake of argument, imagine a mushroom sensitive neuron in conv5 that fires highly for mushrooms of *any* color but not for anything else. If the dataset contains only red-capped mushrooms, would the color selectivity index for this neuron be high or low? If it is high, it’s somewhat misleading because the unit itself actually isn’t color selective; the dataset just happens only to have red mushrooms in it. (It’s a subtle point but worth considering and probably discussing in the paper)","While this is interesting work, one major concern comes from Reviewer 2 regarding the attempt of characterizing the tuning properties, which has proven useless both in neuroscience and in machine learning. Currently, no attempt so far has lived up to the promises this line of research is aiming for. In summary, this work is explorative and incremental but worthwhile. We encourage the authors to further refine their research effort and resubmit."
https://openreview.net/forum?id=BJtNZAFgg,"The authors extend GANs by an inference path from the data space to the latent space and a discriminator that operates on the joint latend/data space. They show that the theoretical properties of GANs still hold for BiGAN and evaluate the features learned unsupervised in the inference path with respect to performance on supervised tasks after retraining deeper layers.----------------I see one structural issue with this paper: Given that, as stated in the abstract, the main purpose of the paper is to learn unsupervised features (and not to improve GANs), the paper might spent too much space on detailing the relationship to GANs and all the theoretical properties. It is not clear whether they actually would help with the goal of learning good features. While reading the paper, I actually totally forgot about the unsupervised features until they reappeared on page 6. I think it would be helpful if the text of the paper would be more aligned with this main story.----------------Still, the BiGAN framework is an elegant and compelling extension to GANs. However, it is not obvious how much the theoretical properties help us as the model is clearly not fully converged. To me, especially Figure 4 seems to suggest that G(E(x)) might be doing not much more than some kind of nearest neighbour retrival (and indeed one criticism for GANs has always been that they might just memorize some samples). By the way, it would be very interesting to know how well the discriminator actually performs after training.----------------Coming back to the goal of learning powerful features: The method does not reach state-of-the-art performance on most evaluated tasks (Table 2 and 3) but performs competitive and it would be interesting to see how much this improves if the BiGAN training (and the convolutional architecture used) would be improved.----------------The paper is very well written and provides most necessary details, although some more details on the training (learning rates, initialization) would be helpful for reproducing the results.----------------Overall I think the paper provides a very interesting framework for further research, even though the results presented here are not too impressive both with respect to the feature evaluation (and the GAN learning).----------------Minor: It might be helpful to highlight the best performance numbers in Tables 2 and 3.","Note: the latest version includes significantly improved ImageNet classification results in Table 2 (up ~2% across the board compared to the previous version).  This is due to the fact that in previous versions, we evaluated using the predictions for a single (center) crop at test time, rather than averaging over 10 crops as we learned by correspondence was how the previous results from Noroozi & Favaro (2016) were obtained.  (Thanks to Mehdi for bringing this up!) There are also very slightly improved VOC classification results (Table 3) due to an unrelated minor bug."
https://openreview.net/forum?id=Bk3F5Y9lx,"This paper replaces the Gaussian prior often used in a VAE with a group sparse prior. They modify the approximate posterior function so that it also generates group sparse samples. The development of novel forms for the generative model and inference process in VAEs is an active and important area of research. I don't believe the specific choice of prior proposed in this paper is very well motivated however. I believe several of the conceptual claims are incorrect. The experimental results are unconvincing, and I suspect compare log likelihoods in bits against competing algorithms in nats.----------------Some more detailed comments:----------------In Table 1, the log likelihoods reported for competing techniques are all in nats. The reported log likelihood of cVAE using 10K samples is not only higher than the likelihood of true data samples, but is also higher than the log likelihood that can be achieved by fitting a 10K k-means mixture model to the data (eg as done in ""A note on the evaluation of generative models""). It should nearly impossible to outperform a 10K k-means mixture on Parzen estimation, which makes me extremely skeptical of these eVAE results. However, if you assume that the eVAE log likelihood is actually in bits, and multiply it by log 2 to convert to nats, then it corresponds to a totally believable log likelihood. Note that some Parzen window implementations report log likelihood in bits. Is this experiment comparing log likelihood in bits to competing log likelihoods in nats? (also, label units -- eg bits or nats -- in table)----------------It would be really, really, good to report and compare the variational lower bound on the log likelihood!! Alternatively, if you are concerned your bound is loose, you can use AIS to get a more exact measure of the log likelihood. Even if the Parzen window results are correct, Parzen estimates of log likelihood are extremely poor. They possess any drawback of log likelihood evaluation (which they approximate), and then have many additional drawbacks as well.----------------The MNIST sample quality does not appear to be visually competitive. Also -- it appears that the images are of the probability of activation for each pixel, rather than actual samples from the model. Samples would be more accurate, but either way make sure to describe what is shown in the figure.----------------There are no experiments on non-toy datasets.----------------I am still concerned about most of the issues I raised in my questions below. Briefly, some comments on the authors' response:----------------1. ""minibatches are constructed to not only have a random subset of training examples but also be balanced w.r.t. to epitome assignment (Alg. 1, ln. 4).""--------Nice! This makes me feel better about why all the epitomes will be used.----------------2. I don't think your response addresses why C_vae would trade off between data reconstruction and being factorial. The approximate posterior is factorial by construction -- there's nothing in C_vae that can make it more or less factorial.----------------3. ""For C_vae to have zero contribution from the KL term of a particular z_d (in other words, that unit is deactivated), it has to have all the examples in the training set be deactivated (KL term of zero) for that unit""--------This isn't true. A standard VAE can set the variance to 1 and the mean to 0 (KL term of 0) for some examples in the training set, and have non-zero KL for other training examples.----------------4. The VAE loss is trained on a lower bound on the log likelihood, though it does have a term that looks like reconstruction error. Naively, I would imagine that if it overfits, this would correspond to data samples becoming more likely under the generative model.----------------5/6. See Parzen concerns above. It's strange to train a binary model, and then treat it's probability of activation as a sample in a continuous space.----------------6. ""we can only evaluate the model from its samples""--------I don't believe this is true. You are training on a lower bound on the log likelihood, which immediately provides another method of quantitative evaluation. Additionally, you could use techniques such as AIS to compute the exact log likelihood.----------------7. I don't believe Parzen window evaluation is a better measure of model quality, even in terms of sample generation, than log likelihood.","This paper addresses issues faced when using VAEs and the pruning of latent variables. Improvements to the paper have been accounted for and improved the paper, but after considering the rebuttal and discussion, the reviewers still felt that more was needed, especially in terms of applicability across multiple different data sets. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings."
https://openreview.net/forum?id=S19eAF9ee,"The authors address the problem of modeling temporally-changing signal on a graph, where the signal at one node changes as a function of the inputs and the hidden states of its neighborhood, the size of which is a hyperparameter. The approach follows closely that of Shi et al. 2015, but it is generalized to arbitrary graph structures rather than a fixed grid by using graph convolutions of Defferrard et al. 2016. This is not a strict generalization because the graph formulation treats all edges equally, while the conv kernels in Shi et al. have a built in directionality. The authors show results on a moving MNIST and on the Penn Tree Bank Language Modeling task.----------------The paper, model and experiments are decent but I have some concerns:----------------1. The proposed model is not exceptionally novel from a technical perspective. I usually don't mind if this is the case provided that the authors make up for the deficiency with thorough experimental evaluation, clear write up, and interesting insights into the pros/cons of the approach with respect to previous models. In this case I lean towards this not being the case.----------------2. The experiment results section is rather terse and light on interpretation. I'm not fully up to date on the latest of Penn Tree Bank language modeling results but I do know that it is a hotly contested and well-known dataset. I am surprised to see a comparison only to Zaremba et al 2014 where I would expect to see multiple other results.----------------3. The writing is not very clear and the authors don't make sufficiently strong attempt to compare the models or provide insight or comparisons into why the proposed model works better. In particular, unless I'm mistaken the word probabilities are a function of the neighborhood in the graph. What is the width of this graph? For example, suppose I sample a word in one part of the graph, doesn't this information have to propagate to the other parts of the graph along the edges? Also, it's not clear to me how the model can achieve reasonable results on moving MNIST when it cannot distinguish the direction of the moving edges. The authors state this but do not provide satisfying insight into how this can work. How does a pixel know that it should turn on in the next frame? I wish the authors thought about this more and presented it more clearly.----------------In summary, the paper has somewhat weak technical contribution, the experiments section is not very thorough, and the insights are sparse.","While graph structures are an interesting problem, as the reviewers observed, the paper extends previous work incrementally and the results are not very moving.    pros  - interesting problem space that has not been thoroughly explored  cons  - experimental evaluation was not convincing enough with the results.  - the method itself is a small incremental improvement over prior papers."
https://openreview.net/forum?id=r1rz6U5lg,"Two things I really liked about this paper:--------1. The whole idea of having a data-dependent proposal distribution for MCMC. I wasn't familiar with this, although it apparently was previously published. I went back: the (Zhu, 2000) paper was unreadable. The (Jampani, 2014) paper on informed sampling was good. So, perhaps this isn't a good reason for accepting to ICLR.----------------2. The results are quite impressive. The rough rule-of-thumb is that optimization can help you speed up code by 10%. The standard MCMC results presented on the paper on randomly-generated programs roughly matches this (15%). The fact that the proposed algorithm get ~33% speedup is quite surprising, and worth publishing.----------------The argument against accepting this paper is that it doesn't match the goals of ICLR. I don't go to ICLR to hear about generic machine learning papers (we have NIPS and ICML for that). Instead, I go to learn about how to automatically represent data and models. Now, maybe this paper talks about how to represent (generated) programs, so it tangentially lives under the umbrella of ICLR. But it will compete against more relevant papers in the conference -- it may just be a poster. Sending this to a programming language conference may have more eventual impact.----------------Nonetheless, I give this paper an ""accept"", because I learned something valuable and the results are very good. ","This paper applies REINFORCE to learn MCMC proposals within the existing STOKE scheme for super-optimization. It's a neat paper, with interesting results.    It's not clear whether interesting representations are learned, and the algorithms are not really new. However, it's a neat piece or work, that some ICLR reviewers found interesting, and could inspire more representation learning work in this area."
https://openreview.net/forum?id=Hy-lMNqex,"I do not feel very qualified to review this paper. I studied digital logic back in university, that was it. I think the work deserves a reviewer with far more sophisticated background in this area. It certainly seems useful. My advice is also to submit it another venue.","This metareview is written based on the reviews of the two expert reviewers (scores of 5(5), 5(5)),  given the wide disparity of reviewer expertise, and the relevance of hardware expertise to this particular paper.    Quality, Clarity: The paper is clearly written. It does require hardware expertise to read and appreciate the ideas, methodology, and the results. There was a request to post a clear ""take home"" message for ML research, and the authors did post a clear statement in this regard.    Originality: The size of the contribution is the major point of contention for this paper. The expert reviewers believe it to be an incremental/small idea, particularly in relation to the prior work of the authors. The authors provide rebut that while the implementation builds on their previous work, it can be seen as a general concept that could be applied to other architectures, and that this work targets the important case of fully-connected architectures, vs the case of convolutional architectures that were the case where the prior work did well.    Overall: I have asked the expert reviewers for further input. In the absence of further information, updated scores, or some reasonably strong indication of excitement about the ideas from the expert reviewers, there is little choice but to currently lean towards ""reject"" in terms of this being an inspirational paper for ICLR."
https://openreview.net/forum?id=BJwFrvOeg,"This paper addresses the practical problem of generating rare or unseen words in the context of language modeling. Since language follows a Zipf’s law, most approaches limit the vocabulary (because of computation reasons) and hence rare words are often mapped to a UNK token. Rare words are especially important in context of applications such as question answering. MT etc. This paper proposes a language modeling technique which incorporates facts from knowledge bases (KBs) and thus has the ability to generate (potentially unseen) words from KBs. This paper also releases a dataset by aligning words with Freebase facts and corresponding Wikipedia descriptions.----------------The model first selects a KB fact based on the previously generated words and facts. Based on the selected fact, it then predicts whether to generate a word based on the vocabulary or to output a symbolic word from the KB. For the latter, the model is trained to predict the position of the word from the fact description.----------------Overall the paper could use some rewriting especially the notations in section 3. The experiments are well executed and they definitely get good results. The heat maps at the end are very insightful. ----------------Comments----------------This contributions of this paper would be much stronger if it showed improvements in a practical applications such as Question Answering (although the paper clearly mentions that this technique could be applied to improve QA)--------In section 3, it is unclear why the authors refer the entity as a ‘topic'. This makes the text a little confusing since a topic can also be associated with something abstract, but in this case the topic is always a freebase entity. --------Is it really necessary to predict a fact at every step before generating a word. In other words, how many distinct facts on average does the model choose to generate a sentence. Intuitively a natural language sentence would be describe few facts about an entity. If the fact generation step could be avoided (by adding a latent variable which decides if the fact should be generated or not), the model will also be faster.--------In equation 2, the model has to make a hard decision to choose the fact. For this to be end to end trained, every word needs to be annotated with a corresponding fact which might not be always a realistic scenario. For e.g., in domains such as social media text.--------Learning position embeddings for copying knowledge words seems a little counter-intuitive. Does the sequence of knowledge words follow any particular structure like word O_2 is always the last name (e.g. Obama).--------It would also be nice to compare to char-level LM's which inherently solves the unknown token problem. ","This work introduces a combination of a LM with knowledge based retrieval system. This builds upon the recent trend of incorporating pointers and external information into generation, but includes some novelty, making the paper ""different and more interesting"". Generally though the reviewers found the clarity of the work to be sufficiently an issue that no one strongly defended its inclusion.    Pros:  - The reviewers seemed to like the work and particularly the problem space. Issues were mainly on presentation and experiments.     Mixed:  - Reviewers were divided on experimental quality. The work does introduce a new dataset, but reviewers would also have liked use on some existing tasks.     Cons:  - Clarity and writing issues primarily. All reviewers found details missing and generally struggled with comprehension.  - Novelty was a question. Impact of work could also be improved by more clearly defining new contributions"
https://openreview.net/forum?id=SJg498clg,"The paper proposes a model that aims at learning to label nodes of graph in a semi-supervised setting. The idea of the model is based on the use of the graph structure to regularize the representations learned at the node levels. Experimental results are provided on different tasks----------------The underlying idea of this paper (graph regularization) has been already explored in different papers – e.g 'Learning latent representations of nodes for classifying in heterogeneous social networks' [Jacob et al. 2014],   [Weston et al 2012] where a real graph structure is used instead of a built one. The experiments lack of strong comparisons with other graph models (e.g Iterative Classification, 'Learning from labeled and unlabeled data on a directed graph', ...). So the novelty of the paper and the experimental protocol are not strong enough to accpet the paper.----------------Pros:--------* Learning over graph is an important topic----------------Cons:--------* Many existing approaches have already exploited the same types of ideas, resulting in very close models--------* Lack of comparison w.r.t existing models","The paper is an interesting contribution, primarily in its generalization of Weston's et al's work on semi-supervised embedding method. You have shown convincingly that it can work with multiple architectures, and with various forms of graph. And the PubMed results are good. To improve the paper in the future, I'd recommend 1) relating better to prior work, and 2) extending your exploration of its application to graphs without features."
https://openreview.net/forum?id=S1HcOI5le,"This paper proposes a regularization technique for k-shot learning based on orthogonal grouping of units in a neural network. The units within a group are forced to be maximally similar, at the same time the units from different groups are encouraged to be orthogonal. While I like the motivation of the approach, the empirical analysis provided in the paper doesn’t look particularly convincing.----------------My main concerns are the following:----------------1. The method is sensitive to the values of alpha and beta and a poor choice of those hyperparameters can lead to a quite drastic drop in performance comparing the minor gains one gets when alpha and beta are set properly.----------------2. It seems strange that the best performance is obtained when the group's size ratio is 0.5. From the figures in the paper, it follows that usually, one has more “orthogonal” groups in a filter bank. I have an impression that the empirical evidence doesn’t align well with the motivation of the proposed approach.----------------3. The paper contains a significant amount of typos and incorrectly formatted references. There are also several places in the manuscript that I found hard to understand due to unusual phrasing.----------------I would like to thank the authors for answering/addressing my pre-review questions. I would be grateful if the authors could provide more clarifications of the following:----------------1. Question 2: I’m not sure if modifying \theta_{map} alone would result in any learning at all. Do I understand correctly that \theta_{map} is only used to define groups? If so, then I don’t see how the proposed method can be used in the purely unsupervised regime.----------------2. Question 3: I was not referring to the fixed clustering based on the filter of the pre-trained network. One can perform that clustering at every step of the k-shot learning process. I’m not sure I understand why the authors visualize grouping of _filters_ while in the actual algorithm they group _activations_. ----------------Overall, the paper is quite interesting but needs a stronger empirical justification of the approach as well as a better presentation of the material.",All three reviewers point to significant deficiencies. No response or engagement from the authors (for the reviews). I see no basis for supporting this paper.
https://openreview.net/forum?id=Sys6GJqxl,"This paper present an experimental study of the robustness of state-of-the-art CNNs to different types of ""attacks"" in the context of image classication. Specifically, an attack aims to fool the classification system with a specially corrupted image, i.e. making it misclassify the image as (1) any wrong class (non-targeted attack) or (2) a target class, chosen in advance by the attacker (targeted attack). For instance, the attacker could corrupt an image of an ostrich in such a way that it would be classified as a megalith. Even though the attacker's agenda is not so clear in this example, it is still interesting to study the weaknesses of current systems in view of (1) improving them in general and (2) actual risks with e.g. autonomous vehicles.----------------The paper is mostly experimental. In short, it compares different strategies (already published in previous papers) for all popular networks  (VGG, GoogLeNet, ResNet-50/101/152) and the two aforementionned types of attacks. The experiments are well conducted and clearly exposed. A convincing point is that attacks are also conducted on ""clarifai.com"" which is a black-box classification system. Some analysis and insightful explanations are also provided to help understanding why CNNs are prone to such attacks (Section 6).----------------To sum up, the main findings are that non-targeted attacks are easy to perform, even on a black-box system. Non-targeted attacks are more difficult to realize with existing schemes, but the authors propose a new approach for that that vastly improves over existing attacks (even though it's still far from perfect: ~20% success rate on clarifai.com versus 2% with previous schemes).----------------Arguably, The paper still has some weaknesses:---------------- - The authors are treating the 3 ResNet-based networks as different, yet they are obviously clearly correlated. See Table 7 for instance. This is naturally expected because their architecture is similar (only their depth varies). Hence, it does not sound very fair to state that ""One interesting finding is that [...] the first misclassified label (non-targeted) is the same for all models except VGG-16 and GoogLeNet."", i.e., the three ResNet-based networks.---------------- - A subjective measure is employed to evaluate the effectiveness of the attacks on the black box system. While this is for a good reason (clarifai.com returns image labels that are different from ImageNet), it is not certain that the reported numbers are fair (even though the qualitative results look convincing).---------------- - The novelty of the proposed approach (optimizing an ensemble of network instead of a single network) is limited. However, this was not really the point of the paper, and it is effective, so it seems ok overall.---------------- - The paper is quite long. This is expected because it is an extensive evaluation study, but still. I suggest the authors prune some near-duplicate content (e.g. Section 2.3 has a high overlap with Section 1, etc.).---------------- - The paper would benefit from additional discussions with the recent and related work of Fawzi et al (NIPS'16) in Section 6. Indeed the work of Fawzi et al. is mostly theoretical and well aligned with the experimental findings and observations (in particular in Section 6).------------------------To conclude, I think that this paper is somewhat useful for the community and could help to further improve existing architectures, as well as better assess their flaws and weaknesses.","The paper is the first to demonstrate that it is possible for an adversary to change the label that a convolutional network predicts for an image to a specific value. Like Papernot et al., it presents a successful attack on Clarifai's image-recognition system. I encourage the authors to condense the paper to its key results (13 pages without / 24 pages with supplemental material is too long for a conference paper)."
https://openreview.net/forum?id=rJqBEPcxe,"The authors propose a conceptually simple method for regularisation of recurrent neural networks. The idea is related to dropout, but instead of zeroing out units, they are instead set to their respective values at the preceding time step element-wise with a certain probability.----------------Overall, the paper is well written. The method is clearly represented up to issues raised by reviewers during the pre-review question phase. The related work is complete and probably the best currently available on the matter of regularising RNNs.----------------The experimental section focuses on comparing the method with the current SOTA on a set of NLP benchmarks and a synthetic problem. All of the experiments focus on sequences over discrete values. An additional experiment also shows that the sequential Jacobian is far higher for long-term dependencies than in the dropout case.----------------Overall, the paper bears great potential. However, I do see some points.----------------1) As raised during the pre-review questions, I would like to see the results of experiments that feature a complete hyper parameter search. I.e. a proper model selection process,as it should be standard in the community. I do not see why this was not done, especially as the author count seems to indicate that the necessary resources are available.----------------I want to repeat at this point that Table 2 of the paper shows that validation error is not a reliable estimator for testing error in the respective data set. Thus, overfitting the model selection process is a serious concern here.--------Zoneout does not seem to improve that much in the other tasks.----------------2) Zoneout is not investigated well mathematically. E.g. an analysis of the of the form of gradients from unit K at time step T to unit K’ at time step T-R would have been interesting, especially as these are not necessarily non-zero for dropout. Also, the question whether zoneout has a variational interpretation in the spirit of Yarin Gal’s work is an obvious one. I can see that it is if we treat zoneout in a resnet framework and dropout on the incremental parts. Overall, little effort is done answering the question *why* zoneout works well, even though the literature bears plenty of starting points for such analysis.----------------3) The data sets used are only symbolic. It would have been great if more ground was covered, i.e. continuous data such as from dynamical systems. To me it is not obvious whether it will transfer right away.------------------------An extreme amount of “tricks” is being published currently for improved RNN training. How does zoneout stand out? It is a nice idea, and simple to implement. However, the paper under delivers: the experiments do not convince me (see 1) and 3)). There authors do not provide convincing theoretical insights either. (2)----------------Consequently, the paper reduces to a “epsilon improvement, great text, mediocre experimental evaluation, little theoretical insight”.","Very nice paper, with simple, intuitive idea that works quite well, solving the problem of how to do recurrent dropout.    Pros:  - Improved results  - Very simple method    Cons:  - Almost the best results (aside from Variational Dropout)"
https://openreview.net/forum?id=ry3iBFqgl,"Paper Summary: --------This paper presents a new comprehension dataset called NewsQA dataset, containing 100,000 question-answer pairs from over 10,000 news articles from CNN. The dataset is collected through a four-stage process -- article filtering, question collection, answer collection and answer validation. Examples from the dataset are divided into different types based on answer types and reasoning required to answer questions. Human and machine performances on NewsQA are reported and compared with SQuAD.----------------Paper Strengths: ---------- I agree that models can benefit from diverse set of datasets. This dataset is collected from news articles, hence might pose different sets of problems from current popular datasets such as SQuAD.---------- The proposed dataset is sufficiently large for data hungry deep learning models to train. ---------- The inclusion of questions with null answers is a nice property to have.---------- A good amount of thought has gone into formulating the four-stage data collection process.---------- The proposed BARB model is performing as good as a published state-of-the-art model, while being much faster.    ----------------Paper Weaknesses: ---------- Human evaluation is weak. Two near-native English speakers' performance on 100 examples each can hardly be a representative of the complete dataset. Also, what is the model performance on these 200 examples?---------- Not that it is necessary for this paper, but to clearly demonstrate that this dataset is harder than SQuAD, the authors should either calculate the human performance the same way as SQuAD or calculate human performances on both NewsQA and SQuAD in some other consistent manner on large enough subsets which are good representatives of the complete datasets. Dataset from other communities such as VQA dataset (Antol et al., ICCV 2015) also use the same method as SQuAD to compute human performance. ---------- Section 3.5 says that 86% of questions have answers agreed upon by atleast 2 workers. Why is this number inconsistent with the 4.5% of questions which have answers without agreement after validation (last line in Section 4.1)?---------- Is the same article shown to multiple Questioners? If yes, is it ensured that the Questioners asking questions about the same article are not asking the same/similar questions?---------- Authors mention that they keep the same hyperparameters as SQuAD. What are the accuracies if the hyperparameters are tuned using a validation set from NewsQA?---------- 500 examples which are labeled for reasoning types do not seem enough to represent the complete dataset. Also, what is the model performance on these 500 examples?---------- Which model's performance has been shown in Figure 1?---------- Are the two ""students"" graduate/undergraduate students or researchers?---------- Test set seems to be very small.---------- Suggestion: Answer validation step is nice, but maybe the dataset can be released in 2 versions -- one with all the answers collected in 3rd stage (without the validation step), and one in the current format with the validation step. ----------------Preliminary Evaluation: --------The proposed dataset is a large scale machine comprehension dataset collected from news articles, which in my suggestion, is diverse enough from existing datasets that state-of-the-art models can definitely benefit from it. With a better human evaluation, I think this paper will make a good poster. ","The program committee appreciates the authors' response to concerns raised in the reviews. All reviewers agree that the paper is not convincingly above the acceptance threshold. The paper will be stronger, and the benefit of this dataset over SQuAD will likely be more clear once authors incorporate reviewers' comments and finish evaluation of inter-human agreement."
https://openreview.net/forum?id=Bk8N0RLxx,"This paper compares several strategies for guessing a short list of vocabulary for the target language in neural machine translation. The primary findings are that word alignment dictionaries work better than a variety of other techniques.----------------My take on this paper is that to have a significant impact, it needs to make the case for why one might want vocabulary rather than characters or sub word units like BPE. I think there are likely many very good reasons to do this that could be argued for (synthesize morphology, deal with transliteration, etc), but most of these would suggest some particular models and experiments, which are of course not in this paper. As it is, I think this paper is a useful but minor contribution that shows that word alignment is a good way of getting short lists, but it does not strongly make the case that we should abandon work in other directions.----------------Minor comments:--------In addition to the SVM approach for modeling vocabulary, the discriminative word lexicon of Mauser et al. (2009) and the neural version of Ha et al. (2014) are also worth mentioning.----------------It would be useful to know what the coverage rate of the actual full vocabulary would be (rather than the 100k “full vocabulary”). Since presumably this technique could be used to work with much larger vocabularies.----------------When reducing the vocabulary size for training, the Mi et al. (2016) technique of taking the union of all the vocabularies in a mini batch seems like a rather strange objective. If the vocabulary of a single sentence is used, the probabilistic semantics of the translation model can still be preserved since p(e | f, vocab(f)) = p(e | f) if p(vocab(f) | f) = 1, i.e., is deterministic, which it is here. Whereas the objective is no longer a sensible probability model in the mini batch vocabulary case. Thus, while it may be a bit more difficult to implement, it seems like it would at least be a sensible comparison to make.","The reviewers agree that the method is exciting as practical contributions go, but the case for originality is not strong enough."
https://openreview.net/forum?id=SJGCiw5gl,"This paper presents a novel way of pruning filters from convolutional neural networks with a strong theoretical justification. The proposed methods is derived from the first order Taylor expansion of the loss change while pruning a particular unit. This leads to simple weighting of the unit activation with its gradient w.r.t. loss function and performs better than simply using the activation magnitude as the heuristic for pruning. This intuitively makes sense, as we would like to remove not only the filters with low activation, but also filters where the incorrect activation value would not have small influence on the target loss.----------------Authors thoroughly investigate multiple baselines, including an oracle which sets an upper bound on the target performance even though it is computationally expensive. The devised method seems to be quite elegant and authors show that it generalizes well on multiple tasks and is computationally more than feasible as it is easy to combine with traditional fine tuning procedure. Also, the work clearly shows the trade-offs of increased speed and decreased performance, which is useful for practical applications.----------------It would be also useful to compare against different baselines, e.g. [1]. However this method seems to be more useful as it does not involve training of a new network (and thus is probably much faster).----------------Suggestion - maybe it can be extended in the future towards also removing only parts of the filters(e.g. for the 3D convolution)? This may be more complicated as it would need to change the implementation of convolution operator, but can lead to further speedup.----------------[1] https://arxiv.org/abs/1503.02531",The paper presents a method for pruning filters from convolutional neural networks based on the first order Taylor expansion of the loss change. The method is novel and well justified with extensive empirical evaluation.
https://openreview.net/forum?id=SJk01vogl,"This paper considers different methods of producing adversarial examples for generative models such as VAE and VAEGAN. Specifically, three methods are considered: classification-based adversaries which uses a classifier on top of the hidden code, VAE loss which directly uses the VAE loss and the ""latent attack"" which finds adversarial perturbation in the input so as to match the latent representation of a target input.----------------I think the problem that this paper considers is potentially useful and interesting to the community. To the best of my knowledge this is the first paper that considers adversarial examples for generative models. As I pointed out in my pre-review comments, there is also a concurrent work of ""Adversarial Images for Variational Autoencoders"" that essentially proposes the same ""latent attack"" idea of this paper with both L2 distance and KL divergence.----------------Novelty/originality: I didn't find the ideas of this paper very original. All the proposed three attacks are well-known and standard methods that here are applied to a new problem and this paper does not develop *novel* algorithms for attacking specifically *generative models*. However I still find it interesting to see how these standard methods compare in this new problem domain.----------------The clarity and presentation of the paper is very unsatisfying. The first version of the paper proposes the ""classification-based adversaries"" and reports only negative results. In the second set of revisions, the core idea of the paper changes and almost an entirely new paper with a new co-author is submitted and the idea of ""latent attack"" is proposed which works much better than the ""classification-based adversaries"". However, the authors try to keep around the materials of the first version, which results in a 13 page long paper, with different claims and unrelated set of experiments. ""in our attempts to be thorough, we have had a hard time keeping the length down"" is not a valid excuse.----------------In short, the paper is investigating an interesting problem and apply and compare standard adversarial methods to this domain, but the novelty and the presentation of the paper is limited.","The main idea in this paper is interesting, of considering various forms of adversarial examples in generative models. The paper has been considerably revised since the original submission. The results showing the susceptibility of generative models to attacks are interesting, but the demonstrations need to be more solid and on other datasets to be convincing."
https://openreview.net/forum?id=H1VyHY9gg,"Strengths--------- A simple “noising” method for improving LM--------- “noise” added to word history by using a probabilistic distribution using N-gram smoothing--------- Experimental evidence that such simple techniques improve LM and MT----------------Weaknesses--------- Purely empirical, with no theoretical justification--------- Rather primitive step so far; would be nice to see future work in modeling different types of LM “noise” as hidden variables and then “denoise” them.","The reviewers are reasonably supportive of this paper. The ideas presented in the paper are nice and the results are encouraging. The authors should consider, for the final version of this work, providing comparisons to other approaches on the text8 corpus (or on the 1 Billion Words corpus, Chelba et al.)."
https://openreview.net/forum?id=BJAA4wKxg,"The system described works comparably to bi-directional LSTM baseline for NMT, and CNN's are naturally parallelizable.----------------Key ideas include the use of two stacked CNN's (one for each of encoding and decoding) for translation, with res connections and position embeddings. The use of CNN's for translation has been attempted previously (as described by the authors), but presumably it is the authors' combination of various architectural choices (attention, position embeddings, etc) that make the present system competitive with RNN's, whereas earlier attempts were not. They describe system's sensitivity to some of these choices (e.g. experiments to choose appropriate number of layers in each of the CNN's).----------------The experimental results are well reported in detail.----------------One or two figures would definitely be required to help clarify the architecture.----------------This paper is less about new ways of learning representations than about the combination of choices made (over the set of existing techniques) in order to get the good results that they do on the reported NMT tasks. In this respect, while I am fairly confident that the paper represents good work in machine learning, I am not quite as confident about its fit for this particular conference.","This work demonstrates architectural choices to make conv nets work for NMT. In general the reviewers liked the work and were convinced by the results but found the main contributions to be ""incremental"".     Pros:  - Clarity: The work was clearly presented, and besides for minor comments (diagrams) the reviewers understood the work  - Quality: The experimental results were thorough, ""very extensive and leaves no doubt that the proposed approach works well"".    Mixed:  - Novelty: There is appreciation that the work is novel. However as the work is somewhat ""application-specific"" the reviewers felt the technical contribution was not an overwhelming contribution.  - Impact: While some speed ups were shown, not all reviewers were convinced that the benefit was sufficient, or ""main speed-up factor(s)"" were.     This work is clearly worthwhile, but the reviews place it slightly below the top papers in this area."
https://openreview.net/forum?id=Bk0FWVcgx,"This work contributes to understanding the landscape of deep networks in terms of its topology and geometry. The paper analyzes the former theoretically, and studies the latter empirically. Although the provided contributions are very specific (ReLU nets with single hidden layer, and a heuristic to calculate the normalized geodesic), the results are original and of interest. Thus, they could potentially be used as stepping stones for deeper developments in this area.----------------Pros:--------1. Providing new theory about existence of ""poor"" local minima for ReLU networks with a hidden unit that relies on input distribution properties as well as the size of the hidden layer.--------2. Coming up with a heuristic algorithm to compute the normalized geodesic between two solution points. The latter reflects how curved the path between the two is. ----------------Cons:--------The results are very specific in both topology and geometry analysis.--------1. The analysis is performed only over a ""single"" hidden layer ReLU network. Given the importance of depth in deep architectures, this result cannot really explain the kinds of architectures we are interested in practically. --------2. The normalized geodesic criterion is somewhat limited in representing how easy it is to connect two equally good points. For example, there might exist a straight line between the two (which is considered as easy by the geodesic criterion), but this line might be going through a very narrow valley, challenging gradient based optimization algorithms (and thus extremely difficult to navigate in practice). In addition, the proposed algorithm for computing the normalized geodesic is a greedy heuristic, which as far as I can tell, makes it difficult to know how we can trust in the estimated geodesics obtained by this algorithm.----------------With all cons said, I stress that I understand both problems tackled in the paper are challenging, and thus I find the contributions valuable and interesting.","The paper presents an analysis of deep ReLU networks, and contrasts it with linear networks. It makes good progress towards providing a theoretical explanation of the difficult problem of characterizing the critical points of this highly nonconvex function.   I agree with the authors that their approach is superior to earlier works based on spin-glass models or other approximations that make highly unrealistic assumptions. The ReLU structure allows them to provide concrete analysis, without making such approximations, as opposed to more general activation units.  The relationship between smoothness of data distribution and the level of model overparamterization is used to characterize the ability to reach the global optimum. The intuition that having no approximation error (due to overparameterization) leads to more tractable optimization problem is intuitive. Such findings have been seen before (e.g. in tensor decomposition, the global optima can be reached when the amount of noise is small). I recommend the authors to connect it to such findings for other nonconvex problems.  The paper does not address overfitting, as they themselves point out in the conclusion. However, I do expect it to be a very challenging problem."
https://openreview.net/forum?id=B1TTpYKgx,"SUMMARY --------This paper studies the expressive power of deep neural networks under various related measures of expressivity. --------It discusses how these measures relate to the `trajectory length', which is shown to depend exponentially on the depth of the network, in expectation (at least experimentally, at an intuitive level, or theoretically under certain assumptions). --------The paper also emphasises the importance of the weights in the earlier layers of the network, as these have a larger influence on the represented classes of functions, and demonstrates this in an experimental setting. ----------------PROS --------The paper further advances on topics related to the expressive power of feedforward neural networks with piecewise linear activation functions, in particular elaborating on the relations between various points of view. ----------------CONS --------The paper further advances and elaborates on interesting topics, but to my appraisal it does not contribute significantly new aspects to the discussion. ----------------COMMENTS--------- The paper is a bit long (especially the appendix) and seems to have been written a bit in a rush. --------Overall the main points are presented clearly, but the results and conclusions could be clearer about the assumptions / experimental vs theoretical nature. --------The connection to previous works could also be clearer. ----------------- On page 2 one finds the statement ``Furthermore, architectures are often compared via ‘hardcoded’ weight values -- a specific function that can be represented efficiently by one architecture is shown to only be inefficiently approximated by another.'' ----------------This is partially true, but it neglects important parts of the discussion conducted in the cited papers. --------In particular, the paper [Montufar, Pascanu, Cho, Bengio 2014] discusses not one hard coded function, but classes of functions with a given number of linear regions. --------That paper shows that deep networks generically* produce functions with at least a given number of linear regions, while shallow networks never do. --------* Generically meaning that, after fixing the number of parameters, any function represented by the network, for parameter values form an open, positive -measure, neighbourhood, belongs to the class of functions which have at least a certain number of linear regions. --------In particular, such statements can be directly interpreted in terms of networks with random weights. ----------------- One of the measures for expressivity discussed in the present paper is the number of Dichotomies. In statistical learning theory, this notion is used to define the VC-dimension. In that context, a high value is associated with a high statistical complexity, meaning that picking a good hypothesis requires more data. ----------------- On page 2 one finds the statement ``We discover and prove the underlying reason for this – all three measures are directly proportional to a fourth quantity, trajectory length.'' --------The expected trajectory length increasing exponentially with depth can be interpreted as the increase (or decrease) in the scale by a composition of the form a*...*a x, which scales the inputs by a^d. Such a scaling by itself certainly is not an underlying cause for an increase in the number of dichotomies or activation patterns or transitions. Here it seems that at least the assumptions on the considered types of trajectories also play an important role. --------This is probably related to another observation from page 4: ``if the variance of the bias is comparatively too large... then we no longer see exponential growth.''----------------OTHER SPECIFIC COMMENTS --------In Theorem 1 --------- Here it would be good to be more specific about ``random neural network'', i.e., fixed connectivity structure with random weights, and also about the kind of one-dimensional trajectory, i.e., finite in length, closed, differentiable almost everywhere, etc. ----------------- The notation ``g \geq O(f)'' used in the theorem reads literally as |g| \geq \leq k |f| for some k>0, for large enough arguments. It could also be read as g being not smaller than some function that is bounded above by f, which holds for instance whenever g\geq 0. --------For expressing asymptotic lower bounds one can use the notation \Omega (see https://en.wikipedia.org/wiki/Big_O_notation). ----------------- It would be helpful to mention that the expectation is being taken with respect to the network weights and that these are normally distributed with variance \sigma. ----------------- Theorem 2. Here it would be good to be more specific about the kind of sign transitions. Is this about transitions at any units of the network, or about sign transitions at the scalar output of the entire network. ----------------- Theorem 3 is quite trivial. --------The bijection between transitions and activation patterns is not clear. --------Take a regular n-gon in the plane and a circle that crosses each edge twice. --------This makes 2n transitions but only n+1 activation patterns. ----------------- Theorem 4. --------Where is the proof of this statement? --------How does this relate to the simple fact that each activation pattern corresponds to the vector indicating the units that are `active'? ------------------------MINOR COMMENTS--------- The names of the theorems (e.g. ``Bound on ...'' in Theorem 1) could be separated more clearly from the statements, for instance using bold font, a dot, or parentheses. --------- On page 4, in Latex one can use \gg for the `much larger' symbol. --------- On page 4, explain the notation \delta z_\orth.  --------- On page 4, explain that ``latent image'' refers to the image in the last layer. --------- Why are there no error bars in Figure 2?  --------- On page 5 explain that the hyperplane is in the last hidden layer. --------- On page 5, ``is transitioning for any input''. This is not clearly stated, since a transition takes place at a point in a trajectory of inputs, not for a single input. --------- The y-axis labels in Figure 1 (c) and (d) are too small. --------- Why are there no error bars in Figure 1 (a) and (b)? The caption could at least mention that shown are the averages over experiments. --------- In Figure 4 (b) the curves are occluded by the labels. --------- The numbering of results is confusing. In the Appendix some numbers are repeated with the main part and some are missing. --------- On page 19. Theorem 6. As far as I remember Stanley also provides an elementary proof of case with hyperplanes in general position. Many other works also provide elementary proofs using the same induction arguments in what is known as the sweep hyperplane method. ","While the reviewers saw some value in your contribution, there were also serious issues, so the paper does not reach the acceptance threshold."
https://openreview.net/forum?id=SyK00v5xx,"This paper proposes a simple way to reweight the word embedding in the simple composition function for sentence representation. This paper also shows the connection between this new weighting scheme and some previous work.----------------Here are some comments on technical details:----------------- The word ""discourse"" is confusing. I am not sure whether the words ""discourse"" in ""discourse vector c_s"" and the one in ""most frequent discourse"" have the same meaning.--------- Is there any justification about  related to syntac?--------- Not sure what thie line means: ""In fact the new model was discovered by our detecting the common component c0 in existing embeddings."" in section ""Computing the sentence embedding""--------- Is there any explanation about the results on sentiment in Table 2?",A new method for sentence embedding that is simple and performs well. Important contribution that will attract attention and help move the field forward.
https://openreview.net/forum?id=BJwFrvOeg,"The paper proposes an evolution upon traditional Recurrent Language Models to give the capability to deal with unknown words. It is done by pairing the traditional RNNLM with a module operating on a KB and able to copy from KB facts to generate unseen words. It is shown to be efficient and much better than plain RNNLM on a new dataset.----------------The writing could be improved. The beginning of Section 3 in particular is hard to parse.----------------There have been similar efforts recently (like ""Pointer Sentinel Mixture Models"" by Merity et al.) that attempt to overcome limitations of RNNLMs with unknown words; but they usually do it by adding a mechanism to copy from a longer past history. The proposal of the current paper is different and more interesting to me in that it try to bring knowledge from another source (KB) to the language model. This is harder because one needs to leverage the large scale of the KB to do so. Being able to train that conveniently is nice.----------------The architecture appears sound, but the writing makes it hard to fully understand completely so I can not give a higher rating. ------------------------Other comments:--------* How to cope with the dependency on the KB? Freebase is not updated anymore so it is likely that a lot of the new unseen words in the making are not going to be in Freebase.--------* What is the performance on standard benchmarks like Penn Tree Bank?--------* How long is it to train compare to a standard RNNLM?--------* What is the importance of the knowledge context ?--------* How is initialized the fact embedding  for the first word?--------* When a word from a fact description has been chosen as prediction (copied), how is it encoded in the generation history for following predictions if it has no embedding (unknown word)? In other words, what happens if ""Michelle"" in the example of Section 3.1 is not in the embedding dictionary, when one wants to predict the next word?","This work introduces a combination of a LM with knowledge based retrieval system. This builds upon the recent trend of incorporating pointers and external information into generation, but includes some novelty, making the paper ""different and more interesting"". Generally though the reviewers found the clarity of the work to be sufficiently an issue that no one strongly defended its inclusion.    Pros:  - The reviewers seemed to like the work and particularly the problem space. Issues were mainly on presentation and experiments.     Mixed:  - Reviewers were divided on experimental quality. The work does introduce a new dataset, but reviewers would also have liked use on some existing tasks.     Cons:  - Clarity and writing issues primarily. All reviewers found details missing and generally struggled with comprehension.  - Novelty was a question. Impact of work could also be improved by more clearly defining new contributions"
https://openreview.net/forum?id=HJ0NvFzxl,"This paper proposes learning on the fly to represent a dialog as a graph (which acts as the memory), and is first demonstrated on the bAbI tasks. Graph learning is part of the inference process, though there is long term representation learning to learn graph transformation parameters and the encoding of sentences as input to the graph. This seems to be the first implementation of a differentiable memory as graph: it is much more complex than previous approaches like memory networks without significant gain in performance in bAbI tasks, but it is still very preliminary work, and the representation of memory as a graph seems much more powerful than a stack. Clarity is a major issue, but from an initial version that was constructive and better read by a computer than a human, the author proposed a hugely improved later version. This original, technically accurate (within what I understood) and thought provoking paper is worth publishing.----------------The preliminary results do not tell us yet if the highly complex graph-based differentiable memory has more learning or generalization capacity than other approaches. The performance on the bAbI task is comparable to the best memory networks, but still worse than more traditional rule induction (see http://www.public.asu.edu/~cbaral/papers/aaai2016-sub.pdf). This is still clearly promising.---------------- The sequence of transformation in algorithm 1 looks sensible, though the authors do not discuss any other operation ordering. In particular, it is not clear to me that you need the node state update step T_h if you have the direct reference update step T_h,direct. ----------------It is striking that the only trick that is essential for proper performance is the ‘direct reference’ , which actually has nothing to do with the graph building process, but is rather an attention mechanism for the graph input: attention is focused on words that are relevant to the node type rather than the whole sentence. So the question “how useful are all these graph operations” remain. A much simpler version of a similar trick may have been proposed in the context of memory networks, also for ICLR'17 (see match type in ""LEARNING END-TO-END GOAL-ORIENTED DIALOG"" by Bordes et al)------------------------The authors also mention the time and size needed to train the model: is the issue arising for learning, inference or both? A description of the actual implementation would help  (no pointer to open source code is provide). The author mentions Theano in one of my questions: how are the transformations compiled in advance as units? How is the gradient back-propagated through the graph is this one is only described at runtime?------------------------Typo: in the appendices B.2 and B.2.1, the right side of the equation that applies the update gate has h’_nu while it should be h_nu.----------------In the references, the author could mention the pioneering work  of Lee Giles on representing graphs with  RNNs.----------------Revision: I have improved my rating for the following reasons:--------- Pointers to an highly readable and well structured Theano source is provided.--------- The delta improvement of the paper has been impressive over the review process, and I am confident this will be an impactful paper.--------- Much simpler alternatives approaches such as Memory Networks seem to be plateauing for problems such as dialog modeling, we need alternatives.--------- The architecture is this work is still too complex, but this is often as we start with DNNs, and then find simplifications that actually improve performance","The idea of building a graph-based differentiable memory is very good. The proposed approach is quite complex, but it is likely to lead to future developments and extensions. The paper has been much improved since the original submission. The results could be strengthened, with more comparisons to existing results on bAbI and baselines on the experiments here. Exploring how it performs with less supervision, and different types of supervision, from entirely labeled graphs versus just node labels, would be valuable."
https://openreview.net/forum?id=ryMxXPFex,"Paper proposes a novel Variational Encoder architecture that contains discrete variables. Model contains an undirected discrete component that captures distribution over disconnected manifolds and a directed hierarchical continuous component that models the actual manifolds (induced by the discrete variables). In essence the model clusters the data and at the same time learns a continuous manifold representation for the clusters. The training procedure for such models is also presented and is quite involved. Experiments illustrate state-of-the-art performance on public datasets (including MNIST, Omniglot, Caltech-101). ----------------Overall the model is interesting and could be useful in a variety of applications and domains. The approach is complex and somewhat mathematically involved. It's not exactly clear how the model compares or relates to other RBM formulations, particularly those that contain discrete latent variables and continuous outputs. As a prime example:----------------Graham Taylor and Geoffrey Hinton. Factored conditional restricted Boltzmann machines for modeling motion style. In Proc. of the 26th International Conference on Machine Learning (ICML), 1025–1032, 2009.----------------Discussion of this should certainly be added. ","The authors present a novel reparameterization framework for VAEs with discrete random variables. The idea is to carry out symmetric projections of the approximate posterior and the prior into a continuous space and evaluating the autoencoder term in that space by marginalizing out the discrete variables. They consider the KL divergence between the approximating posterior and the true prior in the original discrete space and show that due to the symmetry of the projection into the continuous space, it does not  contribute to the KL term.     One question that warrants further investigation is whether this framework can be extended to GANs and what empirical success they would have.    The reviewers have presented a strong case for the acceptance of the paper and I go with their recommendation."
https://openreview.net/forum?id=HyWWpw5ex,"The paper seeks to predict user events (interactions with items at a particular point in time). Roughly speaking the contributions are as follows:--------(a) the paper models the co-evolutionary process of users' preferences toward items--------(b) the paper is able to incorporate external sources of information, such as user and item features--------(c) the process proposed is generative, so is able to estimate specific time-points at which events occur--------(d) the model is able to account for non-linearities in the above----------------Following the pre-review questions, I understand that it is the combination of (a) and (c) that is the most novel aspect of the paper. A fully generative process which can be sampled is certainly nice (though of course, non-generative processes like regular old regression can estimate specific time points and such too, so not sure in practice how relevant this distinction is).----------------Other than that the above parts have all appeared in some combination in previous work, though the combination of parts here certainly passes the novelty bar.----------------I hadn't quite followed the issue mentioned in the pre-review discussion that the model requires multiple interactions per userXitem pair in order to fit the model (e.g. a user interacts with the same business multiple times). This is a slightly unusual setting compared to most temporal recommender systems work. I question to some extent whether this problem setting isn't a bit restrictive. That being said I take the point about why the authors had to subsample the Yelp data, but keeping only users with ""hundreds"" of events means that you're left with a very biased sample of the user base.----------------Other than the above issues, the paper is technically nice, and the experiments include strong baselines and reports good performance.","A nice paper, with sufficient experimental validation, and the idea of incorporating a form of change point detection is good. However, the technical contribution relative to the NIPS paper by the same authors is not significant, in that it primarily involves using an RNN instead of a Hawkes process to model the temporal dynamics. The results are significantly better than this earlier paper -- the authors should explore if this is due only to the RNN, or to the optimization method."
https://openreview.net/forum?id=Sk36NgFeg,"This paper is motivated by the ability that human's visual system can recognize contents of environment by from critical features, and tried to investigate whether neural networks can also have this kind of ability.  Specifically, the paper proposed to use Auto-Encoder (AE) as the network to reconstruct the low fidelity of visual input. Moreover, similar to Mnih et al. (2014),  the paper also proposed to use a recurrent fashion to mimic the sequential behavior the  human visual system. ----------------I think the paper is well motivated. However, there are several concerns:--------1. The baselines of the paper are too weak. Nearest neighbor, bilinear, bicubic and cubic interpolations without any learning procedure are of course performed worse than AE based models. The author should compare with the STOA methods such as https://arxiv.org/abs/1609.04802--------2. Can the experiments based on AE support the idea that artificial neural networks can perceive an image from low fidelity? AE is only a kind of neural network, can the conclusion extend to other kind of networks? I think it would be much better if the authors can provide a more general conclusion.","The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers find this direction of exploration to be interesting, but a bit preliminary at the moment. Authors are strongly encouraged to incorporate reviewer comments to make future iterations of the work stronger."
https://openreview.net/forum?id=BkfiXiUlg,"This paper proposes to use a hierarchical softmax to speed up attention based memory addressing in memory augmented network (e.g. NTM, memNN…).----------------The model build a hierarchical softmax on top of the input sequence then at each time step SEARCH for the most relevant input to predict the next output (this search is discrete), and use its corresponding embedding to update the state of an LSTM that will then produce the output. Finally the embedding of the used input is update by a WRITE function (an LSTM working that takes hidden state of the other LSTM as an input). The model has a discrete component (the SEARCH) and is thus trained with REINFORCE. In the experimental section they test their approach on several algorithmic tasks such as search, sort...----------------The main advantage of replacing the full softmax by a hierarchical softmax is that during inference, the complexity goes from O(N) to O(log(N)). It would be great to see if the gain in complexity allows to tackle problem which are a few orders of magnitude bigger than the one addressed with full softmax. However the authors only test on toy sequences up to 32 tokens, which is quite small. ----------------The model requires a relatively complex search mechanism that can only be trained with REINFORCE. While this seems to work on problems with relatively small and simple sequences, it would be great to see how performance changes with the size of the problem. ----------------Overall, while the idea of replacing the softmax in the attention mechanism by a hierachical softmax is appealing, this work is not quite convincing yet. Their approach is not very natural, may be hard to train and may not be that simple to scale. The experiment section is very weak.",All three reviewers point to significant deficiencies. No response or engagement from the authors (for the reviews). I see no basis for supporting this paper.
https://openreview.net/forum?id=HyM25Mqel,"This paper studies the off-policy learning of actor-critic with experience replay. This is an important and challenging problem in order to improve the sample efficiency of the reinforcement learning algorithms. The paper attacks the problem by introducing a new way to truncate importance weight, a modified trust region optimization, and by combining retrace method. The combination of the above techniques performs well on Atari and MuJoCo in terms of improving sample efficiency. My main comment is how does each of the technique contribute to the performance gain? If some experiments could be carried out to evaluate the separate gains from these tricks, it would be helpful. ","First of all, thanks for your kind words.  As to the Equation 4, I think \psi(s_t, a_t) = \grad_\theta(\pi (a_t | s_t) ) /  \pi (a_t | s_t) instead of \psi(s_t, a_t) = \grad_\theta( log \pi (a_t | s_t) ) /  \pi (a_t | s_t). (The log term is missing.)   Therefore, we have \grad_\theta( log  \pi (a_t | s_t) ) =\grad_\theta(\pi (a_t | s_t) )/ \pi (a_t | s_t).  Hopefully that answers your question. :)"
https://openreview.net/forum?id=HJlgm-B9lx,"The authors did not bother responding or fixing any of the pre-review comments. Hence I repeat here:----------------Please do not make incredibly unscientific statements like this one:--------""The working procedure of this model is just like how we human beings read a text and then answer a related question. ""--------Really, ""humans beings"" have an LSTM like model to read a text? Can you cite an actual neuroscience paper for such a claim? The answer is no, so please delete such statements from future drafts.----------------Generally, your experiments are about simple classification and the methods you're competing against are simple models like NB-SVM. So I would change the title, abstract ad introduction accordingly and not attempt hyperbole like ""Learning to Understand"" in the title.----------------Lastly, your attention level approach seems similar to dynamic memory networks by Kumar et al. they also have experiments for sentiment and it would be interesting to understand the differences to your model and compare to them.----------------Other reviewers included further missing related work and fitting this paper into the context of current literature.--------Given that no efforts were made to fix the pre-review questions and feedback, I doubt this will become ready in time for publication.","The consensus amongst reviewers' was that this paper, incorporating global context into classification, is not ready for publication. It provides no novelty over similar methods. The evaluation did not convince most of the reviewers. The paper seems peppered with unjustified and (as rather bluntly, but accurately, put by one reviewer) unscientific claims. Disappointingly, the authors did not respond to pre-review questions. Perhaps more understandably, they did not respond to the uniformly negative reviews of their paper to defend it. I see no reason to diverge from the reviewers' recommendation, and advocate rejection of this paper."
https://openreview.net/forum?id=HJF3iD9xe,"This review is only an informed guess - unfortunately I cannot assess the paper due to my lack of understanding of the paper. --------I have spent several hours trying to read this paper - but it has not been possible for me to follow - partially due to my own limitations, but also I think due to an overly abstract level of presentation. The paper is clearly written, but in the same way that a N. Bourbaki book is clearly written.----------------I would prefer to leave the accept/reject decision to the other reviewers who may have a better understanding - even if the authors had made a serious mistake, I would not be able to tell. My proposal is positive because the paper is apparently clearly written and the empirical evaluation is quite promising. But some effort will be needed in order to address the broader audience that could potentially be interested in the topic. ----------------I therefore would like to provide feedback only at the level of presentation. ----------------My main source of problems is that the authors do not try to ground their abstract formalism with concrete examples; when the examples show up it is by ""revelation"" rather than by explaining how they connect to the previous concepts. ----------------The one example that could unlock most people's understanding is how convolution, or inner product operations connect with the setting described here. For what I know convolution is tied with space (or time) and is understood as an equivariant operation - shifting the signal shifts the output. --------It is not explained how the '(x, x')' pairs used by the authors in order to build relations, structures and then to define invariance relate to this setting. --------Going from sets, to relations, to functions, to operators, and then to shift-invariant operators (convolutions) involves many steps, and some hand-holding is needed.----------------Why is the 3x3 convolution associated to 9 relations? --------Are these relations referring to the input at a given coordinate and its contribution to the output? (w_{offset} x_{i-offset})? In that case, why is there a backward arrow from the center node to the other nodes? And why are there arrows across nodes? --------What is a Cardinal and what is a Cartesian convolution in signal processing terms? (clearly these are not standard terms). --------Are we talking about separable filters? --------What are the X and Square symbols in Figure 2? And what are the horizontal and vertical sub-graphs standing for? What is x_1 and what is x_{11},x_{1,2},x_{1,3} and what is the relationship between them?----------------I realize that to the authors these questions may seem to be trivial and left as  homework for the reader. But I think part of publishing a paper is doing a big part of the homework for the readers so that it becomes easy to get the idea. ----------------Clearly the authors target the more general case - but spending some time to explain how the particular case is an instance of the the general case would be a good use of space. ----------------I would propose that the authors explain what are  x, x_{I}, and x_{S} for the simplest possible example, e.g. convolving a 1x5 signal with a 1x3 filter, how the convolution filter parameters show up in the function f, as well as how the spatial invariance (or, equivariance) of convolution is reflected here. ","This paper studies neural models that can be applied to set-structured inputs and thus require permutation invariance or equivariance. After a first section that introduces necessary and sufficient conditions for permutation invariance/equivariance, the authors present experiments in supervised and semi-supervised learning on point-cloud data as well as cosmology data.    The reviewers agreed that this is a very promising line of work and acknowledged the effort of the authors to improve their paper after the initial discussion phase. However, they also agree that the work appears to be missing more convincing numerical experiments and insights on the choice of neural architectures in the class of permutation-covariant.     In light of these reviews, the AC invites their work to the workshop track.   Also, I would like to emphasize an aspect of this work that I think should be addressed in the subsequent revision.    As the authors rightfully show (thm 2.1), permutation equivariance puts very strong constraints in the class of 1-layer networks. This theorem, while rigorous, reflects a simple algebraic property of matrices that commute with permutation matrices. It is therefore not very surprising, and the resulting architecture relatively obvious. So much so that it already exists in the literature. In fact, it is a particular instance of the graph neural network model of Scarselli et al. '09 (http://ieeexplore.ieee.org/abstract/document/4700287/) when you consider a complete graph, which has been used in the setup of full set equivariance for example in 'Learning Multiagent communication with backpropagation', Sukhbaatar et al NIPS'16; see also 'Order Matters: sequence to sequence for sets', Vinyals et al. https://arxiv.org/abs/1511.06391.   The general question of how to model point-cloud data, or more generally data defined over graphs, with neural networks is progressing rapidly; see for example https://arxiv.org/abs/1611.08097 for a recent survey.    The question then is what is the contribution of the present work relative to this line of work. The authors should answer this question explicitly in the revised manuscript, either with a new application of the model, or with theory that advances our understanding of these models, or with new numerical applications."
https://openreview.net/forum?id=HyenWc5gx,"This paper proposes a regularization technique for neural network training that relies on having multiple related tasks or datasets in a transfer learning setting. The proposed technique is straightforward to describe and can also leverage external labeling systems perhaps based on logical rules. The paper is clearly written and the experiments seem relatively thorough. ----------------Overall this is a nice paper but does not fully address how robust the proposed technique is. For each experiment there seems to be a slightly different application of the proposed technique, or a lot of ensembling and cross validation. I can’t figure out if this is because the proposed technique does not work well in general and thus required a lot of fiddling to get right in experiments, or if this is simply an artifact of ad-hoc experiments to try and get the best performance overall. If more datasets or addressing this issue directly in discussion was able to show this the strengths and limitations of the proposed technique more clearly, this could be a great paper. ----------------Overall the proposed method seems nice and possibly useful for other problems. However in the details of logical rule distillation and various experiment settings it seems like there is a lot of running the model many times or selecting a particular way of reusing the models and data that makes me wonder how robust the technique is or whether it requires a lot of trying various approaches, ensembling, or picking the best model from cross validation to show real gains. The authors could help by discussing this explicitly for all experiments in one place rather than listing the various choices / approaches in each experiment. As an example, these sorts of phrases make me very unsure how reliable the method is in practice versus how much the authors had to engineer this regularizer to perform well:--------“We noticed that equation 8 is actually prone to overfitting away from a good solution on the test set although it often finds a pretty good one early in training. “----------------The introduction section should first review the definitions of transfer learning vs multi-task learning to make the discussion more clear. It also deems justification why “catastrophic forgetting” is actually a problem. If the final target task is the only thing of interest then forgetting the source task is not an issue and the authors should motivate why forgetting matters in their setting. This paper explores sequential transfer so it’s not obvious why forgetting the source task matters.----------------Section 7 introduces the logical rules engine in a fairly specific context. Rather it would be good state more generally what this system entails to help people figure out how this method would apply to other problems.","The proposed approach is not consistently applied for the different experiments; this significantly harms the overall value of the research. The results are also quite domain-specific, and it is not clear if the findings would hold more generally. The paper is not clearly organised or written and does not give a specific enough introduction to the field of transfer learning."
https://openreview.net/forum?id=Bk3F5Y9lx,"The paper presents a version of a variational autoencoder that uses a discrete latent variable that masks the activation of the latent code, making only a subset (an ""epitome"") of the latent variables active for a given sample. The justification for this choice is that by letting different latent variables be active for different samples, the model is forced to use more of the latent code than a usual VAE.--------While the problem of latent variable over pruning is important and has been highlighted in the literature before in the context of variational inference, the proposed solution doesn't seem to solve it beyond, for instance, a mixture of VAEs. Indeed, a mixture of VAEs would have been a great baseline for the experiments in the paper, as it uses a categorical variable (the mixture component) along with multiple VAEs. The main difference between a mixture and an epitomic VAE is the sharing of parameters between the different ""mixture components"" in the epitomic VAE case.--------The experimental section presents misleading results.--------1. The log-likelihood of the proposed models is evaluated with Parzen window estimator. A significantly more accurate lower bound on likelihood that is available for the VAEs is not reported. In reviewer's experience continuous MNIST likelihood of upwards of 900 nats is easy to obtain with a modestly sized VAE.--------2. The exposition changes between dealing with binary MNIST and continuous MNIST experiments. This is confusing, because these versions of the dataset present different challenges for modeling with likelihood-based models. Continuous MNIST is harder to model with high-capacity likelihood optimizing models, because the dataset lies in a proper subspace of the 784-dimensional space (some pixels are always or almost always equal to 0), and hence probability density can be arbitrarily large on this subspace. Models that try to maximize the likelihood often exploit this option of maximizing the likelihood by concentrating the probability around the subspace at the expense of actually modeling the data. The samples of a well-tuned VAE trained on binary MNIST (or a VAE trained on continuous MNIST to which noise has been appropriately added) tend to look much better than the ones presented in experimental results.--------3. The claim that the VAE uses its capacity to ""overfit"" to the training data is not justified. No evidence is presented that the reconstruction likelihood on the training data is significantly higher than the reconstruction likelihood on the test data. It's misleading to use a technical term like ""overfitting"" to mean something else.--------4. The use of dropout in dropout VAE is not specified: is dropout applied to the latent variables, or to the hidden layers of the encoder/decoder? The two options will exhibit very different behaviors.--------5. MNIST eVAE samples and reconstructions look more like a more diverse version of 2d VAE samples/reconstructions - they are blurry, the model doesn't encode precise position of strokes. This is consistent with an interpretation of eVAE as a kind of mixture of smaller VAEs, rather than a higher-dimensional VAE. It is misleading to claim that it outperforms a high-dimensional VAE based on this evidence.----------------In reviewer's opinion the paper is not yet ready for publication. A stronger baseline VAE evaluated with evidence lower bound (or another reliable method) is essential for comparing the proposed eVAE to VAEs.","This paper addresses issues faced when using VAEs and the pruning of latent variables. Improvements to the paper have been accounted for and improved the paper, but after considering the rebuttal and discussion, the reviewers still felt that more was needed, especially in terms of applicability across multiple different data sets. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings."
https://openreview.net/forum?id=Sks3zF9eg,Authors propose using periodic activation functions (sin) instead of tanh for gradient descent training of neural networks.--------This change goes against common sense and there would need to be strong evidence to show that it's a good idea in practice. --------The experiments show slight improvement (98.0 -> 98.1) for some MNIST configurations. They show strong improvement (almost 100% higher accuracy after 1500 iterations) on a toy algorithmic task. It's not clear that this activation function is good for a broad class of algorithmic tasks or just for the two they present. Hence evidence shown is insufficient to be convincing that this is a good idea for practical tasks.,The reviewers unanimously recommend rejecting the paper.
https://openreview.net/forum?id=Sk8csP5ex,"Summary:--------In this paper, the authors study ResNets through a theoretical formulation of a spin glass model. The conclusions are that ResNets behave as an ensemble of shallow networks at the start of training (by examining the magnitude of the weights for paths of a specific length) but this changes through training, through which the scaling parameter C (from assumption A4) increases, causing it to behave as an ensemble of deeper and deeper networks.----------------Clarity:--------This paper was somewhat difficult to follow, being heavy in notation, with perhaps some notation overloading. A summary of some of the proofs in the main text might have been helpful.----------------Specific Comments:--------- In the proof of Lemma 2, I'm not sure where the sequence beta comes from (I don't see how it follows from 11?)----------------- The ResNet structure used in the paper is somewhat different from normal with multiple layers being skipped? (Can the same analysis be used if only one layer is skipped? It seems like the skipping mostly affects the number of paths there are of a certain length?)----------------- The new experiments supporting the scale increase in practice are interesting! I'm not sure about Theorems 3, 4 necessarily proving this link theoretically however, particularly given the simplifying assumption at the start of Section 4.2?","The paper presents an analysis of residual networks and argues that the residual networks behave as ensembles of shallow networks, whose depths are dynamic. The authors argue that their model provides a concrete explanation to the effectiveness of resnets.     However, I have to agree with reviewer 1 that the assumption of path independence is deeply flawed. In my opinion, it was also flawed in the original paper. Using that as a justification to continue this line of research is not the right approach. We cannot construct a single practical scenario where path independence may be expected to hold. So we should not be encouraging papers to continue this line of flawed reasoning.    I thus cannot recommend acceptance of this paper."
https://openreview.net/forum?id=HkNRsU5ge,"The paper presents a method to improve the efficiency of CNNs that encode sequential inputs in a ‘slow’ fashion such that there is only a small change between the representation of adjacent steps in the sequence.--------It demonstrates theoretical performance improvements for toy video data (temporal mnist) and natural movies with a powerful Deep CNN (VGG). ----------------The improvement is naturally limited by the ‘slowness’ of the CNN representation that is transformed into a sigma-delta network: CNNs that are specifically designed to have ‘slow’ representations will benefit most. Also, it is likely that only specialised hardware can fully harness the improved efficiency achieved by the proposed method. Thus as of now, the full potential of the method cannot be thoroughly evaluated.--------However, since the processing of sequential data seems to be a broad and general area of application, it is conceivable that this work will be useful in the design and application of future CNNs.----------------All in all, this paper introduces an interesting idea to address an important topic. It shows promising initial results, but the demonstration of the actual usefulness and relevance of the presented method relies on future work.",The reviewers are in consensus that this paper introduces an interesting idea with potentially huge gains in the efficiency of video analysis tasks (dependent on hardware advances). There was extensive discussion during the question/review period. Two of the reviewers scored this paper in the Top 50% of accepted papers. The lower score is from the less confident reviewer. This seems to the AC to be a clear accept.
https://openreview.net/forum?id=BJ5UeU9xx,"The authors propose a way to visualize which areas of an image provide mostly influence a certain DNN response mostly. They apply some very elegant and convincing improvements to the basic method by Robnik-Sikonja and Konononko from 2008 to DNNs, thus improving it's analysis and making it usable for images and DNNs.----------------The authors provide a very thorough analysis of their methods and show very convincing examples (which they however handpicked. It would be very nice to have maybe at least one figure showing the analysis on e.g. 24 random picks from ImageNet).--------One thing I would like to see is how their method compares to some other methods they mention in the introduction (like gradient-based ones or deconvolution based ones). ----------------They paper is very clearly written, all necessary details are given and the paper is very nice to read.----------------Alltogether: The problem of understanding how DNNs function and how they draw their conclusions is discussed a lot. The author's method provides a clear contribution that can lead to further progress in this field (E.g. I like figure 8 showing how AlexNet, GoogLeNet and VGG differ in where they collect evidence from). I can think of several potential applications of the method and therefore consider it of high significance.----------------Update: The authors did a great job of adopting all of my suggestions. Therefore I improve the rating from 8 to 9.","Reviewers felt the paper was clearly written with necessary details given, leading to a paper that was pleasant to read. The differences with prior art raised by one of the reviewers were adequately addressed by the authors in a revision. The paper presents results on both ImageNet and medical imagery, an aspect of the paper that was appreciated by reviewers."
https://openreview.net/forum?id=B1GOWV5eg,"This paper provides a simple method to handle action repetitions. They make the action a tuple (a,x), where a is the action chosen, and x the number of repetitions. Overall they report some improvements over A3C/DDPG, dramatic in some games, moderate in other. The idea seems natural and there is a wealth of experiment to support it.----------------Comments:----------------- The scores reported on A3C in this paper and in the Mnih et al. publication (table S3) differ significantly. Where does this discrepancy come from? If it's from a different training regime (fewer iterations, for instance), did the authors confirm that running  their replication to the same settings as Mnih et al provide similar results?----------------- It is intriguing that the best results of FiGAR are reported on games where few actions repeat dominate. This seems to imply that for those, the performance overhead of FiGAR over A3C is high since A3C uses an action repeat of 4 (and therefore has 4 times fewer gradient updates). A3C could be run for a comparable computation cost with a lower action repeat, which would probably result in increased performance of A3C.  Nevertheless,  the automatic determination of the appropriate action repeat is interesting, even if the overall message seems to be to not repeat actions too often.----------------- Slightly problematic notation, where r sometimes denotes rewards, sometimes denotes elements of the repetition set R (top of page 5)----------------- In the equation at the bottom of page 5 - since the sum is not indexed over decision steps, not time steps, shouldn't the rewards r_k be modified to be the sum of rewards (appropriately discounted) between those time steps?----------------- The section on DDPG is confusingly written. ""Concatenating"" loss is a strange operation; doesn't FiGAR correspond to a loss to roughly looks like Q(x,mu(x)) + R log p(x) (with separate loss for learning the critic)? It feels that REINFORCE should be applied for the repetition variable x (second term of the sum) and reparametrization for the action a (first term)? ----------------- Is the 'name_this_game' name in the tables  intentional?----------------- A potential weakness of the method is that the agent must decide to commit to an action for a fixed number of steps, independently of what happens next. Have the authors considered a scheme in which, at each time step, the agent decides to stick with the current decision or not? (It feels like it might be a relatively simple modification of FiGAR).","The basic idea of this paper is simple: run RL over an action space that models both the actions and the number of times they are repeated. It's a simple idea, but seems to work really well on a pretty substantial variety of domains, and it can be easily adapted to many different settings. In several settings, the improvement using this approach are dramatic. I think this is an obvious accept: a simple addition to existing RL algorithms that can often perform much better.    Pros:  + Simple and intuitive approach, easy to implement  + Extensive evaluation, showing very good performance    Cons:  - Sometimes unclear _why_ certain domains benefit so much from this"
https://openreview.net/forum?id=SJJN38cge,"This work proposes to use basic probability assignment to improve deep transfer learning. A particular re-weighting scheme inspired by Dempster-Shaffer and exploiting the confusion matrix of the source task is introduced. The authors also suggest learning the convolutional filters separately to break non-convexity. ----------------The main problem with this paper is the writing. There are many typos, and the presentation is not clear. For example, the way the training set for weak classifiers are constructed remains unclear to me despite the author's previous answer. I do not buy the explanation about the use of both training and validation sets to compute BPA. Also, I am not convinced non-convexity is a problem here and the author does not provide an ablation study to validate the necessity of separately learning the filters. One last question is CIFAR has three channels and MNIST only one: How it this handled when pairing the datasets in the second set of experiments?----------------Overall, I believe the proposed idea of reweighing is interesting, but the work can be globally improved/clarified. ----------------I suggest a reject. ","All three reviewers appeared to have substantial difficulties understanding the proposed approach due to unclear presentation. This makes it hard for the reviewers to evaluate the originality and potential merits of the proposed approach, and to assess the quality of the empirical evaluation. I encourage the authors to improve the presentation of the study."
https://openreview.net/forum?id=HyFkG45gl,"The authors describe a system for solving physics word problems. The system consists of two neural networks: a labeler and a classifier, followed by a numerical integrator. On the dataset that the authors synthesize, the full system attains near full performance. Outside of the pipeline, the authors also provide some network activation visualizations.----------------The paper is clear, and the data generation procedure/grammar is rich and interesting. However, overall the system is not well motivated. Why did they consider this particular problem domain, and what challenges did they specifically hope to address? Is it the ability to label sequences using LSTM networks, or the ability to classify what is being asked for in the question? This has already been illustrated, for example, by work on POS tagging and by memory networks for the babi tasks. A couple of standard architectural modifications, i.e. bi-directionality and a content-based attention mechanism, were also not considered.","The paper explores a model for solving simple physics problems (posed in automatically generated natural language). Whilst this an interesting problem, the reviewers worry that the problem is too simple because all problems are automatically generated. The paper should at least incorporate some reasonable baseline models and/or apply the proposed methodology on real physics problems."
https://openreview.net/forum?id=Bkbc-Vqeg,"CONTRIBUTIONS --------This paper introduces a method for learning semantic ""word-like"" units jointly from audio and visual data. The authors use a multimodal neural network architecture which accepts both image and audio (as spectrograms) inputs. Joint training allows one to embed both image and spoken language captions into a shared representation space. Audio-visual groundings are generated by measuring affinity between image patches and audio clips. This allows the model to relate specific visual regions to specific audio segments. Experiments cover image search (audio to image) and annotation (image to audio) tasks and acoustic word discovery.------------------------NOVELTY+SIGNIFICANCE--------As correctly mentioned in Section 1.2, the computer vision and natural language communities have studied multimodal learning for use in image captioning and retrieval. With regards to multimodal learning, this paper offers incremental advancements since it primarily uses a novel combination of input modalities (audio and images).----------------However, bidirectional image/audio retrieval has already been explored by the authors in prior work (Harwath et al, NIPS 2016). Apart from minor differences in data and CNN architecture, the training procedure in this submission is identical to this prior work. The novelty in this submission is therefore the procedure for using the trained model for associating image regions with audio subsequences.----------------The methods employed for this association are relatively straightforward combination of standard techniques with limited novelty. The trained model is used to compute alignment scores between densely sampled image regions and audio subsequences; from these alignment scores a number of heuristics are applied to associate clusters of image regions with clusters of audio subsequences.------------------------MISSING CITATION--------There is a lot of work in this area spanning computer vision, natural language, and speech recognition. One key missing reference:----------------Ngiam, et al. ""Multimodal deep learning."" ICML 2011------------------------POSITIVE POINTS--------- Using more data and an improved CNN architecture, this paper improves on prior work for bidirectional image/audio retrieval--------- The presented method performs efficient acoustic pattern discovery--------- The audio-visual grounding combined with the image and acoustic cluster analysis is successful at discovering audio-visual cluster pairs----------------NEGATIVE POINTS--------- Limited novelty, especially compared with Harwath et al, NIPS 2016--------- Although it gives good results, the clustering method has limited novelty and feels heuristic--------- The proposed method includes many hyperparameters (patch size, acoustic duration, VAD threshold, IoU threshold, number of k-means clusters, etc) and there is no discussion of how these were set or the sensitivity of the method to these choices","This paper received borderline reviews. All reviewers as well as AC agree that the authors pursue a very interesting and less explored direction. The paper essentially addresses the problem of double grounding; visual information helping to group acoustic signal into words, and words helping to localize object-like regions in images. While somewhat hidden under the rug, this is what makes the paper different from the authors' previous work. The reviewers mentioned this to be a minor contribution. The AC agrees with the authors that this is an interesting and novel problem worth studying. However, the AC also agrees with the reviewers that this major novelty over the previous work is missing technical depth. The AC strongly encourages the authors to improve on this aspect of the paper. A simple intuition would be to look at https://arxiv.org/abs/1609.01704 in order to discover words, and use something along the lines of attention (eg https://arxiv.org/abs/1502.03044) to link with image regions."
https://openreview.net/forum?id=rJJRDvcex,"The paper proposes a method of integrating recurrent layers within larger, potentially pre-trained, convolutional networks. The objective is to combine the feature extraction abilities of CNNs with the ability of RNNs to gather global context information.--------The authors validate their idea on two tasks, image classification (on CIFAR-10) and semantic segmentation (on PASCAL VOC12).----------------On the positive side, the paper is clear and well-written (apart from some occasional typos), the proposed idea is simple and could be adopted by other works, and can be deployed as a beneficial perturbation of existing systems, which is practically important if one wants to increase the performance of a system without retraining it from scratch. The evaluation is also systematic, providing a clear ablation study. ----------------On the negative side, the novelty of the work is relatively limited, while the validation is lacking a bit. --------Regarding novelty, the idea of combining a recurrent layer with a CNN, something practically very similar was proposed in Bell et al (2016). There are a few technical differences (e.g. cascading versus applying in parallel the recurrent layers), but in my understanding these are minor changes. The idea of initializing the recurrent network with the CNN is reasonable but is at the level of improving one wrong choice in the original work of Bell, rather than really proposing something novel. --------This contribution ("" we use RNNs within layers"") is repeatedly mentioned in the paper (including intro &  conclusion), but in my understanding was part of Bell et al, modulo minor changes. ----------------Regarding the evaluation, experiments on CIFAR are interesting, but only as proof of concept. ----------------Furthermore, as noted in my early question, Wide Residual Networks (Sergey Zagoruyko, Nikos Komodakis, BMVC16)--------report  better results on CIFAR-10 (4% error), while not using any recurrent layers (rather using instead a wide, VGG-type, ResNet variant). So. --------The authors answer: ""Wide Residual Networks use the depth of the network to spread the receptive field across the entire image (DenseNet (Huang et al., 2016) similarly uses depth). Thus there is no need for recurrence within layers to capture contextual information. In contrast, we show that a shallow CNN, where the receptive field would be limited, can capture contextual information within the whole image if a L-RNN is used.""----------------So, we agree that WRN do not need recurrence - and can still do better. --------The point of my question has practically been whether using a recurrent layer is really necessary; I can understand the answer as being ""yes, if you want to keep your network shallow"".  I do not necessarily see why one would want to keep one's network shallow.----------------Probably an evaluation on imagenet would bring some more insight about the merit of this layer. ------------------------Regarding semantic segmentation, one of my questions has been:--------""Is the boost you are obtaining due to something special to the recurrent layer, or is simply because one is adding extra parameters on top of a pre-trained network? (I admit I may have missed some details of your experimental evaluation)""--------The answer was:--------""...For PASCAL segmentation, we add the L-RNN into a pre-trained network (this adds recurrence parameters), and again show that this boosts performance - more so than adding the same number of parameters as extra CNN layers - as it is able to model long-range dependences""--------I could not find one such experiment in the paper ('more so than adding the same number of parameters as extra CNN layers'); I understand that you have 2048 x 2048 connections for the recurrence, it would be interesting to see what you get by spreading them over (non-recurrent) residual layers.--------Clearly, this is not going to be my criterion for rejection/acceptance, since one can easily make it fail - but I was mostly asking for some sanity check ----------------Furthermore, it is a bit misleading to put in Table 3 FCN-8s and FCN8s-LRNN, since this gives the impression that the LRNN gives a  boost by 10%. In practice the ""FCN8s"" prefix of ""FCN8s-LRNN"" is that of the authors, and not of Long et al (as indicated in Table 2, 8s original is quite worse than 8s here). ----------------Another thing that is not clear to me is where the boost comes from in Table 2; the authors mention that ""when inserting the L-RNN after pool 3 and pool4 in FCN-8s, the L-RNN is able to learn contextual information over a much larger range than the receptive field of pure local convolutions. ""--------This is potentially true, but I do not see why this was not also the case for FCN-32s (this is more a property of the recurrence rather than the 8/32 factor, right?)----------------A few additional points: --------It seems like Fig 2b and Fig2c never made it into the pdf. ----------------Figure 4 is unstructured and throws some 30 boxes to the reader - I would be surprised if anyone is able to get some information out of this (why not have a table?) ----------------Appendix A: this is very mysterious. Did you try other learning rate schedules? (e.g. polynomial)--------What is the performance if you apply a standard training schedule? (e.g. step). --------Appendix C: ""maps .. is"" -> ""maps ... are""","This paper proposes a hybrid architecture that combines traditional CNN layers with separable RNN layers that quickly increase the receptive field of intermediate features. The paper demonstrates experiments on CIfar-10 and semantic segmentation, both by fine-tuning pretrained CNN models and by training them from scratch, showing numerical improvements.     The reviewers agreed that this paper presents a sound modification of standard CNN architectures in a clear, well-presented manner. They also highlighted the clear improvement of the manuscipt between the first draft and subsequent revisions.   However, they also agreed that the novelty of the approach is limited compared to recent works (e.g. Bell'16), despite acknowledging the multiple technical differences between the approaches. Another source of concern is the lack of large-scale experiments on imagenet, which would potentially elucidate the role of the proposed interleaved lrnn modules in the performance boost and demonstrate its usefulness to other tasks.     Based on these remarks, the AC recommends rejection of the current manuscript, and encourages the authors to resubmit the work once the large-scale experiments are completed."
https://openreview.net/forum?id=ByToKu9ll,"The paper compares several defense mechanisms against adversarial attacks: retraining, two kinds of autoencoders and distillation with the conclusion that the retraining methodology proposed by Li et al. works best of those approaches.----------------The paper documents a series of experiments on making models robust against adversarial examples. The methods proposed here are not all too original, RAD was proposed by Li et al, distillation was proposed in Goodfellow et al's ""Explaining and harnessing adversarial examples"", stacked autoencoders were proposed by Szegedy et al's ""Intriguing Properties of Neural Networks"". The most original part of the paper is the improved version of autoencoders proposed in this paper.----------------The paper establishes experimental evidence that the RAD framework provides the best defense mechanism against adversarial attacks which makes the introduction of the improved autoencoder mechanism less appealing.----------------Although the paper establishes interesting measurement points and therefore it has the potential for being cited as a reference, its relative lack of originality decreases its significance.","The paper investigates several retraining approached based upon adversarial data. While the experimental evaluation looks reasonable, the actual contribution of this paper is quite small. The approaches being evaluated, for the most part, are already proposed in the literature, with the one exception being the ""improved autoencoder stacked with classifier"" (IAEC), which is really just a minor modification to the existing AEC approach with an additional regularization term. The results are fairly thorough, and seem to suggest that the IAEC method performs best in some cases, but this is definitely not a novel enough contribution to warrant publication at ICLR.    Pros:  + Nice empirical evaluation of several adversarial retraining methods    Cons:  - Extremely minor algorithmic advances  - Not clear what is the significant contribution of the paper"
https://openreview.net/forum?id=r1Usiwcex,"The paper tackles the task of music generation. They use an orderless NADE model for the task of ""fill in the notes"". Given a roll of T timesteps of pitches, they randomly mask out some pitches, and the model is trained to predict the missing notes. This follows how the orderless NADE model can be trained. During sampling, one normally follows an ancestral sampling procedure. For this, an ordering is defined over outputs, and one runs the model on the current input, samples one of the outputs according to the order, adds this output to the next input, and continues this procedure until all outputs have been sampled. The key point of the paper is that this is a bad sampling strategy. Instead, they suggest the strategy of Yao et al. 2014, which uses a blocked Gibbs sampling approach. The blocked Gibbs strategy instead masks N inputs randomly and independently, samples them, and repeats this procedure. The point of this strategy is the make sure the sampling chain mixes well, which will happen for large N. However, since the samples are independent, having a large N gives incoherent samples. Thus, the authors follow an annealed schedule for N, making it smaller over time, which will eventually reduce to ancestral sampling (giving global structure to the sample). They conduct a variety of experiments involving both normal metrics and human evaluations, and find that this blocked Gibbs sampling outperforms other sampling procedures.----------------This is a well written paper - great job.----------------My main problem with the paper is that having read Uria and Yao, I don't know how much I have learned from this work in the context of this being an ICLR submission. If this was submitted to some computational music / art conference, this paper would be a clear accept. However, for ICLR, I don't see enough novelty compared with previous works this builds upon. Orderless NADE is an established model. The blocked Gibbs sampling and annealing scheme are basically the exact same one used in Yao. Thus, the main novelty of this paper is its application to the music domain, and finding that Yao's method works better for sampling music. This is a good contribution, but more tailored to those working in the music domain. If the authors found that these results also hold for other domains like images (e.g. on CIFAR / tiny Imagenet) and text (e.g. document generation), then I would change my mind and accept this paper for ICLR. Even just trying musical domains other than Bach chorales would be useful. However, as it stands, the experiments are not convincing enough.","This paper applies an existing idea (Yao's block Gibbs sampling of NADE) to a music model. There is also prior art for applying NADE to music. The main novel and interesting result is that block Gibbs sampling (an approximation) actually improves performance, highlighting problems with NADE.    This work is borderline for inclusion. The paper is mainly an application of existing ideas. The implications of the interesting results could perhaps have been explored further for a paper at a meeting on learning representations."
https://openreview.net/forum?id=rJe-Pr9le,"This paper proposes a new approach to model based reinforcement learning and--------evaluates it on 3 ATARI games. The approach involves training a model that--------predicts a sequence of rewards and probabilities of losing a life given a--------context of frames and a sequence of actions. The controller samples random--------sequences of actions and executes the one that balances the probabilities of--------earning a point and losing a life given some thresholds. The proposed system--------learns to play 3 Atari games both individually and when trained on all 3 in a--------multi-task setup at super-human level.----------------The results presented in the paper are very encouraging but there are many--------ad-hoc design choices in the design of the system. The paper also provides--------little insight into the importance of the different components of the system.----------------Main concerns:--------- The way predicted rewards and life loss probabilities are combined is very ad-hoc.--------  The natural way to do this would be by learning a Q-value, instead different--------  rules are devised for different games.--------- Is a model actually being learned and improved? It would be good to see--------  predictions for several actions sequences from some carefully chosen start--------  states. This would be good to see both on a game where the approach works and--------  on a game where it fails. The learning progress could also be measured by--------  plotting the training loss on a fixed holdout set of sequences.--------- How important is the proposed RRNN architecture? Would it still work without--------  the residual connections? Would a standard LSTM also work?----------------Minor points:--------- Intro, paragraph 2 - There is a lot of much earlier work on using models in--------  RL. For example, see Dyna and ""Memory approaches to reinforcement learning in--------  non-Markovian domains"" by Lin and Mitchell to name just two.--------- Section 3.1 - Minor point, but using a_i to represent the observation is--------  unusual.  Why not use o_i for observations and a_i for actions?--------- Section 3.2.2 - Notation again, r_i was used earlier to represent the--------  reward at time i but it is being used again for something else.--------- Observation 1 seems somewhat out of place. Citing the layer normalization--------  paper for the motivation is enough.--------- Section 3.2.2, second last paragraph - How is memory decoupled from--------  computation here? Models like neural turning machines accomplish this by using--------  an external memory, but this looks like an RNN with skip connections.--------- Section 3.3, second paragraph - Whether the model overfits or not depends on--------  the data. The approach doesn't work with demonstrations precisely because it--------  would overfit.--------- Figure 4 - The reference for Batch Normalization should be Ioffe and Szegedy--------  instead of Morimoto et al.----------------Overall I think the paper has some really promising ideas and encouraging--------results but is missing a few exploratory/ablation experiments and some polish.","The authors have proposed a new method for deep RL that uses model-based evaluation of states and actions and reward/life loss predictions. The evaluation, on just 3 ATari games with no comparisons to state of the art methods, is insufficient, and the method seems ad-hoc and unclear. Design choices are not clearly described or justified. The paper gives no insight as to how the different aspects of the approach relate or contribute to the overall results."
https://openreview.net/forum?id=HkpbnH9lx,"This paper presents a clever way of training a generative model which allows for exact inference, sampling and log likelihood evaluation. The main idea here is to make the Jacobian that comes when using the change of variables formula (from data to latents) triangular - this makes the determinant easy to calculate and hence learning possible. --------The paper nicely presents this core idea and a way to achieve this - by choosing special ""routings"" between the latents and data such that part of the transformation is identity and part some complex function of the input (a deep net, for example) the resulting Jacobian has a tractable structure. This routing can be cascaded to achieve even more complex transformation. ----------------On the experimental side, the model is trained on several datasets and the results are quite convincing, both in sample quality and quantitive measures. --------I would be very happy to see if this model is useful with other types of tasks and if the resulting latent representation help with classification or inference such as image restoration.----------------In summary - the paper is nicely written, results are quite good and the model is interesting - I'm happy to recommend acceptance.","This paper clearly lays out the recipe for a family of invertible density models, explores a few extensions, and demonstrates the overall power of the approach. The work is building closely on previous approaches, so it suffers a bit in terms of originality. However, I expect this overall approach to become a standard tool for model-building.    The main weakness of this paper is that it did not explore the computational tradeoffs of this approach against related methods. However, the paper already had a lot of content."
https://openreview.net/forum?id=Bk8BvDqex,"This paper introduces an approach to reinforcement learning and control wherein, rather than training a single controller to perform a task, a metacontroller with access to a base-level controller and a number of accessory « experts » is utilized. The job of the metacontroller is to decide how many times to call the controller and the experts, and which expert to invoke at which iteration. (The controller is a bit special in that in addition to being provided the current state, it is given a summary of the history of previous calls to itself and previous experts.) The sequence of controls and expert advice is embedded into a fixed-size vector through an LSTM. The method is tested on an N-body  control task, where it is shown that there are benefits to multiple iterations (« pondering ») even for simple experts, and that the metacontroller can deliver accuracy and computational cost benefits over fixed-iteration controls.----------------The paper is in general well written, and reasonably easy to follow. As the authors note, the topic of metareasoning has been studied to some extent in AI, but its use as a differentiable and fully trainable component within an RL system appears new. At this stage, it is difficult to evaluate the impact of this kind of approach: the overall model architecture is intriguing and probably merits publication, but whether and how this will scale to other domains remains the subject of future work. The experimental validation is interesting and well carried out, but remains of limited scope. Moreover, given such a complex architecture, there should be a discussion of the training difficulties and convergence issues, if any.----------------Here are a few specific comments, questions and suggestions:----------------1) in Figure 1A, the meaning of the graphical language should be explained. For instance, there are arrows of different thickness and line style — do these mean different things? ----------------2) in Figure 3, the caption should better explain the contents of the figure. For example, what do the colours of the different lines refer to? Also, in the top row, there are dots and error bars that are given, but this is explained only in the « bottom row » part. This makes understanding this figure difficult.----------------3) in Figure 4, the shaded area represents a 95% confidence interval on the regression line; in addition, it would be helpful to give a standard error on the regression slope (to verify that it excludes zero, i.e. the slope is significant), as well as a fraction of explained variance (R^2). ----------------4) in Figure 5, the fraction of samples using the MLP expert does not appear to decrease monotonically with the increasing cost of the MLP expert (i.e. the bottom left part of the right plot, with a few red-shaded boxes). Why is that? Is there lots of variance in these fractions from experiment to experiment?----------------5) the supplementary materials are very helpful. Thank you for all these details.","This paper fairly clearly presents a totally sensible idea. The details of the method presented in this paper are clearly preliminary, but is enough to illustrate a novel approach."
https://openreview.net/forum?id=ryAe2WBee,"The paper presents the semantic embedding model for multi-label prediction.--------In my questions, I pointed that the proposed approach assumes the number of labels to predict is known, and the authors said this was an orthogonal question, although I don't think it is!--------I was trying to understand how different is SEM from a basic MLP with softmax output which would be trained with a two step approach instead of stochastic gradient descent. It seems reasonable given their similarity to compare to this very basic baseline.--------Regarding the sampling strategy to estimate the posterior distribution, and the difference with Jean et al, I agree it is slightly different but I think you should definitely refer to it and point to the differences.--------One last question: why is it called ""semantic"" embeddings? usually this term is used to show some semantic meaning between trained embeddings, but this doesn't seem to appear in this paper.","This is largely a well written paper proposing a sensible approach for multilabel learning that is shown to be effective in practice. However, the main technical elements of this work: the model used and its connections to basic MLPs and related methods in the literature, the optimization strategy, and the speedup tricks are all familiar from prior work. Hence the reviewers are somewhat unanimous in their view that the novelty aspect of this paper is its main shortcomings. The authors are encouraged to revise the paper and clarify the precise contributions."
https://openreview.net/forum?id=SkpSlKIel,The main contribution of this paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network that uses O(log(1/eps)) layers and O(poly log(1/eps)) hidden units where the activation functions can be either ReLU or binary step or any combination of them. The paper is well written and clear. The arguments and proofs are easy to follow. I only have two questions:----------------1- It would be great to have similar results without binary step units. To what extent do you find the binary step unit central to the proof?----------------2- Is there an example of piecewise smooth function that requires at least poly(1/eps) hidden units with a shallow network?,"The paper makes a solid technical contribution in proving that the deep networks are exponentially more efficient in function approximation compared to the shallow networks. They take the case of piecewise smooth networks, which is practically motivated (e.g. images have edges with smooth regions), and analyze the size of both the deep and shallow networks required to approximate it to the same degree.    The reviewers recommend acceptance of the paper and I am happy to go with their recommendation."
https://openreview.net/forum?id=ByC7ww9le,"SUMMARY.----------------The paper propose a new scoring function for knowledge base embedding.--------The scoring function called TransGaussian is an novel take on (or a generalization of) the well-known TransE scoring function.--------The proposed function is tested on two tasks knowledge-base completion and question answering.------------------------------------------OVERALL JUDGMENT--------While I think this proposed work is very interesting and it is an idea worth to explore further, the presentation and the experimental section of the paper have some problems.--------Regarding the presentation, as far as I understand this is not an attention model as intended standardly in the literature.--------Plus, it has hardly anything to share with memory networks/neural Turing machines, the parallel that the authors try to make is not very convincing.--------Regarding the experimental section, for a fair comparison the authors should test their model on standard benchmarks, reporting state-of-the-art models.--------Finally, the paper lack of discussion of results and insights on the behavior of the proposed model.--------------------------------------------------DETAILED COMMENTS------------------------In section 2.2 when the authors calculate \mu_{context} do not they loose the order of relations? And if it is so, does it make any sense?","Three knowledgable reviewers recommend rejection. While they agree that the paper has interesting aspects, they suggest a more convincing evaluation. The authors did not address some of the reviewer's concerns. The AC strongly encourages the authors to improve their paper and resubmit it to a future conference."
https://openreview.net/forum?id=BkUDvt5gg,"There have been numerous works on learning from raw waveforms and training letter-based CTC networks for speech recognition, however, there are very few works on combining both of them with purely ConvNet as it is done in this paper. It is interesting to see results on a large scale corpus such as Librispeech that is used in this paper, though some baseline results from hybrid NN/HMM systems should be provided. To readers, it is unclear how this system is close to state-of-the-art only from Table 2.----------------The key contribution of this paper may be the end-to-end sequence training criterion for their CTC variant (where the blank symbol is dropped), which may be viewed as sequence training of CTC as H. Sak, et al. ""Learning acoustic frame labeling for speech recognition with recurrent neural networks"", 2015. However, instead of generating the denominator lattices using a frame-level trained CTC model first, this paper directly compute the sequence-level loss by considering all the competing hypothesis in the normalizer. Therefore, the model is trained end-to-end. From this perspective, it is closely related to D. Povey's LF-MMI for sequence-training of HMMs. As another reviewer has pointed out, references and discussions on that should be provided. ----------------This approach should be more expensive than frame-level training of CTCs, however, from Table 1, the authors' implementation is much faster. Did the systems there use the same sampling rate? You said at the end of 2.2 that the step size for your model is 20ms. Is it also the same for Baidu's CTC system. Also, have you tried increasing the step size, e.g. to 30ms or 40ms, as people have found that it may work (equally) better, while significantly cut down the computational cost.","Without revisions to this paper or a rebuttal from the authors, it is hard to accept this paper. The main contribution of the paper is removing the blank from CTC to create a somewhat different criterion, but this is not particularly novel (see, for example, http://www.isca-speech.org/archive/Interspeech_2016/pdfs/1446.PDF). None of the reviewers were willing to argue for acceptance in the deliberation phase, so unfortunately the recommendation must be to reject this paper."
https://openreview.net/forum?id=rkpACe1lx,"This paper proposes an interesting new method for training neural networks, i.e., a hypernetwork is used to generate the model parameters of the main network. The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task. In particular, the hyperLSTM with non-shared weights can achieve excellent results compared to conventional LSTM and its variants on a couple of LM talks, which is very inspiring.    ------------------pros----------------This work demonstrates that it is possible to generate the neural network model parameters using another network that can achieve competitive results by a few relative large scale experiments. The idea itself is very inspiring, and the experiments are very solid.------------------cons----------------The paper would be much stronger if it was more focused. In particular, it is unclear what is the key advantage of this hypernetwork approach. It is argued that in the paper that can achieve competitive results using smaller number of trainable model parameters. However, in the running time, the computational complexity is the same as the standard main network for static networks, such as ConvNet, and the computational cost is even larger for dynamic networks such as LSTMs. The improvements of hyperLSTMs over conventional LSTM and its variants seem mainly come from increasing the number of model parameters.------------------minor question,---------------- The ConvNet and LSTM used in the experiments do not have a large softmax layer. For most of the word-level tasks for either LM or MT, the softmax layer could be more than 100K. Is it going to be challenging for the hyperNetwork generate large number of weights for that case, and is it going to slowing the training down significantly?","The paper contains an interesting idea, and after the revision of 3rd Jan, the presentation is clear enough as well. (Although I find it now contains an odd repetition where related work is presented first in section 2, and then later in section 3.2)."
https://openreview.net/forum?id=r1Ue8Hcxg,"This paper explores an important part of our field, that of automating architecture search. While the technique is currently computationally intensive, this trade-off will likely become better in the near future as technology continues to improve.----------------The paper covers both standard vision and text tasks and tackle many benchmark datasets, showing there are gains to be made by exploring beyond the standard RNN and CNN search space. While one would always want to see the technique applied to more datasets, this is already far more sufficient to show the technique is not only competitive with human architectural intuition but may even surpass it. This also suggests an approach to tailor the architecture to specific datasets without resulting in hand engineering at each stage.----------------This is a well written paper on an interesting topic with strong results. I recommend it be accepted.","The paper reports that ""[a]fter the controller RNN is done training, we take the best RNN cell according to the lowest validation perplexity and then run a grid search over learning rate, weight initialization, dropout rates and decay epoch. The best cell found was then run with three different configurations and sizes to increase its capacity.""  Is it possible for you to include (or provide here) the hyperparameters and type of dropout (i.e. recurrent dropout, embedding dropout, ...) used? Without them, replication would at best require a great deal of trial and error. As with ""Recurrent Neural Network Regularization"" (Zaremba et al., 2014), releasing a base set of hyper parameters greatly assists in the future work of the field.  This will likely also be desired for the other experiments, such as character LM."
https://openreview.net/forum?id=SJZAb5cel,"The authors propose a transfer learning approach applied to a number of NLP tasks; the set of tasks appear to have an order in terms of complexity (from easy syntactic tasks to somewhat harder semantic tasks).----------------Novelty: the way the authors propose to do transfer learning is by plugging models corresponding to each task, in a way that respects the known hierarchy (in terms of NLP ""complexity"") of those tasks. In that respect, the overall architecture looks more like a cascaded architecture than a transfer learning one. There are some existing literature in the area (first two Google results found: https://arxiv.org/pdf/1512.04412v1.pdf, (computer vision) and https://www.aclweb.org/anthology/P/P16/P16-1147.pdf (NLP)). In addition to the architecture, the authors propose a regularization technique they call ""successive regularization"".----------------Experiments:----------------- The authors performed a number of experimental analysis to clarify what parts of their architecture are important, which is very valuable;----------------- The information ""transferred"" from one task to the next one is represented both using a smooth label embedding and the hidden representation of the previous task. At this point there is no analysis of which one is actually important, or if they are redundant (update: the authors mentioned they would add something there). Also, it is likely one would have tried first to feed label scores from one task to the next one, instead of using the trick of the label embedding -- it is unclear what the latter is actually bringing.----------------- The successive regularization does not appear to be important in Table 8; a variance analysis would help to conclude.","There is a bit of spread in the reviewer scores, but ultimately the paper does not meet the high bar for acceptance to ICLR. The lack of author responses to the reviews does not help either."
https://openreview.net/forum?id=ryT9R3Yxe,"This work reframes paragraph vectors from a generative point of view and in so doing, motivates the existing method of inferring paragraph vectors as well as applying a L2 regularizer on the paragraph embeddings. The work also motivates joint learning of a classifier on the paragraph vectors to perform text classification.----------------The paper has numerous citation issues both in formatting within the text and the formatting of the bibliography, e.g. on some occasions including first names, on others not. I suggest the authors use a software package like BibTex to have a more consistent bibliography. There seems to be little novelty in this work. ----------------The authors claim that there is no proposed method for inferring unseen documents for paragraph vectors. This is untrue. In the original paragraph vector paper, the authors show that to get a new vector, the rest of the model parameters are held fixed and gradient descent is performed on the new paragraph vector. This means the original dataset is not needed when inferring a paragraph vector for new text. This work seems to be essentially doing the same thing when finding the MAP estimate for a new vector. Thus the only contribution from the generative paragraph vector framing is the regularization on the embedding matrix.----------------The supervised generative paragraph vector amounts to jointly training a linear classifier on the paragraph vectors, while inference for the paragraph vector is unchanged. For the n-gram based approach, the authors should cite Li et al., 2015.----------------In the experiments, table 1 and 2 are badly formatted with .0 being truncated. The authors also do not state the size of the paragraph vector. Finally the SGPV results are actually worse than that reported in the original paragraph vector paper where SST-1 got 48.7 and SST-2 got 86.3.----------------Bofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao, Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews, 2015.","The contribution of this paper generally boils down to adding a prior to the latent representations of the paragraph in the Paragraph Vector model. An especially problematic point about this paper is the claim that the original paper considered only the transductive setting (i.e. it could not induce representations of new documents). It is not accurate, they also used gradient descent at test time. Though I agree that regularizing the original model is a reasonable thing to do, I share the reviewers' feeling that the contribution is minimal. There are also some serious issues with presentation (as noted by the reviewers), I am surprised that the authors have not addressed them during the review period."
https://openreview.net/forum?id=SkC_7v5gx,"The paper is about channel sparsity in Convolution layer.--------The paper is well written and it elaborately discussed and investigated different approaches for applying sparsity.  The paper contains detailed literature review.--------In result section, it showed the approach gives good results using 60% sparsity with reducing number of parameters, which can be useful in some embedded application with limited resource i.e. mobile devices.--------The main point is that the paper needs more detailed investigation on different dropout schedule.--------As mentioned implementation details section, they deactivate the connections by applying masks to parameter tensors, which is not helpful in speeding up the training and computation in convolution layer. They can optimize implementation to reduce computation time.","The reviewers agreed that the main contribution is the first empirical analysis on large-scale convolutional networks concerning layer-to-layer sparsity. The main concerns were that of novelty (connection-wise sparsity being explored previously but not in large-scale domains) and the importance given the current state of fast implementations of sparsely connected CNNs. The authors argued that the point of their paper was to drive software/hardware co-evolution and guide the next generation of, e.g. CUDA tools development. The confident scores were borderline while the less confident reviewer was pleased with the paper so I engaged the reviewers in a discussion post author-feedback. The consensus was that the paper presented promising but not fully developed nor convincing research."
https://openreview.net/forum?id=ryCcJaqgl,"Revision of the review:--------The authors did a commendable job of including additional references and baseline experiments.-----------------------------------This paper presents a hybrid architecture for time series prediction, focusing on the slope and duration of linear trends. The architecture consists of combining a 1D convnet for local time series and an LSTM for time series of trend descriptors. The convnet and LSTM features are combined into an MLP for predicting either the slope or the duration of the next trend in a 1D time series. The method is evaluated on 3 small datasets.----------------Summary:--------This paper, while relative well written and presenting an interesting approach, has several methodology flaws, that should be handled by new experiments.----------------Pros:--------The idea of extracting upward or downward trends from time series - although these should, ideally be learned, not rely on an ad-hoc technique, given that this is a submission for ICLR.----------------Methodology:--------* In section 3, what do you mean by predicting “either [the duration] -------- or [slope] --------” of the trend? Predictions are valid only if those two predictions are done jointly. The two losses should be combined during training.--------* In the entire paper, the trend slope and duration need to be predicted jointly. Predicting a time series without specifying the horizon of the prediction is meaningless. If the duration of the trends is short, the time series could go up or down alternatively; if the duration of the trend is long, the slope might be close to zero. Predictions at specific horizons are needed.--------* In general, time series prediction for such applications as trading and load forecasting is pointless if no decision is made. A trading strategy would be radically different for short-term and noisy oscillations or from long-term, stable upward or downward trend. An actual evaluation in terms of trading profit/loss should be added for each of the baselines, including the naïve baselines.--------* As mentioned earlier in the pre-review questions, an important baseline is missing: feeding the local time series to the convnet and connecting the convnet directly to the LSTM, without ad-hoc trend extraction.--------* The convnet -> LSTM architecture would need an analysis of the convnet filters and trend prediction representation.--------* Trend prediction/segmentation by the convnet could be an extra supervised loss.--------* The detailed analysis of the trend extraction technique is missing.--------* In section 5, the SVM baselines have local trend and local time series vectors concatenated. Why isn’t the same approach used for LSTM baselines (as a multivariate input) and why the convnet operates only on local --------* An important, “naïve” baseline is missing: next local trend slope and duration = previous local trend slope and duration.----------------Missing references:--------The related work section is very partial and omits important work in hybrid convnet + LSTM architectures, in particular:--------Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural image caption generator. CoRR, abs/1411.4555, 2014.--------Donahue, Jeff, Hendricks, Lisa Anne, Guadarrama, Sergio, Rohrbach, Marcus, Venugopalan, Subhashini, Saenko, Kate, and Darrell, Trevor. Long-term recurrent convolutional networks for visual recognition and description. CoRR, abs/1411.4389, 2014.--------Karpathy, Andrej, Toderici, George, Shetty, Sanketh, Leung, Thomas, Sukthankar, Rahul, and Fei-Fei, Li. Large-scale video classification with convolutional neural networks. In CVPR, 2014.----------------The organization of the paper needs improvement:--------* Section 3 does not explain the selection of the maximal tolerable variance in each trend segment. The appendix should be moved to the core part of the paper.--------* Section 4 is unnecessarily long and gives well known details and equations about convnets and LSTMs. The only variation from standard algorithm descriptions is that   are concatenated. The feature fusion layer can be expressed by a simple MLP on the concatenation of R(T(l)) and C(L(t)). Details could be moved to the appendix.----------------Additional questions:--------*In section 5, how many datapoints are there in each dataset? Listing only the number of local trends is uninformative.----------------Typos:--------* p. 5, top “duration and slop”","I appreciate the authors putting a lot of effort into the rebuttal. But it seems that all the reviewers agree that the local trend features segmentation and computation is adhoc, and the support for accepting the paper is lukewarm.    As an additional data point, I would argue that the model is not end-to-end since it doesn't address the aspect of segmentation. Incorporating that into the model would have made it much more interesting and novel."
https://openreview.net/forum?id=ryjp1c9xg,"Overall the paper has the feel of a status update by some of the best researchers in the field. The paper is very clear, the observations are interesting, but the remarks are scattered and don't add up to a quantum of progress in the study of what can be done with the Neural GPU model.----------------Minor remark on the use of the term RNN in Table 1: I found Table 1 confusing because several of the columns are for models that are technically RNNs, and use of RNNs for e.g. translation and word2vec highlight that RNNs can be characterized in terms of the length of their input sequence, the length of their input, and the sizes (per step) of their input, output, and working memories.----------------Basic model question: How are inputs presented (each character 1-hot?) and outputs retrieved when there are e.g. 512 “filters” in the model ?  If inputs and outputs are 1-hot encoded, and treated with the same filters as intermediate layers, then the intermediate activation functions should be interpretable as digits, and we should be able to interpret the filters as implementing a reliable e.g. multiplication-with-carry algorithm. Looking at the intermediate values may shed some light on why the usually-working models fail on e.g. the pathological cases identified in Table 3.----------------The preliminary experiment on input alignment is interesting in two ways: the seeds for effective use of an attentional mechanism are there, but also, it suggests that the model is not presently dealing with general expression evaluation the way a correct algorithm should.----------------The remarks in the abstract about improving the memory efficiency of Neural GPU seem overblown -- the paragraph at the top of page 6 describes the improvements as using tf.while_loop instead of unrolling the graph, and using swap_memory to use host memory when GPU memory runs short. These both seem like good practice, but not a remarkable improvement to the efficiency of the model, in fact it would likely slow down training and inference when memory does in fact fit in the GPU.----------------The point about trying many of random seeds to get convergence makes me wonder if the Neural GPU is worth its computational cost at all, when evaluated as means of learning algorithms that are already well understood (e.g. parsing and evaluating S-exprs). Consider spending all of the computational cycles that go into training one of these models (with the multiple seeds) on a traditional search through program space (e.g. sampling lisp programs or something).----------------The notes on the curriculum strategies employed to get the presented results were interesting to read, as an indication of the lengths to which someone might have to go to train this sort of model, but it does leave this reviewer with the impression that despite the stated extensions of the Neural GPU model it remains unclear how useful it might be to practical problems.","This paper is clearly written, and contains original observations on the properties of the neural GPU model. These observations are an important part of research, and sharing them (and code) will help the field move forward. However, these observations do not quite add up to a coherent story, nor is a particular set of hypotheses explored in depth. So the main problem with this paper is that it doesn't fit the 'one main idea' standard format of papers, making it hard to build on this work.    The other big problem with this paper is the lack of comparison to similar architectures. There is lots of intuition given about why the NGPU should work better than other architectures in some situations, but not much empirical validation of this idea."
https://openreview.net/forum?id=BJAFbaolg,"Summary:--------This paper introduces a heuristic approach for training a deep directed generative model, where similar to the transition operator of a Markov chain each layer samples from the same conditional distribution. Similar to optimizing a variational lower bound, the approach is to approximate the gradient by replacing the posterior over latents with an alternative distribution. However, the approximating distribution is not updated to improve the lower bound but heuristically constructed in each step. A further difference to variational optimization is that the conditional distributions are optimized greedily rather than following the gradient of the joint log-likelihood.----------------Review:--------The proposed approach is interesting and to me seems worth exploring more. Given that there are approaches for training the same class of models which are 1) theoretically more sound, 2) of similar computational complexity, and 3) work well in practice (e.g. Rezende & Mohamed, 2015), I am nevertheless not sure of its potential to generate impact. My bigger concern, however, is that the empirical evaluation is still quite limited.----------------I appreciate the authors included proper estimates of the log-likelihood. This will enable and encourage future comparisons with this method on continuous MNIST. However, the authors should point out that the numbers taken from Wu et al. (2016) are not representative of the performance of a VAE. (From the paper: “Therefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.” “Such observation models can easily achieve much higher log-likelihood scores, […].”)----------------Comparisons with inpainting results using other methods would have been nice. How practical is the proposed approach compared to other approaches? Similar to the diffusion approach by Sohl-Dickstein et al. (2015), the proposed approach seems to be both efficient and effective for inpainting. Not making this a bigger point and performing the proper evaluations seems like a missed opportunity.----------------Minor:--------– I am missing citations for “ordered visible dimension sampling”--------– Typos and frequent incorrect use of \citet and \citep","Infusion training is a new, somewhat heuristic, procedure for training deep generative models. It's an interesting novel idea and a good paper, which has also been improved after the authors have been responsive to reviewer feedback."
https://openreview.net/forum?id=ryPx38qge,"In this paper, the authors explicitly design geometrical structure into a CNN by combining it with a Scattering network. This aids stability and limited-data performance. The paper is well written, the contribution of combining Scattering and CNNs is novel and the results seem promising. I feel that such work was a missing piece in the Scattering literature to make it useful for practical applications.----------------I wish the authors would have investigated the effect of the stable bottom layers with respect to adversarial examples. This can be done in a relatively straightforward way with software like cleverhans [1] or deep fool [2]. It would be very interesting if the first layer's stability in the hybrid architectures increases robustness significantly, as this would tell us that these fooling images are related to low-level geometry. Finding that this is not the case, would be very interesting as well.----------------Further, the proposed architecture is not evaluated on real limited data problems. This would further strengthen the improved generalization claim. However, I admit that the Cifar-100 / Cifar-10 difference already seems like a promising indicator in this regard.----------------If one of the two points above will be addressed in an additional experiment, I would be happy to raise my score from 6 to 7.----------------Summary: ----------------+ An interesting approach is presented that might be useful for real-world limited data scenarios.--------+ Limited data results look promising.--------- Adversarial examples are not investigated in the experimental section.--------- No realistic small-data problem is addressed.----------------Minor:--------- The authors should add a SOTA ResNet to Table 3, as NiN is indeed out of fashion these days.--------- Some typos: tacke, developping, learni.----------------[1] https://arxiv.org/abs/1610.00768v3--------[2] https://arxiv.org/abs/1511.04599","The program committee appreciates the authors' response to concerns raised in the reviews. Reviewers are generally excited about the combination of predefined representations with CNN architectures, allowing the model to generalize better in the low data regime. This was an extremely borderline paper, and the PCs have determined that the paper would have needed to be further revised and should be rejected."
https://openreview.net/forum?id=B1G9tvcgx,"This paper proposes a multimodal neural machine translation that is based upon previous work using variational methods but attempts to ground semantics with images. Considering way to improve translation with visual information seems like a sensible thing to do when such data is available. ----------------As pointed out by a previous reviewer, it is not actually correct to do model selection in the way it was done in the paper. This makes the gains reported by the authors very marginal. In addition, as the author's also said in their question response, it is not clear if the model is really learning to capture useful image semantics. As such, it is unfortunately hard to conclude that this paper contributes to the direction that originally motivated it.",The area chair agrees with the reviewers that this paper is not of sufficient quality for ICLR. The experimental results are weak (there might be even be some issues with the experimental methodology) and it is not at all clear whether the translation model benefits from the image data. The authors did not address the final reviews.
https://openreview.net/forum?id=ryb-q1Olg,"This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 biclustering methods, However, my first concern is that from the methodological point of view, the novelty of the proposed method seems small. The authors replied to the same question which another reviewer gave, but the replies were not so convincing. ----------------This paper was actually difficult for me to follow. For instance, in Figure 1, a bicluster matrix is constructed as an outer product of  and .  is a hidden unit, but what is ? I could not find any definition. Furthermore, I could not know how  is estimated in this method. Therefore, I do NOT understand how this method performs biclustering. ----------------Totally, I am not sure that this paper is suitable for publication. ----------------Prons:--------Empirical performance is good.----------------Cons:--------Novelty of the proposed method--------Some description in the paper is unclear.","The reviewers pointed out several issues with the paper, and all recommended rejection."
https://openreview.net/forum?id=ByW2Avqgg,"The authors extend their method of causal discovery (Chalupka et al 2016) to include assumptions about sparsity via regularization.  They apply this extension to an interesting private dataset from Sutter Health.  While an interesting direction, I found the presentation somewhat confused, the methodological novelty smaller than the bulk of ICLR works, and the central results (or perhaps data; see below) inadequate to address questions of causality.----------------First, I found the presentation somewhat unclear.  The paper at some points seems to be entirely focused on healthcare data, at other points it uses it as a motivating example, and at other points it is neglected.  Also, algorithm 1 seems unreferenced, and I'm not entirely sure why it is needed.  Figure 2 is not needed for this community.  The key methodological advance in this work appears in section 2.1 (Causal regularizer), but it is introduced amidst toy examples and without clear terminology or standard methodological assumptions/build-up.  In Section 3.1 (bottom of first paragraph), key data and results seem to be relegated to the appendices.  Thus overall the paper read rather haphazardly.  Finally, there seems to be an assumption throughout of fairly intimate familiarity with the Cholupka preprint, which i think should be avoided.  This paper should stand alone.----------------Second, while the technical contributions/novelty are not a focus of the paper's presentation, I am concerned by the lack of methodological advance.  Essentially a regularization objective is added to the previous method, which of itself is not a bad idea, but I can't point to a technical novelty in the paper that the community can not do without.----------------Third, fundamentally i don't see how the experiments address the central question of causality; they show regularization behaving as expected (or rather, influencing weights as expected), but I don't think we really have any meaningful quantitative evidence that causality has been learned.  This was briefly discussed (see ""ground truth causality?"" and the response below).  I appreciate the technical challenges/impossibility of having such a dataset, but if that's the case, then I think this work is premature, since there is no way to really validate.----------------Overall it's clearly a sincere effort, but I found it wanting in terms of a few critical areas.","The reviewers pointed out several issues with the paper, and all recommended rejection. The revision seems to not have been enough to change their minds."
https://openreview.net/forum?id=HkJq1Ocxl,"This paper develops a differentiable interpreter for the Forth programming--------language. This enables writing a program ""sketch"" (a program with parts left--------out), with a hole to be filled in based upon learning from input-output--------examples. The main technical development is to start with an abstract machine--------for the Forth language, and then to make all of the operations differentiable.--------The technique for making operations differentiable is analogous to what is done--------in models like Neural Turing Machine and Stack RNN. Special syntax is developed--------for specifying holes, which gives the pattern about what data should be read--------when filling in the hole, which data should be written, and what the rough--------structure of the model that fills the hole should be. Motivation for why one--------should want to do this is that it enables composing program sketches with other--------differentiable models like standard neural networks, but the experiments focus--------on sorting and addition tasks with relatively small degrees of freedom for how--------to fill in the holes.----------------Experimentally, result show that sorting and addition can be learned given--------strong sketches.----------------The aim of this paper is very ambitious: convert a full programming language to--------be differentiable, and I admire this ambition. The idea is provocative and I--------think will inspire people in the ICLR community.----------------The main weakness is that the experiments are somewhat trivial and there are no--------baselines. I believe that simply enumerating possible values to fill in the--------holes would work better, and if that is possible, then it's not clear to me what--------is practically gained from this formulation. (The authors argue that the point--------is to compose differentiable Forth sketches with neural networks sitting below,--------but if the holes can be filled by brute force, then could the underlying neural--------network not be separately trained to maximize the probability assigned to any--------filling of the hole that produces the correct input-output behavior?)----------------Related, one thing that is missing, in my opinion, is a more nuanced outlook of--------where the authors believe this work is going. Based on the small scale of the--------experiments and from reading other related papers in the area, I sense that it--------is hard to scale up differentiable forth to large real-world problems. It--------would be nice to have more discussion about this, and perhaps even an experiment--------that demonstrates a failure case. Is there a problem that is somewhat more--------complex than the ones that appear in the paper where the approach does not work?--------What has been tried to make it work? What are the failure modes? What are the--------challenges that the authors believe need to be overcome to make this work.----------------Overall, I think this paper deserves consideration for being provocative.--------However, I'm hesitant to strongly recommend acceptance because the experiments--------are weak.","This work is stood out for many reviewers in terms of it's clarity (""pleasure to read"") and originality, with reviewers calling it ""very ambitious"" and ""provocative"". Reviewers find the approach novel, and to fill an interesting niche in the area. All the reviewers were interested in the results, even if they did not buy completely the motivation (what ""practically gained from this formulation"", how does this fit in with prob programming).    The main quality and impact issue is the lack of experimental results and baselines. Several reviewers find that the experiments ""do not fit the claims"", and ask for any type of baselines, even just enumeration. Lacking empirical evidence, there is a desire for a future plan showing what this type of approach could be useful for, even if it cannot really scale. I recommend this paper to be submitted to the workshop track."
https://openreview.net/forum?id=B16Jem9xe,"I just noticed I submitted my review as a pre-review question - sorry about this. Here it is again, with a few more thoughts added...----------------The authors present a great and - as far as I can tell - accurate and honest overview of the emerging theory about GANs from a likelihood ratio estimation/divergence minimisation perspective. It is well written and a good read, and one I would recommend to people who would like to get involved in GANs.----------------My main problem with this submission is that it is hard as a reviewer to pin down what precisely the novelty is - beyond perhaps articulating these views better than other papers have done in the past. A sentence from the paper ""But it has left us unsatisfied since we have not gained the insight needed to choose between them.” summarises my feeling about this paper: this is a nice 'unifying review’ type paper that - for me - lacks a novel insight.----------------In summary, my assessment is mixed: I think this is a great paper, I enjoyed reading it. I was left a bit disappointed by the lack of novel insight, or a singular key new idea which you often expect in conference presentations, and this is why I’m not highly confident about this as a conference submission (and hence my low score) I am open to be convinced either way.----------------Detailed comments:----------------I think the authors should probably discuss the connection of Eq. (13) to KLIEP: Kullback-Leibler Importance Estimation by Shugiyama and colleagues.----------------I don’t quite see how the part with equation (13) and (14) fit into the flow of the paper. By this point the authors have established the view that GANs are about estimating likelihood ratios - and then using these likelihood ratios to improve the generator. These paragraphs read like: we also tried to derive another particular formulation for doing this but we failed to do it in a practical way.----------------There is a typo in spelling Csiszar divergence----------------Equation (15) is known (to me) as Least Squares Importance Estimation by Kanamori et al (2009). A variant of least-squares likelihood estimation uses the kernel trick, and finds a function from an RKHS that best represents the likelihood ratio between the two distributions in a least squares sense. I think it would be interesting to think about how this function is related to the witness function commonly used in MMD and what the properties of this function are compared to the witness function - perhaps showing the two things for simple distributions.----------------I have stumbled upon the work of Sugiyama and collaborators on direct density ratio estimation before, and I found that work very insightful. Generally, while some of this work is cited in this paper, I felt that the authors could do more to highlight the great work of this group, who have made highly significant contributions to density ratio estimation, albeit with a different goal in mind.----------------On likelihood ratio estimation: some methods approximate the likelihood ratio directly (such as least-squares importance estimation), some can be thought of more as approximating the log of this quantity (logistic regression, denoising autoencoders). An unbiased estimate of the ratio will provide a biased estimate of the logarithm and vice versa. To me it feels like estimating the log of the ratio directly is more useful, and in more generality estimating the convex function of the ratio which is used to define the f-divergence seems like a good approach. Could the authors comment on this?----------------I think the hypothesis testing angle is oversold in the paper.  I’m not sure what additional insight is gained by mixing in some hypothesis testing terminology. Other than using quantities that appear in hypothesis testing as tests statistics, his work does not really talk about hypothesis testing, nor does it use any tools from the hypothesis testing literature. In this sense, this paper is in contrast with Sutherland et al (in review for ICLR) who do borrow concepts from two-sample testing to optimise hyperparameters of the divergence used.","Hello Authors,  Congratulations on the acceptance of the paper.  I've just reread parts of the revised paper and noticed a few things that you might want to consider and change before the camera-ready deadline.  * You now include a reference to KLIEP after Eqn. (16), but this procedure is in fact known as least-squares importance estimation. * in turn, Eqn. (14) is actually more akin to KLIEP, the main difference being the use of the unnormalised form of the KL-divergence. So I think you meant to put the KLIEP reference here.  Further comment on making Eqn. (14) practical: If you read the KLIEP paper, they formulate the procedure as a constrained optimisation problem: maximise subject to the constraint that Compare this constrained optimisation to your solution, it is easy to make the connection: if you introduce a Lagrange multiplier to handle the constraint, one obtains the following unconstrained optimisation problem:  seek stationary points of I do think that solving this unconstrained optimisation problem is actually possible, you can do that via stochastic gradient descent, and it does not include your nasty cross-entropy term.  What am I missing?  Thanks,  Rev1"
https://openreview.net/forum?id=r1Aab85gg,"This paper extends preceding works to create a mapping between the word embedding space of two languages. The word embeddings had been independently trained on monolingual data only, and various forms of bilingual information is used to learn the mapping. This mapping is then used to measure the precision of translations.----------------In this paper, the authors propose two changes: ""CCA"" and ""inverted softmax"".  Looking at Table 1, CCA is only better than Dina et al in 1 out of 6 cases (It/En @1).  Most of the improvements are in fact obtained by the introduction of the inverted softmax normalization.----------------Overall, I wonder which aspect of this paper is really new. You mention:-------- - Faruqui & Dyer 2014 already used CCA and dimensionality reduction-------- - Xing et al 2015 argued already that Mikolov's linear matrix should be orthogonal----------------Could you make clear in what aspect your work is different from Faruqui & Dyer 2014 (other the fact that you applied the method to measure translation precision) ?----------------Using cognates instead of a bilingual directory is a nice trick. Please explain how you obtained this list of cognates ? Obviously, this only works for languages with the same alphabet (for instance Greek and Russian are excluded)----------------Also, it seems to me that in linguistics the term ""cognate"" refers to words which have a common etymological origin - they don't necessarily have the same written form (e.g. night, nuit, noche, Nacht). Maybe, you should use a different term ? Those words are probably proper names in news texts.","We have uploaded the final version. The text is unchanged, but we have modified the title to emphasise the aspects of the paper which have been of most interest to readers (particularly the inverted softmax).  We'd like to thank the PC for accepting our manuscript, Sam"
https://openreview.net/forum?id=SkgSXUKxx,"This paper proposes analysis of regularization, weight Froebius-norm and feature L2 norm, showing that it is equivalent to another proposed regularization, gradient magnitude loss. They then argue that: 1) it is helpful to low-shot learning, 2) it is numerically stable, 3) it is a soft version of Batch Normalization. Finally, they demonstrate experimentally that such a regularization improves performance on low-shot tasks.----------------First, this is a nice analysis of some simple models, and proposes interesting insights in some optimization issues. Unfortunately, the authors do not demonstrate, nor argue in a convincing manner, that such an analysis extends to deep non-linear computation structures. I feel like the authors could write a full paper about ""results can be derived for φ(x) with convex differentiable non-linear activation functions such as ReLU"", both via analysis and experimentation to measure numerical stability.----------------Second, the authors again show an interesting correspondance to batch normalization, but IMO fail to experimentally show its relevance.----------------Finally, I understand the appeal of the proposed method from a numerical stability point of view, but am not convinced that it has any effect on low-shot learning in the high dimensional spaces that deep networks are used for.----------------I commend the authors for contributing to the mathematical understanding of our field, but I think they have yet to demonstrate the large scale effectiveness of what they propose. At the same time, I feel like this paper does not have a clear and strong message. It makes various (interesting) claims about a number of things, but they seem more or less disparate, and only loosely related to low-shot learning.----------------notes:--------- ""an expectation taken with respect to the empirical distribution generated by the training set"", generally the training set is viewed as a ""montecarlo"" sample of the underlying, unknown data distribution \mathcal{D}.--------- ""we can see that our model learns meaningful representations"", it gets a 6.5% improvement on the baseline, but there is no analysis of the meaningfulness of the representations.--------- ""Table 13.2"" should be ""Table 2"".--------- please be mindful of formatting, some citations should be parenthesized and there are numerous extraneous and missing spacings between words and sentences.","The paper extends a regularizer on the gradients recently proposed by Hariharan and Girshick. I agree with the reviewers that while the analysis is interesting, it is unclear why this particular regularizer is especially relevant for low-shot learning. And the experimental validation is not strong enough to warrant acceptance."
https://openreview.net/forum?id=SJx7Jrtgl,"The authors proposes to a variant of Variational Auto-Encoders using a mixture distribution to enable unsupervised clustering that they combine with an information-theoretical regularization. To demonstrate the merit of such approach, they perform experiments on a synthetic dataset, MNIST and SVHN.--------The use of a mixture of VAE is an incremental idea if novel.--------I would like to see the comparison with the more straightforward use of a mixture of gaussians prior. This model is more complex and I would like to see a justification of this additional complexity.--------The results in Table 1 are questionable. First of all, GMVAE+ seems to outperform other methods with M=1, there should be a run of GMVAE+ with M=10 for proper comparison. But what I find more disturbing in this table is the variance of the results, especially since you are taking the ""Best Run"". Was the best run maximizing the validation performance or the test performance ? Moreover, the average performance (higher) and standard deviation (lower) of Adversarial Autoencoder makes me question the claim that ""we have advanced the state of the art in deep unsupervised clustering both in theory and practice"".--------The consistency violation regularization might be interesting. But the cluster degeneracy problem is, as far as I know, also a problem in plain mixture of gaussians models. So making an experiment on this simpler model on a synthetic dataset should also be done.--------In general, I would recommend running more experiments as to solidify your claims.","The reviewers have looked through both the responses, updates, and had much discussion. We agree that the paper is well executed and exposes ideas that are of value and interest. At the same same, the extent to which the methods can be applied in practice and scaled to different types of problems remains unclear, especially in high-dimensional settings. For this reason, we feel that the paper is not yet ready for acceptance in this years conference."
https://openreview.net/forum?id=rkjZ2Pcxe,"This paper presents a simple method of adding gradient noise to improve the training of deep neural networks. This paper first appeared on arXiv over a year ago and while there have been many innovations in the area of improving the training of deep neural networks in tha time (batch normalization for RNNs, layer normalization, normalization propagation, etc.) this paper does not mention or compare to these methods. ----------------In particular, the authors state ""However, recent work on applying batch normalization to recurrent networks (Laurent et al., 2015) has not shown promise in improving generalization ability for recur- rent architectures, which are the focus of this work."" This statement is simply incorrect and was thoroughly explored in, e.g. Cooijmans et al. (2016) that establish that batch normalization is effective for RNNs.----------------The proposed method itself is extremely simple and is similar to numerous training strategies that have previously been advocated in the literature. As a result the contribution would be incremental at best and could be significant with sufficiently strong empirical results supporting this particular variant. However, as discussed above there are now multiple training strategies and algorithms in the literature that are not empirically compared.----------------Unfortunately, this paper is now fairly seriously out of date. It would not be appropriate to publish this at ICLR 2017. ","This paper presents a simple heuristic for training deep nets: add noise to the gradients at time t with variance . There is a battery of experimental tests, representing a large amount of computer time. However, these do not compare to any similar ideas, some of which are theoretically motivated. For example the paper identifies SANTA as the closest related work, but there is no comparison.    We encourage the authors to address these outstanding issues and to resubmit."
https://openreview.net/forum?id=BybtVK9lg,"This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called “amortized inference” can be much faster than normal inference where inference must be run iteratively for every document. Some comments:--------Eqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ?--------The generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network?--------The ProdLDA model might well be equivalent to exponential family PCA or some variant thereof: http://jmlr.csail.mit.edu/proceedings/papers/v9/li10b/li10b.pdf--------Section 4.1: error in the equation. The last term should be --------Prod_i exp(delta*_r_i) * exp((1-delta)*s_i).--------Last paragraph 4.1. The increment relative to NVDM seems small: approximating the Dirichlet with a Gaussian and high momentum training. While these aspects may be important in practice they are somewhat incremental.--------I couldn’t find the size of the vocabularies of the datasets in the paper. Does this method work well for very high dimensional sparse document representations?--------The comment on page 8 that the method is very sensitive to optimization tricks like very high momentum in ADAM and batch normalization is a bit worrying to me. --------In the end, it’s a useful paper to read, but it’s not going to be the highlight of the conference. The relative increment is somewhat small and seems to heavily rely optimization tricks.","The reviewers agree that the approach is interesting and the paper presents useful findings. They also raise enough questions and suggestions for improvements that I believe the paper will be much stronger after further revision, though these seem straightforward to address."
https://openreview.net/forum?id=BJC_jUqxe,"This paper proposes a method for representing sentences as a 2d matrix by utilizing a self-attentive mechanism on the hidden states of a bi-directional LSTM encoder. This work differs from prior work mainly in the 2d structure of embedding, which the authors use to produce heat-map visualizations of input sentences and to generate good performance on several downstream tasks.----------------There is a substantial amount of prior work which the authors do not appropriately address, some of which is listed in previous comments. The main novelty of this work is in the 2d structure of embeddings, and as such, I would have liked to see this structure investigated in much more depth. Specifically, a couple important relevant experiments would have been:----------------* How do the performance and visualizations change as the number of attention vectors (r) varies?--------* For a fixed parameter budget, how important is using multiple attention vectors versus, say, using a larger hidden state or embedding size?----------------I would recommend changing some of the presentation in the penalization term section. Specifically, the statement that ""the best way to evaluate the diversity is definitely the Kullback Leibler divergence between any 2 of the summation weight vectors"" runs somewhat counter to the authors' comments about this topic below.----------------In Fig. (2), I did not find the visualizations to provide particularly compelling evidence that the multiple attention vectors were doing much of interest beyond a single attention vector, even with penalization. To me this seems like a necessary component to support the main claims of this paper.----------------Overall, while I found the architecture interesting, I am not convinced that the model's main innovation -- the 2d structure of the embedding matrix -- is actually doing anything important or meaningful beyond what is being accomplished by similar attentive embedding models already present in the literature. Further experiments demonstrating this effect would be necessary for me to give this paper my full endorsement.","A solid paper about sentence representation learning using ""internal attention"" (representing sentences as a self-attention matrix rather than a vector). The idea is novel, well presented, and has potential applications to a variety of sequence-based problems. The main criticism in reviews was about minor points needing clarification, but I am suitably satisfied that these have been acknowledged and incorporated into the current revision. Reviewer 2 is concerned that the experiments do not fully support the claim that this model will improve end task performance over more standard attention mechanisms. Glancing over the paper, and in light of other reviews and discussion, I am not convinced that this objection should halt acceptance, but invite the authors to take this suggestion onboard for future work (or in the final version of their paper). This paper should be accepted to the conference."
https://openreview.net/forum?id=H1MjAnqxg,"Summary:  The authors present a simple RNN with linear dynamics for language modeling. The linear dynamics greatly enhance the interpretability of the model, as well as provide the potential to improve performance by caching the dynamics for common sub-sequences. Overall, the quantitative comparison on a benchmark task is underwhelming. It’s unclear why the authors didn’t consider a more common dataset, and they only considered a single dataset. On the other hand, they present a number of well-executed techniques for analyzing the behavior of the model, many of which would be impossible to do for a non-linear RNN. ----------------Overall, I recommend that the paper is accepted, despite the results. It provides an interesting read and an important contribution to the research dialogue. ----------------Feedback----------------The paper could be improved by shortening the number of analysis experiments and increasing the discussion of related sequence models. Some of the experiments were very compelling, whereas some of them (eg. 4.6) sort of feels like you’re just showing the reader that the model fits the data well, not that the model has any particularly important property. We trust that the model fits the data well, since you get reasonable perplexity results. ----------------LSTMS/GRUs are great for for language modeling for data with rigid combinatorial structure, such as nested parenthesis. It would have been nice if you compared your model to non-linear methods on this sort of data. Don’t be scared of negative results! It would be interesting if the non-linear methods were substantially better on these tasks. ----------------You should definitely add a discussion of Belanger and Kakade 2015 to the related work. They have different motivations (fast, scalable learning algorithms) rather than you (interpretable latent state dynamics and simple credit assignment for future predictions given past). On the other hand, they also have linear dynamics, and look at the singular vectors of the transition matrix to analyze the model. ----------------More broadly, it would be useful for readers if you discussed LDS more directly. A lot of this comparison came up in the openreview discussion, and I recommend folding this into the paper. For example, it would be useful to emphasize that the bias vectors correspond to columns of the Kalman gain matrix. ----------------One last thing regarding LDS: your model corresponds to Kalman filtering but in an LDS you can also do Kalman smoothing, where state vectors are inferred using the future in addition to the past observations. Could you do something similar in your model?----------------What if you said that each matrix is a sparse/convex combination of a set of dictionary matrices? This parameter sharing could provide even more interpretability, since the characters are then represented by the low-dimensional weights used to combine the dictionary elements. This could also provide more scalability to word-level problems. ",All reviewers have carefully looked at the paper and weakly support acceptance of the paper. Program Chairs also looked at this paper and believe that its contribution is too marginal and incremental in its current form. We encourage the authors to resubmit.
https://openreview.net/forum?id=Bk8N0RLxx,"This paper evaluates several strategies to reduce output vocabulary size in order to speed up NMT decoding and training. It could be quite useful to practitioners, although the main contributions of the paper seem somewhat orthogonal to representation learning and neural networks, and I am not sure ICLR is the ideal venue for this work.----------------- Do the reported decoding times take into account the vocabulary reduction step?--------- Aside from machine translation, might there be applications to other settings such as language modeling, where large vocabulary is also a scalability challenge?--------- The proposed methods are helpful because of the difficulties induced by using a word-level model. But (at least in my opinion) starting from a character or even lower-level abstraction seems to be the obvious solution to the huge vocabulary problem.","The reviewers agree that the method is exciting as practical contributions go, but the case for originality is not strong enough."
https://openreview.net/forum?id=BkSmc8qll,"The authors proposed a dynamic neural Turing machine (D-NTM) model that overcomes the rigid location-based memory access used in the original NTM model. The paper has two main contributions: 1) introducing a learnable addressing to NTM. 2) curriculum learning using hybrid discrete and continuous attention. The proposed model was empirically evaluated on Facebook bAbI task and has shown improvement over the original NTM.----------------Pros:--------+ Comprehensive comparisons of feed-forward controllers v.s. recurrent controllers--------+ Encouraging results on the curriculum learning on hybrid discrete and continuous attentions----------------Cons:--------- Very weak NTM baseline (due to some hyper-parameter engineering?) in Table 1, 31% err. comparing to the NTM 20% err. reported in Table 1 in(Graves et al, 2016, Hybrid computing using a neural network with dynamic external memory). In fact, the NTM baseline in (Graves et al 2016) is better than the proposed D-NTM with GRU controller. Maybe it is worthwhile to reproduce their results using the hyper-parameter setting in their Table2 which could potentially lead to better D-NTM performance?--------- Section 3 of the paper is hard to follow. The overall clarity of the paper needs improvement.","This paper proposes some novel architectural elements, and the results are not far from published DNC results. However, the main issues of this paper are the complexity of the model, lack of justification for certain architectural choices, gaps with reported DNC numbers on BABI, and also a somewhat toy-ish task."
https://openreview.net/forum?id=H1Go7Koex,"This paper proposes a new model for sentence classification. ----------------Pros:--------- Some interesting architecture choices in the network.----------------Cons:--------- No evaluation of the architecture choices. An ablation study is critical here to understand what is important and what is not.--------- No evaluation on standard datasets. On the only pre-existing dataset evaluated on a simple TFIDF-SVM method is state-of-the-art, so results are unconvincing.","The paper introduces some interesting architectural ideas for character-aware sequence modelling. However, as pointed out by reviewers and from my own reading of the paper, this paper fails badly on the evaluation front. First, some of the evaluation tasks are poorly defined (e.g. question task). Second, the tasks look fairly simple, whereas there are ""standard"" tasks such as language modelling datasets (one of the reviewers suggests TREC, but other datasets such as NANT, PTB, or even the Billion Word Corpus) which could be used here. Finally, the benchmarks presented against are weak. There are several character-aware language models which obtain robust results on LM data which could readily be adapted to sentence representation learning, eg. Ling et al. 2016, or Chung et al. 2016, which should have been compared against. The authors should look at the evaluations in these papers and consider them for a future version of this paper. As it stands, I cannot recommend acceptance in its current form."
https://openreview.net/forum?id=Hy3_KuYxg,"I find this paper extremely hard to read. The main promise of the paper is to train models for combinatorial search procedures, especially for dynamic programming to learn where to split and merge. The present methodology is supposed to make use of some form of scale invariance property which is scarcely motivated for most problems this approach should be relevant for. However, the general research direction is fruitful and important.----------------The paper would be much more readable if it would start with a clear, formal problem formulation, followed by some schematic view on the overall flow and description on which parts are supervised, which parts are not. Also a tabular form and sample of the various kinds problems solved by this method could be listed in the beginning as a motivation with some clear description on how they fit the central paradigm and motivate the rest of the paper in a more concrete manner.----------------Instead, the paper is quite chaotic, switching between low-level and high level details, problem formulations and their solutions in a somewhat random, hard to parse order.----------------Both split and merge phases seem to make a lot of discrete choices in a hierarchical manner during training. The paper does not explain how those discrete choices are backpropagated through the network in an unbiased manner, if that is the case at all.----------------In general, the direction this paper is exciting, but the paper itself is a frustrating read in its present form. I have spent several hours on it without having to manage to achieve a clear mental image on how all the presented pieces fit together. I would revise my score if the paper would be improved greatly from a readability perspective, but I think it would require a major rewrite.","The area chair agrees with the reviewers that this paper is not ready for ICLR yet. There are significant issues with the writing, making it difficult to follow the technical details. Writing aside, the technique seems somewhat limited in its applicability. The authors also promised an updated version, but this version was never delivered (latest version is from Nov 13)."
https://openreview.net/forum?id=HJOZBvcel,"The paper proposes a novel algorithm to estimate graph structures by using a convolutional neural network to approximate the function that maps from empirical covariance matrix to the sparsity pattern of the graph. Compared with existing approaches, the new algorithm can adapt to different network structures, e.g. small-world networks, better under the same empirical risk minimization framework. Experiments on synthetic and real-world datasets show promising results compared with baselines.----------------In general, I think it is an interesting and novel paper. The idea of framing structure estimation as a learning problem is especially interesting and may inspire further research on related topics. The advantage of such an approach is that it allows easier adaptation to different network structure properties without designing specific regularization terms as in graph lasso.----------------The experiment results are also promising. In both synthetic and real-world datasets, the proposed algorithm outperforms other baselines in the small sample region. ----------------However, the paper can be made clearer in describing the network architectures. For example, in page 5, each o^k_{i,j} is said be a d-dimensional vector. But from the context, it seems o^k_{i,j} is a scalar (from o^0_{i,j} = p_{i,j}). It is not clear what o^k_{i,j} is exactly and what d is. Is it the number of channels for the convolutional filters?----------------Figure 1 is also quite confusing. Why in (b) the table is 16 x 16 whereas in (a) there are only six nodes? And from the figure, it seems there is only one channel in each layer? What do the black squares represent and why are there three blocks of them. There are some descriptions in the text, but it is still not clear what they mean exactly.----------------For real-world data, how are the training data (Y, Sigma) generated? Are they generated in the same way as in the synthetic experiments where the entries are uniformly sparse? This is also related to the more general question of how to sample from the distribution P, in the case of real-world data.","The authors provide a modern twist to the classical problem of graphical model selection. Traditionally, the sparsity priors to encourage selection of specific structures is hand-engineered. Instead, the authors propose using a neural network to train for these priors. Since graphical models are useful in the small-sample regime, using neural networks directly on the training data is not effective. Instead, the authors propose generating data based on the desired graph structures to train the neural network.     While this is a nice idea, the paper is not clear and convincing enough to be accepted to the conference, and instead, recommend it to the workshop track."
https://openreview.net/forum?id=S1HEBe_Jl,"The submission proposes to modify the typical GAN architecture slightly to include ""encrypt"" (Alice) and ""decrypt"" (Bob) modules as well as a module trying to decrypt the signal without a key (Eve).  Through repeated transmission of signals, the adversarial game is intended to converge to a system in which Alice and Bob can communicate securely (or at least a designated part of the signal should be secure), while a sophisticated Eve cannot break their code.  Examples are given on toy data:--------""As a proof-of-concept, we implemented Alice, Bob, and Eve networks that take N-bit random plain-text and key values, and produce N-entry floating-point ciphertexts, for N = 16, 32, and 64.  Both plaintext and key values are uniformly distributed.""----------------The idea considered here is cute.  If some, but not necessarily all of the signal is meant to be secure, the modules can learn to encrypt and decrypt a signal, while an adversary is simultaneously learned that tries to break the encryption.  In this way, some of the data can remain unencrypted, while the portion that is e.g. correlated with the encrypted signal will have to be encrypted in order for Eve to not be able to predict the encrypted part.----------------While this is a nice thought experiment, there are significant barriers to this submission having a practical impact:--------1) GANs, and from the convergence figures also the objective considered here, are quite unstable to optimize.  The only guarantees of privacy are for an Eve that is converged to a very strong adversary (stronger than a dedicated attack over time).  I do not see how one can have any sort of reliable guarantee of the safety of the data transmission from the proposed approach, at least the paper does not outline such a guarantee.--------2) Public key encryption systems are readily available, computationally feasible, and successfully applied almost anywhere.  The toy examples given in the paper do not at all convince me that this is solving a real-world problem at this point.  Perhaps a good example will come up in the near future, and this work will be shown to be justified, but until such an example is shown, the approach is more of an interesting thought experiment.",Interesting paper but not over the accept bar.
https://openreview.net/forum?id=rJxDkvqee,"this proposes a multi-view learning approach for learning representations for acoustic sequences. they investigate the use of bidirectional LSTM with contrastive losses. experiments show improvement over the previous work.----------------although I have no expertise in speech processing, I am in favor of accepting this paper because of following contributions:--------- investigating the use of fairly known architecture on a new domain.--------- providing novel objectives specific to the domain--------- setting up new benchmarks designed for evaluating multi-view models----------------I hope authors open-source their implementation so that people can replicate results, compare their work, and improve on this work.","The paper explores a model that performs joint embedding of acoustic sequences and character sequences. The reviewers agree the paper is well-written, the proposed loss function is interesting, and the experimental evaluation is sufficient. Having said that, there are also concerns that the proxy tasks used in the experiments are somewhat artificial."
https://openreview.net/forum?id=S1_pAu9xl,"The paper shows a different approach to a ternary quantization of weights.--------Strengths:--------1. The paper shows performance improvements over existing solutions--------2. The idea of learning the quantization instead of using pre-defined human-made algorithm is nice and very much in the spirit of modern machine learning.----------------Weaknesses:--------1. The paper is very incremental.--------2. The paper is addressed to a very narrow audience. The paper very clearly assumes that the reader is familiar with the previous work on the ternary quantization. It is ""what is new in the topic"" update, not really a standalone paper. The description of the main algorithm is very concise, to say the least, and is probably clear to those who read some of the previous work on this narrow subject, but is unsuitable for a broader deep learning audience.--------3. There is no convincing motivation for the work. What is presented is an engineering gimmick, that would be cool and valuable if it really is used in production, but is that really needed for anything? Are there any practical applications that require this refinement? I do not find the motivation ""it is related to mobile, therefore it is cool"" sufficient.----------------This paper is a small step further in a niche research, as long as the authors do not provide a sufficient practical motivation for pursuing this particular topic with the next step on a long list of small refinements, I do not think it belongs in ICLR with a broad and diversified audience.----------------Also - the code was not released is my understanding.",The paper describes a method for training neural networks with ternary weights. The results are solid and have a potential for high impact on how networks for high-speed and/or low-power inference are trained.
https://openreview.net/forum?id=ByldLrqlx,"This is a good paper, well written, that presents a simple but effective approach to predict code properties from input output pairs. ----------------The experiments show superiority to the baseline, with speedup factors between one to two orders of magnitude. This is a solid gain!----------------The domain of programs is limited, so there is more work to do in trying such ideas on more difficult tasks. Using neural nets to augment the search is a good starting point and a right approach, instead of generating full complex code.----------------I see this paper as being above the threshold for acceptance.","This is a well written paper that attempts to craft a practical program synthesis approach by training a neural net to predict code attributes and exploit these predicted attributes to efficiently search through DSL constructs (using methods developed in programming languages community). The method is sensible and appears to give consistent speedups over baselines, though its viability for longer programs remains to be seen. There is potential to improve the paper. One of the reviewers would have liked more analysis on what type of programs are difficult and how often the method fails, and how performance depends on training set size etc. The authors should improve the paper based on reviewer comments."
https://openreview.net/forum?id=HkEI22jeg,"This paper fits models to spike trains of retinal ganglion cells that are driven by natural images. I think the title should thus include the word “activity” at the end for otherwise it is actually formally incorrect.----------------Anyhow, this paper proposes more specifically a recurrent network for this time series prediction and compares it to what seems to be the previous approach of a generalized linear model. Overall the stated paradigm is that when one can predict the spikes well then one can look into the model and learn how nature does it. ----------------In general the paper sounds plausible, though I am not convinced that I learned a lot. The results in figure 2 show that the RNN model can predict the spikes a bit better. So this is nice. But now what? You have shown that a more complicated model can produce better fits to the data, though there are of course still some variations to the real data. Your initial outline was that a better predictive model helps you to better understand the neural processing in the retina. So tell us what you learned. I am not a specialist of the retina, but I know that there are several layers and recurrencies in the retina, so I am not so surprised that the new model is better than the GLM. ----------------It seems that more complicated recurrent models such as LSTM do not improve the performance according to a statement in the paper. However, comparisons on this level are also difficult as a more complex models needs more data. Hence, I would actually expect that more layers and even a more detailed model of the retina could eventually improve the prediction even further. --------I was also a bit puzzled that all the neurons in the network share all the same parameters (weights). While the results show that these simplified models can capture a lot of the spike train characteristics, couldn’t a model with free parameters eventually outperform this one (with correspondingly more training data)?","This work is an important step in developing the tools for understanding the nonlinear response properties of visual neurons. The methods are sound and the results are meaningful. Reviewer 3 gave a much lower score than the other two reviewers because Rev 3 does not appreciate the improvement of prediction performance as an advance in itself. For understanding of the visual algorithms in the brain, however, prediction performance is the most critical success criterion. The paper provides convincing evidence that the approach is promising and likely to facilitate further advances towards achieving this long-term goal.    I am confident enough to defend acceptance of this paper for a poster."
https://openreview.net/forum?id=BJjn-Yixl,"This paper presents an attention based recurrent approach to one-shot learning. It reports quite strong experimental results (surpassing human performance/HBPL) on the Omniglot dataset, which is somewhat surprising because it seems to make use of very standard neural network machinery. The authors also note that other have helped verify the results (did Soumith Chintala reproduce the results?) and do provide source code.----------------After reading this paper, I'm left a little perplexed as to where the big performance improvements are coming from as it seems to share a lot of the same components of previous work. If the author's could report result from a broader suite of experiments like in previous work (e.g matching networks), it would much more convincing. An ablation study would also help with understanding why this model does so well.","This paper shows some strong performance numbers, but I agree with the reviewers that it requires more analysis of where those gains come from. The model is very simple, which is a positive, but more studies such as ablation studies and other examples would help a lot."
https://openreview.net/forum?id=B1TTpYKgx,"This paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks.----------------Random networks (deep networks with random Gaussian weights, hard tanh or ReLU activation) are studied according to several criterions: number of neutron transitions, activation patterns, dichotomies and trajectory length.----------------There doesn't seem to be a solid justification for why the newly introduced measures of expressivity really measure expressivity.--------For instance the trajectory length seems a very discutable measure of expressivity. The only justification given for why it should be a good measure of expressivity is proportionality with other measures of expressivity in the specific case of random networks.----------------The paper is too obscure and too long. The work may have some interesting ideas but it does not seem to be properly replaced in context.----------------Some findings seem trivial.----------------detailed comments----------------p2 ----------------""Much of the work examining achievable functions relies on unrealistic architectural assumptions such as layers being exponentially wide""----------------I don’t think so. In ""Deep Belief Networks are Compact Universal Approximators"" by Leroux et al., proof is given that deep but narrow feed-forward neural networks with sigmoidal units can represent any Boolean expression i.e. A neural network with 2n−1 + 1 layers of n units (with n the number of input neutron).----------------“Comparing architectures in such a fashion limits the generality of the conclusions”----------------To my knowledge much of the previous work has focused on mathematical proof, and has led to very general conclusions on the representative power of deep networks (one example being Leroux et al again).----------------It is much harder to generalise the approach you propose, based on random networks which are not used in practice.----------------“[we study] a family of networks arising in practice: the behaviour of networks after random initialisation”----------------These networks arise in practice as an intermediate step that is not used to perform computations; this means that the representative power of such intermediate networks is a priori irrelevant. You would need to justify why it is not.----------------“results on random networks provide natural baselines to compare trained networks with”----------------random networks are not “natural” for the study of expressivity of deep networks. It is not clear how the representative power of random networks (what kind of random networks seems an important question here) is linked to the representative power of (i) of the whole class of networks or (ii) the class of networks after training. Those two classes of networks are the ones we would a priori care about and you would need to justify why the study of random networks helps in understanding either (i) or (ii).----------------p5----------------“As FW is a random neural network […] it would suggest that points far enough away from each other would have independent signs, i.e. a direct proportionality between the length of z(n)(t) and the number of times it crosses the decision boundary.”----------------As you say, it seems that proportionality of the two measures depends on the network being random. This seems to invalidate generalisation to other networks, i.e. if the networks are not random, one would assume that path lengths are not proportional.----------------p6----------------the expressivity w.r.t. remaining depth seems a trivial concerns, completely equivalent to the expressivity w.r.t. depth. This makes the remark in figure 5 that the number of achievable dichotomies only depends *only* on the number of layers above the layer swept seem trivial----------------p7----------------in figure 6 a network width of 100 for MNIST seems much too small. Accordingly performance is very poor and it is difficult to generalise the results to relevant situations.","While the reviewers saw some value in your contribution, there were also serious issues, so the paper does not reach the acceptance threshold."
https://openreview.net/forum?id=BysZhEqee,"This paper proposes to initialize the weights of a deep neural network layer-wise with a marginal Fisher analysis model, making use of potentially the similarity metric.-------- --------Pros: --------There are a lot of experiments, albeit small datasets, that the authors tested their proposed method on.----------------Cons:--------lacking baseline such as discriminatively trained convolutional network on standard dataset such as CIFAR-10.--------It is also unclear how costly in computation to compute the association matrix A in equation 4.----------------This is an OK paper, where a new idea is proposed, and combined with other existing ideas such as greedy-layerwise stacking, dropout, and denoising auto-encoders.--------However, there have been many papers with similar ideas perhaps 3-5 years ago, e.g. SPCANet. ----------------Therefore, the main novelty is the use of marginal Fisher Analysis as a new layer. This would be ok, but the baselines to demonstrate that this approach works better is missing. In particular, I'd like to see a conv net or fully connected net trained from scratch with good initialization would do at these problems.----------------To improve the paper, the authors should try to demonstrate without doubt that initializing layers with MFA is better than just random weight matrices.",The reviewers unanimously recommend rejection.
https://openreview.net/forum?id=BJ9fZNqle,"The authors introduce some new prior and approximate posterior families for variational autoencoders, which are compatible with the reparameterization trick, as well as being capable of expressing multiple modes. They also introduce a gating mechanism between prior and posterior. They show improvements on bag of words document modeling, and dialogue response generation. The original abstract is overly strong in its assertion that a unimodal latent prior p(z) cannot fit a multimodal marginal int_z p(x|z)p(x)dz with a DNN response model p(x|z) (""it cannot possibly capture more complex aspects of the data distribution"", ""critical restriction"", etc).----------------While the assertion that a unimodal latent prior is necessary to model multimodal observations is false, there are sensible motivations for the piecewise constant prior and posterior. For example, if we think of a VAE as a sort of regularized autoencoder where codes are constrained to ""fill up"" parts of the prior latent space, then there is a sphere-packing argument to be made that filling a Gaussian prior with Gaussian posteriors is a bad use of code space. Although the authors don't explore this much, a hypercube-based tiling of latent code space is a sensible idea.----------------As stated, I found the message of the paper to be quite sloppy with respect to the concept of ""multi-modality."" There are 3 types of multimodality at play here: multimodality in the observed marginal distribution p(x), which can be captured by any deep latent Gaussian model, multimodality in the prior p(z), which makes sense in some situations (e.g. a model of MNIST digits could have 10 prior modes corresponding to latent codes for each digit class), and multimodality in the posterior z for a given observation x_i, q(z_i|x_i). The final type of multimodality is harder to argue for, except in so far as it allows the expression of flexibly shaped distributions without highly separated modes. I believe flexible posterior approximations are important to enable fine-grained and efficient tiling of latent space, but I don't think these need to have multiple strong modes. I would be interested to see experiments demonstrating otherwise for real world data.----------------I think this paper should be more clear about the different types of multi-modality and which parts of their analysis demonstrate which ones. I also found it unsatisfactory that the piecewise variable analysis did not show different components of the multi-modal prior corresponding to different words, but rather just a separation between the Gaussian and the piecewise variables.----------------As I mention in my earlier questions, I found it surprising that the learned variance and mean for the Gaussian prior helps so dramatically with G-NVDM likelihood when the powerful networks transforming to and from latent space should make it scale-invariant. Explicitly separating out the contributions of a reimplemented base model, prior-posterior interpolation and the learned prior parameters would strengthen these experiments. Overall, the very strong improvements on the text modeling task over NVDM seem hard to understand, and I would like to see an ablation analysis of all the differences between that model and the proposed one.----------------The fact that adding more constant components helps for document modeling is interesting, and it would be nice to see more qualitative analysis of what the prior modes represent. I also would be surprised if posterior modes were highly separated, and if they were it would be interesting to explore if they corresponded to e.g. ambiguous word-senses.----------------The experiments on dialog modeling are mostly negative results, quantitatively. The observation that the the piecewise constant variables encode time-related words and the Gaussian variables encode sentiment is interesting, especially since it occurs in both sets of experiments. This is actually quite interesting, and I would be interested in seeing analysis of why this is the case. As above, I would like to see an analysis of the sorts of words that are encoded in the different prior modes and whether they correspond to e.g. groups of similar holidays or days.----------------In conclusion, I think the piecewise constant variational family is a good idea, although it is not well-motivated by the paper. The experimental results are very good for document modeling, but without ablation analysis against the baseline it is hard to see why they should be with such a small modification in G-NVDM. The fact that H-NVDM performs better is interesting, though. This paper should better motivate the need for different types of multi-modality, and demonstrate that those sorts of things are actually being captured by the model. As it is, the paper introduces an interesting variational family and shows that it performs better for some tasks, but the motivation and analysis is not clearly focused. To demonstrate that this is a broadly applicable family, it would also be good to do experiments on a more standard datasets like MNIST. Even without an absolute log-likelihood improvement, if the method yielded interpretable multiple modes this would be a valuable contribution.","This paper explores a variational autoencoder variant.    ICLR gives authors some respect that other conferences don't. It is flexible about the length of the paper, and allows revisions to be submitted. The understanding should be that authors should in turn treat reviewers with respect. The paper should still be finished. Reviewers can't be expected to read a churn of large revisions. The final paper should be roughly the right length, unless with very good reason.    This paper was clearly not finished, and now is too long, with issues remaining. I hope that it will be submitted again, but not until it is actually ready."
https://openreview.net/forum?id=rJq_YBqxx,"The paper presents one of the first neural translation systems that operates purely at the character-level, another one being https://arxiv.org/abs/1610.03017 , which can be considered a concurrent work. The system is rather complicated and consists of a lot of recurrent networks.  The quantitative results are quite good and the qualitative results are quite encouraging.----------------First, a few words about the quality of presentation. Despite being an expert in the area, it is hard for me to be sure that I exactly understood what is being done. The Subsections 3.1 and 3.2 sketch two main features of the architecture at a rather high-level. For example, does the RNN sentence encoder receive one vector per word as input or more? Figure 2 suggests that it’s just one. The notation h_t is overloaded, used in both Subsection 3.1 and 3.2 with clearly different meaning. An Appendix that explains unambiguously how the model works would be in order. Also, the approach appears to be limited by its reliance on the availability of blanks between words, a trait which not all languages possess.----------------Second, the results seem to be quite good. However, no significant improvement over bpe2char systems is reported. Also, I would be curious to know how long it takes to train such a model,  because from the description it seems like the model would be very slow to train (400 steps of BiNNN). On a related note, normally an ablation test is a must for such papers, to show that the architectural enhancements applied were actually necessary. I can imagine that this would take a lot of GPU time for such a complex model.----------------On the bright side, Figure 3 presents some really interesting properties that of the embeddings that the model learnt. Likewise interesting is Figure 5.----------------To conclude, I think that this an interesting application paper, but the execution quality could be improved. I am ready to increase my score if an ablation test confirms that the considered encoder is better than a trivial baseline, that e.g. takes the last hidden state for each RNN. ","This paper is concurrently one of the first successful attempts to do machine translation using a character-character MT modeling, and generally the authors liked the approach. However, the reviewers raised several issues with the novelty and experimental setup of the work.     Pros:  - The analysis of the work was strong. This illustrated the underlying property, and all reviewers praised these figures.    Mixed:   - Some found the paper clear, praising it as a ""well-written paper"", however other found that important details were lacking and the notation was improperly overloaded. As the reviewers were generally experts in the area, this should be improved  - Reviewers were also split on results. Some found the results quite ""compelling"" and comprehensive, but others thought there should be more comparison to BPE and other morphogically based work  - Modeling novelty was also questionable. Reviewers like the partial novelty of the character based approach, but felt like the ML contributions were too shallow for ICLR    Cons:  - Reviewers generally found the model itself to be overly complicated and the paper to focus too much on engineering.   - There were questions about experimental setup. In particular, a call for more speed numbers and broader comparison."
https://openreview.net/forum?id=rJiNwv9gg,"The paper proposes a neural approach to learning an image compression-decompression scheme as an auto-encoder. While the idea is certainly interesting and well-motivated, in practice, it turns out to achieve effectively identical rates to JPEG-2000.----------------Now, as the authors argue, there is some value to the fact that this scheme was learned automatically rather than by expert design---which means it has benefits beyond the compression of natural images (e.g., it could be used to automatically learning a compression scheme for signals for which we don't have as much domain knowledge). However, I still believe that this makes the paper unsuitable for publication in its current form because of the following reasons-------------------1. Firstly, the fact that the learned encoder is competitive---and not clearly better---than JPEG 2000 means that the focus of the paper should more be about the aspects in which the encoder is similar to, and the aspects in which it differs, from JPEG 2000. Is it learning similar filters or completely different ones ? For what kinds of textures does it do better and for what kinds does it do worse (the paper could show the best and worst 10 patches at different bit-rates) ?----------------2. Secondly, I think it's crucial that the paper demonstrate that the benefits come from a better coding scheme (as opposed to just a better decoder), as suggested in my initial pre-review question. How would a decoder trained on JPEG-2000 codes (and perhaps also on encoded random projections) do worse or better ?----------------3. Finally, I think the fact that it does as well/worse than JPEG-2000 significantly diminishes the case for using a 'deep' auto-encoder. JPEG-2000 essentially uses a wavelet transform, which is a basis that past studies have shown could be recovered using a simple sparse dictionary algorithm like K-SVD. This is why I feel that the method needs to clearly outperform JPEG-2000, or show comparisons to (or atleast discuss) a well-crafted traditional/generative model-based baseline.","This paper optimizes autoencoders for lossy image compression. Minimal adaptation of the loss makes autoencoders competitive with JPEG2000 and computationally efficient, while the generalizability of trainable autoencoders offers the added promise of adaptation to new domains without domain knowledge.  The paper is very clear and the authors have tried to give additional results to facilitate replication. The results are impressive. While another ICLR submission that is in the same space does outperform JPEG2000, this contributions nevertheless also offers state-of-the-art performance and should be of interest to many ICLR attendees."
https://openreview.net/forum?id=SJIMPr9eg,"The authors mention that they are not aiming to have SOTA results.--------However, that an ensemble of resnets has lower performance than some of single network results, indicates that further experimentation preferably on larger datasets is necessary.--------The literature review could at least mention some existing works such as wide resnets https://arxiv.org/abs/1605.07146 or the ones that use knowledge distillation for ensemble of networks for comparison on cifar.--------While the manuscript is well-written and the idea is novel, it needs to be extended with experiments.",All three reviewers point to significant deficiencies. No response or engagement from the authors (for the reviews). I see no basis for supporting this paper.
https://openreview.net/forum?id=rJRhzzKxl,"This paper studies the problem of transfer learning in the context of domain adaptation. They propose to study it in the framework of knowledge distillation. Several settings are presented along with experiments on the Amazon Reviews dataset.----------------The paper is nicely written and the problem studied is very important towards progress in AI. The results of the experiments could be improved but still justify the validity of applying distillation for transfer learning. Of course, the experimental setting is rather limited but the benchmarks are competitive enough to be meaningful.----------------I had concerns regarding discussion of previous work but the extensive responses helped clarify this point (the authors should turn the arguments used in this thread into an appendix).----------------I think this paper would make an interesting ICLR paper.","The paper introduces a reasonable but somewhat heurstic and straightforward approach to domain adaptation. Especially, since the approach is not so principled, it does not seem sufficient to evaluate it on a single benchmark (document classification, specifically sentiment polarity prediction).     + the results on the sentiment dataset are strong  + the paper is easy to follow    - relatively straightforward  - the novel aspects are a bit heuristic  - extra evaluation is needed"
https://openreview.net/forum?id=SJAr0QFxe,"This paper studies the optimization issue of linear ResNet, and shows mathematically that for 2-shortcuts and zero initialization, the Hessian has condition number independent of depth. I skimmed through the proof but have not checked them carefully. ----------------This result is a nice observation for training deep linear networks.  But I do not think the paper has fully resolved the linear vs nonlinear issue. Some question:----------------1. Though the revision has added some results using ReLU units, it seems it is only added to the mid positions of the network (sec 5.3), is this how it is typically done in ResNet? Moreover, ReLU is not differentiable at zero point, which does not satisfy the condition you had in Theorem 1. Why not use differentiable activations like sigmoid or tanh?----------------2. From equation (22) in the appendix, it seems for nonlinear activations, the condition number depends on the derivative \sigma^\prime at 0. Therefore, if we use tanh which has derivative 1 at zero, the condition number is the same for linear and tanh activations. But this probably is not enough to explain the bit difference in performance or optimization for linear and nonlinear networks, or how the situations evolve after learning the 0 point.----------------3. As for the success of ResNet (or convnets in general) in computer vision, I believe there are more types of nonlinearity such as pooling? Can the result here generalizes to pooling as well?----------------Minor: --------- sec 1 last paragraph, low approximation error typically means more powerful model class and better training error, but not necessarily better test error--------- sec 4.1 what do you mean by ""zero initialization with small random perturbations""? why not exactly zero initialization, how large is the random perturbation?","This paper endeavors to offer theoretical explanations of the performance of ResNet. Providing better theoretical understanding of existing empirically powerful architectures is very important work and I commend the authors for tackling this. Unfortunately, this paper falls short in its current form: the particular choices and restrictions made (0 weights, linear regime) limit applicability to ResNet, and do not seem to offer insights sufficient to capture the causes of ResNet's performance."
https://openreview.net/forum?id=rJeKjwvclx,"Summary: The paper proposes a novel deep neural network architecture for the task of question answering on the SQuAD dataset. The model consists of two main components -- coattention encoder and dynamic pointer decoder. The encoder produces attention over the question as well as over the document in parallel and thus learns co-dependent representations of the question and the document. The decoder predicts the starting and the end token of the answer iteratively, with the motivation that multiple iterations will help the model escape local maxima and thus will reduce the errors made by the model. The proposed model achieved state-of-art result on SQuAD dataset at the time of writing the paper. The paper reports some analyses of the results such as performance across question types, document, question, answer lengths, etc. The paper also performs some ablation studies such as performing only single round of iteration on decoder, etc.----------------Strengths:----------------1. The paper is well-motivated with two main motivations -- co-attending to the document and the question, and iteratively producing the answer.----------------2. The proposed model architecture is novel and the design choices made seem reasonable.----------------3. The experiments show that the proposed model outperforms the existing model (at the time of writing the paper) on the SQuAD dataset by significant margin.----------------4. The analyses of the results and the ablation studies performed (as per someone's request) provide insights into the various modelling design choices made.----------------Weaknesses/Questions/Suggestions:----------------1. In order to gain insights into how much each additional iteration in the decoder help, I would like to see the following -- for every iteration report the mean F1 for questions that converged in that iteration along with the number of questions that converged in that iteration.----------------2. Example of Question 3 in figure 5 is an interesting example where the model is unable to decide between multiple local maxima despite several iterations. Could authors please report how often this happens?----------------3. In order to estimate how much modelling of attention in the encoder helps, it would be good if authors could report the performance of the model when attention is not modeled at all in the encoder (neither over question, nor over document).----------------4. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required.----------------5. In Wang and Jiang (2016), the attention is predicted over question for each word in the document. But in table 2, when performing ablation study to make the proposed model similar to Wang and Jiang, C^D is set to C^Q. But isn’t C^Q attention over document for each word in the question? So, how is this similar to Wang and Jiang’s attention? I think QA^D will be similar to Wang and Jiang's attention since QA^D is attention over question for each word in the document. Please clarify.----------------6. In section 2.1, “n” and “m” are swapped when explaining the Document and Question encoding matrix. Please fix it.----------------Review Summary: The paper presents a novel and interesting model for the task of question answering on SQuAD dataset and shows that the model outperforms existing models. However, to gain more insights into the functioning of the model, I would like see more analyses of the results and one more ablation study (see weaknesses section above).","We sincerely thank all reviewers for your feedback and comments! We have updated our paper, making minor adjustments such as fixing typos. We have also included some additional ablation studies, requested during the review process, in the appendix."
https://openreview.net/forum?id=H1wgawqxl,"This paper provides a principled framework for nonparametrically learning activation--------functions in deep neural networks. A theoretical justification for authors' choice of --------nonparametric activation functions is given. --------Theoretical results are satisfactory but I particularly like the experimental setup--------where their methods are tested on image--------recognition datasets and achieve up to a 15% relative increase in test performance--------compared to the baseline.--------Well-written paper and novel theoretical techniques. --------The intuition behind the proof of Theorem 4.7 can be given in a little bit more clear way in the main body of the paper, but the--------Appendix clarifies everything.","The authors propose a nonparametric regression approach to learn the activation functions in deep neural networks. The proposed theoretical analysis, based on stability arguments, is quite interesting. Experiments on MNIST and CIFAR-10 illustrate the potential of the approach.     Reviewers were somewhat positive, but preliminary empirical evidence on small datasets makes this contribution better suited for the workshop track."
https://openreview.net/forum?id=rywUcQogx,"The authors propose to combine a CCA objective with a downstream loss.  This is a really nice and natural idea.  However, both the execution and presentation leave a lot to be desired in the current version of the paper.----------------It is not clear what the overall objective is.  This was asked in a pre-review question but the answer did not fully clarify it for me.  Is it the sum of the CCA objective and the final (top-layer) objective, including the CCA constraints?  Is there some interpolation of the two objectives?  ----------------By saying that the top-layer objective is ""cosine distance"" or ""squared cosine distance"", do you really mean you are just minimizing this distance between the matched pairs in the two views?  If so, then of course that does not work out of the box without the intervening CCA layer:  You could minimize it by setting all of the projections to a single point.  A better comparison would be against a contrastive loss like the Hermann & Blunsom one mentioned in the reviewer question, which aims to both minimize the distance for matched pairs and separate mismatched ones (where ""mismatched"" ones can be uniformly drawn, or picked in some cleverer way).  But other discriminative top-layer objectives that are tailored to a downstream task could make sense.----------------There is some loose terminology in the paper.  The authors refer to the ""correlation"" and ""cross-correlation"" between two vectors.  ""Correlation"" normally applies to scalars, so you need to define what you mean here.  ""Cross-correlation"" typically refers to time series.  In eq. (2) you are taking the max of a matrix.  Finally I am not too sure in what way this approach is ""fully differentiable"" while regular CCA is not -- perhaps it is worth revisiting this term as well.----------------Also just a small note about the relationship between cosine distance and correlation:  they are related when we view the dimensions of each of the two vectors as samples of a single random variable.  In that case the cosine distance of the (mean-normalized) vectors is the same as the correlation between the two corresponding random variables.  In CCA we are viewing each dimension of the vectors as its own random variable.  So I fear the claim about cosine distance and correlation is a bit of a red herring here.----------------A couple of typos:----------------""prosed"" --> ""proposed""--------""allong"" --> ""along""","The authors propose to use CCA as a transformation within a network that optimally correlates two views. The authors then back-propagate gradients through the CCA. Promising experimental results on for cross-modality retrieval experiments on two public image-to-text datasets are presented.     The main concern with the paper is the clarity of the exposition. The novelty and motivation of the approach remains unclear, despite significant effort from the reviewers to understand.     A major rewriting of the paper will generate a stronger submission to a future venue."
https://openreview.net/forum?id=BJhZeLsxx,"This work proposed a simple but strong baseline for parametric texture synthesis. In empirical experiments, samples generated by the baseline composed by multi-scale and random filters sometime rival the VGG-based model which has multi-layer and pre-trained filters. The authors concluded that texture synthesis does not necessarily depend on deep hierarchical representations or the learned feature maps. --------This work is indeed interesting and insightful. However, the conclusions are needed to be further testified (especially for deep hierarchical representations). Firstly, all of generated samples by both VGG and single layer model are not perfect and much worse than the results from non-parametric methods.  Besides VGG-based model seems to do better in inpainting task in Figure 7. Last but not least, would a hierarchical model (instead of lots of filters with different size) handle multi-scale more efficiently? ","The authors agreed that the paper presented a solid contribution and interesting (and somewhat surprising findings). The experiments are thorough and convincing, and while some reviewers raised concerns about a lack of comparisons of methods on a clear quantifiable objective, this is unfortunately a common issue in this field. Overall, this paper is a solid contribution with findings that would be interesting to the ICLR audience."
https://openreview.net/forum?id=rJqBEPcxe,"Paper Summary--------This paper proposes a variant of dropout, applicable to RNNs, in which the state--------of a unit is randomly retained, as opposed to being set to zero. This provides--------noise which gives the regularization effect, but also prevents loss of--------information over time, in fact making it easier to send gradients back because--------they can flow right through the identity connections without attenuation.--------Experiments show that this model works quite well. It is still worse that--------variational dropout on Penn Tree bank language modeling task, but given the--------simplicity of the idea it is likely to become widely useful.----------------Strengths--------- Simple idea that works well.--------- Detailed experiments help understand the effects of the zoneout probabilities--------  and validate its applicability to different tasks/domains.----------------Weaknesses--------- Does not beat variational dropout (but maybe better hyper-parameter tuning--------  will help).----------------Quality--------The experimental design and writeup is high quality.----------------Clarity--------The paper clear and well written, experimental details seem adequate.----------------Originality--------The proposed idea is novel.----------------Significance--------This paper will be of interest to anyone working with RNNs (which is a large--------group of people!).----------------Minor suggestion---------- As the authors mention - Zoneout has two things working for it - the noise and--------  the ability to pass gradients back without decay. It might help to tease apart--------the contribution from these two factors. For example, if we use a fixed--------mask over the unrolled network (different at each time step) instead of resampling--------it again for every training case, it would tell us how much help comes from the--------identity connections alone.","Very nice paper, with simple, intuitive idea that works quite well, solving the problem of how to do recurrent dropout.    Pros:  - Improved results  - Very simple method    Cons:  - Almost the best results (aside from Variational Dropout)"
https://openreview.net/forum?id=S1oWlN9ll,"Taking into account the loss in the binarization step through a proximal Newton algorithm is a nice idea. This is at least one approach to bringing in the missing loss in the binarization step, which has recently gone from a two step process of train and binarize to a single step simultaneous train/compress. Performance on a few small tasks show the benefit. It would be nice to see some results on substantial networks and tasks which really need compression on embedded systems (a point made in the introduction). Is it necessary to discuss exploding/vanishing gradients when the RNN experiments are carried out by an LSTM, and handled by the cell error carousel? We see the desire to tie into proposition 2, but not clear that the degradation we see in the binary connect is related. Adam is used in the LSTM optimization, was gradient clipping really needed, or is the degradation of binary connect simply related to capacity? For proposition 3.1, theorem 3.1 and proposition 3.2 put the pointers to proofs in appendix.","It's a simple contribution supported by empirical and theoretical analyses. After some discussion, all reviewers viewed the paper favourably."
https://openreview.net/forum?id=Hy8X3aKee,"In absence of authors' responses, the rating is maintained.-----------------------------------This paper introduces an approach for learning predictive time series models that can handle heterogenous multivariate sequence. The first step is in three possible ways to perform embedding of the d-dimensional sequences into d-character words, or a sum of d character embeddings, or a concatenation of d character embeddings. The embedding layer is the first layer of a deep architecture such as LSTM. The models are then trained to perform event prediction at a fixed horizon, with temporal weighting, and applied to hard disk or heating system failures or seizures.----------------The approach is interesting and the results seem to outperform an LSTM baseline, but need additional clarification.----------------The experimental section on seizure prediction is very short and would need to be considerably extended, in an appendix. What are the results obtained using LSTM vs. RNN? What is the state-of-the-art on that dataset? Given that EEG data contain mostly frequential information, how is this properly handled in per-sample embeddings?----------------Please also extend your reference and previous work section to include PixelRNN as well as: --------* van den Oord, et al. (2016)--------WaveNet: A Generative Model for Raw Audio--------arXiv 1609.03499--------* Huang et al. (2013)--------""Learning deep structured semantic models for web search using clickthrough data""--------CIKM--------In the latter paper, the authors embedded 3-gram hashes of the input sequences (e.g., text), which is somewhat similar to a time-delay embedding of the input sequence.","The main strengths and weaknesses pointed out by the reviewers were:    Strengths  -Domain is interesting, problem is important (R2, R1)  -Discretization of continuous domain may enable leveraging of advanced tools developed for discrete domains, e.g. NLP (R1)    Weaknesses  -Issues with experiments: models under-capacity, omission of obvious baselines (R1, R3)  -Unclear conclusion: is quantization and embedding superior to working with the raw data? (R1)  -Fair amount of relevant work omitted (R1, R3)    While the authors engaged in the pre-review discussion, they did not respond to the official reviews. Therefore I have decided to align with the reviewers who are in consensus that the paper does not meet the acceptance bar."
https://openreview.net/forum?id=BymIbLKgl,"Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves. With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants, as evaluated on few toy examples.----------------The paper is generally well written and shows an interesting application of the Siamese architecture. However, the experimental evaluation and the results show that these are rather preliminary results as not many of the choices are validated. My biggest concern is in the choice of the negative samples, as the network basically learns only to distinguish between shapes at different scales, instead of recognizing different shapes. It is well known fact that in order to achieve a good performance with the contrastive loss, one has to be careful about the hard negative sampling, as using too easy negatives may lead to inferior results. Thus, this may be the underlying reason for such choice of the negatives? Unfortunately, this is not discussed in the paper.----------------Furthermore the paper misses a more thorough quantitative evaluation and concentrates more on showing particular examples, instead of measuring more robust statistics over multiple curves (invariance to noise and sampling artifacts).----------------In general, the paper shows interesting first steps in this direction, however it is not clear whether the experimental section is strong and thorough enough for the ICLR conference. Also the novelty of the proposed idea is limited as Siamese networks are used for many years and this work only shows that they can be applied to a different task.","This work proposes learning of local representations of planar curves using convolutional neural networks.  Invariance to rigid transformations and discriminability are enforced with a metric learning framework using a siamese architecture. Preliminary experiments on toy datasets compare favorably with predefined geometric invariants (both differential and integral).     The reviewers found value on the problem set-up and the proposed model, and were generally satisfied with the author's response. They also expressed concern that the experimental section is currently a bit weak and does not include any real data. Also, the paper does not offer theoretical insights that inform us about the design of the representation or about the provable invariance guarantees. All things considered, the AC recommends acceptance in the form of a poster, but strongly encourages the authors to strengthen the work both in the experimental and the theoretical aspects."
https://openreview.net/forum?id=r1S083cgx,"The paper presents a method for sequence generation with a known method applied to feature extracted from another existing method. The paper is heavily oriented towards to chosen technologies and lacks in literature on sequence generation. In principle, rich literature on motion prediction for various applications could be relevant here. Recent models exist for sequence prediction (from primed inputs) for various applications, e.g. for skeleton data. These models learn complex motion w/o any pre-processing. ----------------Evaluation is a big concern. There is no quantitative evaluation. There is no comparision with other methods.----------------I still wonder whether the intermediate representation (developed by Plamondon et al.) is useful in this context of a fully trained sequence generation model and whether the model could pick up the necessary transformations itself. This should be evaluated.----------------Details:----------------There are several typos and word omissions, which can be found by carefully rereading the paper.----------------At the beginning of section 3, it is still unclear what the application is. Prediction of dynamic parameters? What for? Section 3 should give a better motivation of the work.----------------Concerning the following paragraph----------------""While such methods are superior for handwriting analysis and biometric purposes, we opt for a less precise method (Berio & Leymarie, 2015) that is less sensitive to sampling quality and is aimed at generating virtual target sequences that remain perceptually similar to the original trace. --------""--------This method has not been explained. A paper should be self-contained.----------------The authors mentioned that the ""V2V-model is conditioned on (...)""; but not enough details are given. ----------------Generally speaking, more efforts could be made to make the paper more self-contained.","The paper presents a system, namely a recurrent model for handwriting generation. However it doesn't make a clear case for what contribution is being made, or convincing experimental comparisons. The reviews, while short, provide consistent suggestions and directions for how this work could be improved and reworked."
https://openreview.net/forum?id=BJbD_Pqlg,"The author works to compare DNNs to human visual perception, both quantitatively and qualitatively. ----------------Their first result involves performing a psychophysical experiment both on humans and on a model and then comparing the results (actually I think the psychophysical data was collected in a different work, and is just used here).   The specific psychophysical experiment determined, separately for each of a set of approx. 1110 images, what the noise level of additive noise would have to be to make a just-noticeable-difference for humans in discriminating the noiseless image from the noisy one.   The authors then define a metric on neural networks that allows them to measure what they posit might be a similar property for the networks.  They then correlate the pattern of noise levels between neural networks that the humans.    Deep neural networks end up being much better predictors of the human pattern of noise levels than simpler measure of image perturbation (e.g. RMS contrast).  ----------------A second result involves comparing DNNs to humans in terms of their pattern errors in a series of highly controlled experiments using stimuli that illustrate classic properties of human visual processing -- including segmentation, crowding and shape understanding.  They then used an information-theoretic single-neuron metric of discriminability to assess similar patterns of errors for the DNNs.   Again, top layers of DNNs were able to reproduce the human patterns of difficulty across stimuli, at least to some extent. ----------------A third result involves comparing DNNs to humans in terms of their pattern of contrast sensitivity across a series of sine-grating images at different frequencies.  (There is a classic result from vision research as to what this pattern should be, so it makes a natural target for comparison to models.)   The authors define a DNN correlate for the propertie in terms of the cross-neuron average of the L1-distance between responses to a blank image and responses to a sinuisoid of each contrast and frequency.   They then qualitatively compare the results of this metric for DNNs models to known results from the literature on humans, finding that, like humans, there is an apparent bandpass response for low-contrast gratings and a mostly constant response at high contrast.  ----------------Pros:--------    * The general concept of comparing deep nets to psychophysical results in a detailed, quantitative way, is really nice.   ----------------    * They nicely defined a set of ""linking functions"", e.g. metrics that express how a specific behavioral result is to be generated from the neural network.  (Ie. the L1 metrics in results 1 and 3 and the information-theoretic measure in result 2.)   The framework for setting up such linking functions seems like a great direction to me. ----------------    * The actual psychophysical data seems to have been handled in a very careful and thoughtful way.   These folks clearly know what they're doing on the psychophysical end.  ------------------------Cons:--------    * To my mind, the biggest problem wit this paper is that that it doesn't say something that we didn't really know already.   Existing results have shown that DNNs are pretty good models of the human visual system in a whole bunch of ways, and this paper adds some more ways.    What would have been great would be: --------         (a) showing that they metric of comparison to humans that was sufficiently sensitive that it could pull apart various DNN models, making one clearly better than the others. --------         (b) identifying a wide gap between the DNNs and the humans that is still unfilled.   They sort of do this, since while the DNNs are good at reproducing the human judgements in Result 1, they are not perfect -- gap is between 60% explained variance and 84% inter-human consistency.    This 24% gap is potentially important, so I'd really like to see them have explored that gap more -- e.g. (i) widening the gap by identifying which images caused the gap most and focusing a test on those, or (ii) closing the gap by training a neural network to get the pattern 100% correct and seeing if that made better CNNs as measured on other metrics/tasks. ----------------In other words, I would definitely have traded off not having results 2 and 3 for a deeper exploration of result 1.    I think their overall approach could be very fruitful, but it hasn't really been carried far enough here. ----------------   * I found a few things confusing about the layout of the paper.  I especially found that the quantitative results for results 2 and 3 were not clearly displayed.   Why was figure 8 relegated to the appendix?  Where are the quantifications of model-human similarities for the data shown in Figure 8?  Isn't this the whole meat of their second result?   This should really be presented in a more clear way.    ----------------    * Where is the quantification of model-human similarity for the data show in Figure 3?  Isn't there a way to get the human contrast-sensitivity curve and then compare it to that of models in a more quantitively precise way, rather than just note a qualitative agreement?   It seems odd to me that this wasn't done. ","I think the reviewers evaluated this paper very carefully and were well balanced. The reviewers all agree that the presented comparison between human vision and DNNs is interesting. At the same time, none of the reviewers would strongly defend the paper. As it stands, this work seems a little too premature for publication as the analysis does not go too much beyond what we already know. We encourage the authors to deepen the investigation and resubmit."
https://openreview.net/forum?id=BJluGHcee,"The paper provides an interesting use of generative models to address the classification with missing data problem. The tensorial mixture models proposed take into account the general problem of dependent samples. This is an nice extension of current mixture models where samples are usually considered as independent. Indeed the TMM model is reduced to the conventional latent variable models. As much as I love the ideas behind the paper, I feel pitiful about the sloppiness of the presentation (such as missing notations) and flaws in the technical derivations.  Before going into the technical details, my high level concerns are as follows:--------(1) The joint density over all samples is modeled as a tensorial mixture generative model. The interpretation of the CP decomposition or HT decomposition on the prior density tensor is not clear. The authors have an interpretation of TMM as product of mixture models when samples are independent, however their interpretation seems flawed to me, and I will elaborate on this in the detailed technical comments below. --------(2) The authors employ convolution operators to compute an inner product. It is realizable by zero padding, but the invariance structure, which is the advantage of CNN compared to feed-forward neural network, will be lost. However, I am not sure how much this would affect the performance in practice. --------(3) The author could comment in the paper a little bit on the sample complexity of this method given the complexity of the model. ----------------Because I liked the ideas of the paper so much, and the ICLR paper submitted didn't present the technical details well due to sloppiness of notations, so I read the technical details in the arXiv version the authors pointed out. There are a few technical typos that I would like to point out (my reference to equations are to the ones in the arXiv paper). --------(1) The generative model as in figure (5) is flawed. P(x_i|d_i;\theta_{d_i}) are vectors of length s, there the product of vectors is not well defined. It is obvious that the dimensions of the terms between two sides of the equation are not equal. In fact, this should be a tucker decomposition instead of multiplication. It should be P(X) = \sum_{d1,\ldots,d_N} P(d_1,\ldots,d_N) (P(x_1|d_1;theta_{d_1},P(x_2|d_2;theta_{d_2},\ldots,P(x_N|d_N;theta_{d_N}), which means a sum of multi-linear operation on tensor P(d_1,\ldots,d_N), and each mode is projected onto P(x_i|d_i;theta_{d_i}. --------(2) I suspect the special case for diagonal Gaussian Mixture Models has some typos as I couldn't derive the third last equation on page 6. But it might be just I didn't understand this example. --------(3) The claim that TMM reduces to product of mixture model is not accurate. The first equation on page 7 is only right when ""sum of product"" operation is equal to ""product of sum"" operation. Similarly, in equation (6), the second equality doesn't hold unless in some special cases. However, this is not true. This might be just a typo, but it is good if the authors could fix this. I also suspect that if the authors correct this typo,the performance on MNIST might be improved.----------------Overall, I like the ideas behind this paper very much. I suggest the authors fix the technical typos if the paper is accepted. ","The authors have recently made several connections between deep learning and tensor algebra. While their earlier works dealt with supervised learning, the current work analyzes generative models through the lens of tensor algebra.   The authors show propose a tensorial mixture model over local structures where the mixture components are expressed as tensor decompositions. They show that hierarchical tensor decomposition is exponentially more expressive compared to the shallow models.   The paper makes original contributions in terms of establishing expressivity of deep generative models. The connections with tensor algebra could lead to further innovations, e.g. in training algorithms.  However, the paper can be improved in two aspects:   (1) It will be nice if the authors make a connection between the algebraic view presented here with the geometric view presented by: https://arxiv.org/abs/1606.05340  (2) It is also desirable if more extensive experiments are performed.    Given the improvements outlined for the paper, it does not meet the bar for acceptance at ICLR. "
https://openreview.net/forum?id=ryAe2WBee,"The paper proposes a semantic embedding based approach to multilabel classification. --------Conversely to previous proposals, SEM considers the underlying parameters determining the--------observed labels are low-rank rather than that the observed label matrix is itself low-rank. --------However, It is not clear to what extent the difference between the two assumptions is significant----------------SEM models the labels for an instance as draws from a multinomial distribution--------parametrized by nonlinear functions of the instance features. As such, it is a neural network.--------The proposed training algorithm is slightly more complicated than vanilla backprop.  The significance of the results compared to NNML (in particular on large datasets Delicious and EUrlex) is not very clear. ----------------The paper is well written and the main idea is clearly presented. However, the experimental results are not significant enough to compensate the lack of conceptual novelty. ","This is largely a well written paper proposing a sensible approach for multilabel learning that is shown to be effective in practice. However, the main technical elements of this work: the model used and its connections to basic MLPs and related methods in the literature, the optimization strategy, and the speedup tricks are all familiar from prior work. Hence the reviewers are somewhat unanimous in their view that the novelty aspect of this paper is its main shortcomings. The authors are encouraged to revise the paper and clarify the precise contributions."
https://openreview.net/forum?id=HyM25Mqel,"The paper looks at several innovations for deep RL, and evaluates their effect on solving games in the Atari domain.  The paper reads a bit like a laundry list of the researcher’s latest tricks.  It is written clearly enough, but lacks a compelling message.  I expect the work will be interesting to people already implementing deep RL methods, but will probably not get much attention from the broader community.----------------The claims on p.1 suggest the approach is stable and sample efficience, and so I expected to see some theoretical analysis with respect to these properties. But this is an empirical claim; it would help to clarify that in the abstract.----------------The proposed innovations are based on sound methods.  It is particularly nice to see the same approach working for both discrete and continuous domains.----------------The paper has reasonably complete empirical results. It would be nice to see confidence intervals on more of the plots. Also, the results don’t really tease apart the effect of each of the various innovations, so it’s harder to understand the impact of each piece and to really get intuition, for example about why ACER outperforms A3C.  Also, it wasn’t clear to me why you only get matching results on discrete tasks, but get state-of-the-art on continuous tasks.----------------The paper has good coverage of the related literature. It is nice to see this work draw more attention to Retrace, including the theoretical characterization in Sec.7.","First of all, thanks for your kind words.  As to the Equation 4, I think \psi(s_t, a_t) = \grad_\theta(\pi (a_t | s_t) ) /  \pi (a_t | s_t) instead of \psi(s_t, a_t) = \grad_\theta( log \pi (a_t | s_t) ) /  \pi (a_t | s_t). (The log term is missing.)   Therefore, we have \grad_\theta( log  \pi (a_t | s_t) ) =\grad_\theta(\pi (a_t | s_t) )/ \pi (a_t | s_t).  Hopefully that answers your question. :)"
https://openreview.net/forum?id=SygGlIBcel,"This paper proposes an extension of neural network language (NLM) models to better handle large vocabularies. The main idea is to obtain word embeddings by combining character-level embeddings with a convolutional network.----------------The authors compare word embeddings (WE),character embeddings (CE) as well a combined character and word embeddings (CWE). It's quite obvious how CE or CWE embeddings can be used at the input of an NLM, but this is more tricky at the output layer. The authors propose to use NCE to handle this problem.  NCE allows to speed-up training, but has no impact on inference during testing: the full softmax output layer must be calculated and normalized (which can be very costly).----------------It was not clear to me how the network is used during TESTING with an open-vocabulary. Since the NLM is only used during reranking, the unnormalized probability of the requested word could be obtained at the output. However, when reranking n-best lists with the NLM feature, different sentences are compared and I wonder whether this does work well without proper normalization.----------------In addition, the authors provide perplexities in Table 2 and Figures 2 and 3.  This needs normalization, but it is not clear to me how this was performed.  The authors mention a 250k output vocabulary. I doubt that the softmax was calculated over 250k values. Please explain.----------------The model is evaluated by reranking n-best lists of an SMT systems for the IWSLT 2016 EN/CZ task.  In the abstract, the authors mention a gain of 0.7 BLEU. I do not agree with this claim. A vanilla word-based NLM, i.e. a well-known model, achieves already a gain of 0.6 BLEU. Therefore, the new model proposed in this paper brings only an additional improvement of 0.1 BLEU. This is not statistically significant. I conjecture that a similar variation could be obtained by just training several models with different initializations, etc.----------------Unfortunately, the NLM models which use a character representation at the output do not work well. There are already several works which use some form of character-level representations at the input.----------------Could you please discuss the computational complexity during training and inference.----------------Minor comments-------- - Figure 2 and 3 have the caption ""Figure 4"". This is misleading.-------- - the format of the citations is unusual, eg.--------   ""While the use of subword units Botha & Blunsom (2014)""--------   -> ""While the use of subword units (Botha & Blunsom, 2014)""",The reviewers raise several important questions about modeling and methodology that should be answered in later versions of the paper. The paper also overstates its findings.
https://openreview.net/forum?id=Byiy-Pqlx,"The paper introduces a novel memory mechanism for NTMs based on differentiable Lie groups. --------This allows to place memory elements as points on a manifold, while still allowing training with backpropagation.--------It's a more general version of the NTM memory, and possibly allows for training a more efficient addressing schemes.----------------Pros:--------- novel and interesting idea for memory access--------- nicely written-------- --------Cons:--------- need to manually specify the Lie group to use (it would be better if network could learn the best way of accessing memory)                                 --------- not clear if this really works better than standard NTM (compared only to simplified version)--------- not clear if this is useful in practice (no comparison on real tasks)","The paper presents a Lie-(group) access neural turing machine (LANTM) architecture, and demonstrates it's utility on several problems.     Pros:  Reviewers agree that this is an interesting and clearly-presented idea.  Overall, the paper is clearly written and presents original ideas.  It is likely to inspire further work into more effective generalizations of NTMs.    Cons:  The true impact and capabilities of these architectures are not yet clear, although it is argued that the same can be said for NTMs.     The paper has been revised to address some NTM features (sharpening) that were not included in the original version.  The purpose and precise definition of the invNorm have also been fixed."
https://openreview.net/forum?id=ry3iBFqgl,"Summary: The paper proposes a novel machine comprehension dataset called NEWSQA. The dataset consists of over 100,000 question answer pairs based on over 10,000 news articles from CNN. The paper analyzes the different types of answers and the different types of reasoning required to answer questions in the dataset. The paper evaluates human performance and the performance of two baselines on the dataset and compares them with the performance on SQuAD dataset. ----------------Strengths:----------------1. The paper presents a large scale dataset for machine comprehension. ----------------2. The question collection method seems reasonable to collect exploratory questions. Having an answer validation step is desirable.----------------3. The paper proposes a novel (computationally more efficient) implementation of the match-LSTM model.----------------Weaknesses:----------------1. The human evaluation presented in the paper is not satisfactory because the human performance is reported on a very small subset (200 questions). So, it seems unlikely that these 200 questions will provide a reliable measure of the human performance on the entire dataset (which consists of thousands of questions).----------------2. NEWSQA dataset is very similar to SQuAD dataset in terms of the size of the dataset, the type of dataset -- natural language questions posed by crowdworkers, answers comprising of spans of text from related paragraphs. The paper presents two empirical ways to show that NEWSQA is more challenging than SQuAD -- 1) the gap between human and machine performance in NEWSQA is larger than that in SQuAD. However, since the human performance numbers are reported on very small subset, these trends might not carry over when human performance is computed on all of the dataset.--------2) the sentence-level accuracy on SQuAD is higher than that in NEWSQA. However, as the paper mentions, the differences in accuracies could likely be due to different lengths of documents in the two datasets. So, even this measure does not truly reflect that SQuAD is less challenging than NEWSQA.--------So, it is not clear if NEWSQA is truly more challenging than SQuAD.----------------3. Authors mention that BARB is computationally more efficient and faster compared to match-LSTM. However, the paper does not report how much faster BARB is compared to match-LSTM.----------------4. On page 7, under ""Boundary pointing"" paragraph, the paper should clarify what ""s"" in ""n_s"" refers to.----------------Review summary: While the dataset collection method seems interesting and promising, I would be more convinced after I see the following ----------1. Human performance on all (or significant percentage of the dataset).--------2. An empirical study that fairly shows that NEWSQA is more challenging (or better in some other way) than SQuAD.","The program committee appreciates the authors' response to concerns raised in the reviews. All reviewers agree that the paper is not convincingly above the acceptance threshold. The paper will be stronger, and the benefit of this dataset over SQuAD will likely be more clear once authors incorporate reviewers' comments and finish evaluation of inter-human agreement."
https://openreview.net/forum?id=r1y1aawlg,"This paper proposes a model for iteratively refining translation hypotheses. This has several benefits, including enabling the translation model to condition not only on “left context”, but also on “right context”, and potentially enabling more rapid and/or accurate decoding. The motivation given is that often translators (and text generators generally) use a process of refinement in generating outputs.----------------This is an important idea that is not currently playing much of a role in neural net models, so this paper is a welcome contribution. However, while I think this is an important first step, I do feel that the lack of in depth analysis suggests this paper is not quite ready for a final publication version. For example, there are many possible connections to prior work in NLP, MT, and other parts of ML that could better contextualize this work (see specifics below). More substantively, the model in Section 3 could be interpreted as a globally normalized, undirected (~CRF) translation model trained using a pseudo-likelihood objective. In this analysis, the model squarely back in the context of traditional discriminative translation models which used “undirected” features, and the decoding algorithm then looks more like a standard greedy hill-climbing algorithm (albeit with an extra heuristic model for selecting which variable to update), which is also nothing unfamiliar.----------------My second criticism the limitations of the model are not well discussed. For example, the proposed editing procedure cannot obviously remove or insert a word from a translation. While I think this is a reasonable assumption than can be made for the sake of tractability, it is very unfortunate since missing or extra words (esp. function words) are a common problem in the baseline models that are being used. Second, the standard objections to absolute positional models (vs. relative positional models) seem particularly crucial to bring up in this work, especially since they might make some of the design decisions a bit more justifiable.----------------Overall, this is an initial step in an interesting direction, but it needs more thorough analysis to demonstrate its value. A more thorough analysis will also likely suggest some important model variants (for example: is a global translation model really the goal? or is a post-editing model that fixes outputs with more complex operations more ideal?)----------------Related work:--------I think that more could be done to put this work in the context of what has come before and what is currently going on in other parts of ML. The idea of iterative refinement has been proposed in other problems that have complex output spaces, for example the DRAW model of Gregor et al. and the conditional adversarial network models used to refine images proposed recently by Isola et al. In NLP, there have been several (stochastic) hill climbing approaches that have been proposed, such as the work on parsing by Zhang and Lei et al. (2014) who use random initial guesses and then do greedy hill climbing using a series of local refinements, the structured prediction cascades of Weiss and Taskar (2009) (not to mention general coarse-to-fine modeling strategies). Finally, in MT, Arun et al. (2009) who use a Gibbs sampler to refine an initial guess to do decoding with a more complex model. The use of an explicit error model is rather novel in the context of correction, but I would point out that although the proposed architecture is different, the discriminative word lexicon models of Mauser et al. (2009) and the neural version of the same by Ha et al. (2014) are similar in spirit. There have also been a number of papers on “automatic post editing”, including the shared task at WMT2016, and there are not only standard test sets and baselines, but also datasets that could actually be used to train a post editing model with human-generated data. Minimally using the techniques they described could be a useful foil for the models presented in this paper.----------------“the target sentence is also embedded in distributional space via a lookup table” I think “distributional space” is a bit unclear. Maybe “the target sentence is represented in terms of distributed word representations via a lookup table” or something like that. “distributional” suggests that the representations are derived from how the words are distributed in the corpus, whereas you are learning these representations on this task which isn’t modeling their distribution except only very indirectly.----------------Section 3 Model: In Section 3, the model computes the distribution over target word types at an absolute position i in the output sentence, given the target language context and the source language context. It is introduced as the model that is used to refine an existing hypothesis, but it is not immediately clear that the training data for this model (at least in this section) are the gold standard translations- “training set” could be interpreted in variety of ways. This becomes clearer when reading later in the paper, but it’s a bit less clear when reading from the beginning for the first time.----------------The use of a fixed sized window for representing the target word in context also seems to make something like a model 1 assumption since only the lexical features (and not any “alignment” or “positional” features) determine the attention. This should be clarified since it will make the assumptions of the model more transparent (and also suggest possible refinements to the model, e.g., including (representations) of i and j as components of S^j and T^i, which would allow model 2/3-like responses to be learned- although by leaving them out, the model might behave a bit more like a relative positional model than an absolute positional model, which is probably attractive).----------------Finally, some discussion for why a fixed window is used to represent the target sentence is worth including (since a global context is apparently used to represent the source sentence).----------------The relationship between this training objective and pseudo likelihood (PL; Besag, 1975) might be worth mentioning. Since I believe this is just a PL objective for a certain global model, this suggests alternative decoding algorithms, or certainly a different analysis of the proposed decoding objective.----------------The section 4 model conditions on the true context of a position in the true target, the current target guess, and the source. I don’t completely understand the rationale for this model since at test time only two of these variables are available, and the replacement of y_ref with y_g seems hard to justify.","The paper is an interesting first step, but the reviewers found flaws in the overall argument. Further, the method is not contextualized well enough in relation to prior work."
https://openreview.net/forum?id=r1kGbydxg,"The paper is straightforward, easy to read, and has clear results. ----------------Since all these parameterisations end up outputting torques, it seems like there shouldn't be much difference between them. There is a known function that convert from one representation to another (or at least to torques). Is it not possible that the only reason proportional control is a little better is that the tracking cost is a function of positions?----------------Would we get the same result if there was no reference-pose cost, only a locomotion cost?----------------Would we get the same result if the task was to spin a top? My guess is no. ----------------This work is interesting, but not likely to generalise to other scenarios, and in that sense is rather limited.----------------The video is nice.","After reading the paper and the reviews, I believe that the paper presents a solid contribution and a detailed empirical exploration of the choice of action space representations for continuous control trajectory tracking tasks, but has limited relevance to the ICLR audience in its present form.    The conclusion that PD controllers with learned gains provide improved learning speed and sometimes better final results than the ""default"" joint torque representation is intriguing and has some implications for future work on trajectory tracking for simulated articulated rigid body characters, but it's unclear from the current results what conclusion there is to take away for general algorithm or model design. Since the only evaluation is on trajectory tracking, it's also not even clear (as pointed out by other reviewers) to what degree these results will generalize to other tasks. In fact, a contrarian view might be that PD tracking is specifically a good fit for trajectory tracking with a known reference trajectory, but might be a poor fit for accomplishing a particular task, where more open-loop behaviors might be more optimal. The passive compliance of MTUs is also shown to often not be beneficial, but it's again not clear whether this might in fact be an artifact of the trajectory tracking task.    But perhaps more problematically, the primary conclusions of the paper are of limited relevance when it comes to design of algorithms or models (or at least, the authors don't discuss this), but are more relevant for practitioners interested in applying deep RL for learning controllers for articulated rigid body systems such as robots or virtual characters. As such, it's not clear what fraction of the ICLR audience will find the paper relevant to their interests.    I would strongly encourage the authors to continue their research: there is a lot to like about the present paper, including an elegant reinforcement learning methodology, rigorous empirical results for the specific case of trajectory following, and some very nice videos and examples."
https://openreview.net/forum?id=By1snw5gl,"It is an interesting idea to go after saddle points in the optimization with an SR1 update and a good start in experiments, but missing important comparisons to recent 2nd order optimizations such as Adam, other Hessian free methods (Martens 2012), Pearlmutter fast exact multiplication by the Hessian. From the mnist/cifar curves it is not really showing an advantage to AdaDelta/Nag (although this is stated), and much more experimentation is needed to make a claim about mini-batch insensitivity to performance, can you show error rates on a larger scale task?","The paper proposes an interesting approach, in that (unlike many second-order methods) SR1 updates can potentially take advantage of negative curvature in the Hessian. However, all reviewers had some significant concerns about the utility of the method. In particular, reviewers were concerned that the method does not show a significant gain over the Adam algorithm (which is simpler/cheaper and easier to implement). The public reviewer also points out that there are many existing quasi-Newton methods designed for DL, so it is up to the authors to compared to at least one of these. For these reasons I'm recommending rejection at this time."
https://openreview.net/forum?id=H178hw9ex,"This works applies steerable frames for various tasks where convolutional neural networks with location invariant operators are traditionally applied. Authors provide a detailed overview of steerable frames followed with an experimental section which applies dynamic steerable network to small machine learning problems where the steerability is conceptually useful.----------------Even though the evaluation is performed only on few small tasks, the reason why more tasks were not evaluated is that piece-wise pose invariance is needed only for a subset of tasks. The fact, that simply using overcomplete bases as a sort of ""feature pre-processing"" improves the results for already highly optimized ResNet and DenseNet architectures is quite interesting achievement.----------------For the edge detection, a relatively hard baseline is selected - the Dynamic Filter Networks, which already attempts to achieve position invariant filters. The fact that DSFN improves the performance on this task verifies that regressing the parametrization of the steerable filters yields better results than regressing the filters directly.----------------In the last experiment authors apply the network to video classification using LSTMs and they show that the improved performance is not due to increased capacity of the network.----------------In general, it is quite interesting work. Even though it does not offer ground-breaking results (mainly in a sense of not performing experiments on larger tasks), it is theoretically interesting and shows promising results.----------------There are few minor issues and suggestions related to the paper:--------* For the LSTM experiment, in order to be more exact, it would be useful to include information about total number of parameters, as the network which estimates the pose also increases the number of parameters.--------* Would it be possible to provide more details about how the back-propagation is done through the steerable filters?--------* For the Edge Detection experiment, it would be useful to provide results for some standard baseline - e.g. CNN with a similar number of parameters. Simply to see how useful it is to have location-variant filters for this task.--------* The last sentence in second paragraph on page 1 is missing a verb. Also it is maybe unnecessary.--------* The hyphenation for ConvNet is incorrect on multiple places (probably `\hyphenation{Conv-Net}` would fix it).","This paper studies how to incorporate local invariance to geometric transformations into a CNN pipeline. It proposes steerable filter banks as the ground-bed to measure and produce such local invariance, building on previous work from the same authors as well as the Spatial Transformer Networks. Preliminary experiments on several tasks requiring different levels of local invariance are presented.     The reviewers had varying opinions about this work; all acknowledged the potential benefits of the approach, while some of them raised questions about the significance and usefulness of the approach. The authors were very responsive during the rebuttal phase and took into account all the feedback.     Based on the technical content of the paper and the reviewers opinion, the AC recommends rejection. Since this decision is not consensual among all reviewers, please let me explain it in more detail.    - The current manuscript does not provide a clear description of the new model in the context of the related works it builds upon (the so-called dynamic filter networks and the spatial transformer networks). The paper spends almost 4 pages with a technical exposition on steerable frames, covering basic material from signal processing. While this might indeed be a good introduction to readers not familiar with the concept of steerable filters, the fact is that it obfuscates the real contributions of the paper. which are not clearly stated. In fact, the model is presented between equations (10) and (11), but it is not clear from these equations what specifically differentiates the dsfn from the other two models -- the reader has to do some digging in order to uncover the differences (which are important).     Besides this clarity issue, the paper does not offer any insight as to how the 'Pose generating network' Psi is supposed to estimate the pose parameters. Which architecture? what is the underlying estimation problem it is trying to solve, and why do we expect this problem to be efficiently estimated with a neural network? when are the pose parameters uniquely determined? how does this network deal with the aperture effects (i.e. the situations where there is no unicity in determining a specific pose) ?  Currently, the reader has no access to these questions, which are to some extent at the core of the proposed technique."
https://openreview.net/forum?id=HyxQzBceg,"Update: raised the score, because I think the arguments about adversarial examples are compelling.  I think that the paper convincingly proves that this method acts as a decent regularizer, but I'm not convinced that it's a competitive regularizer.  For example, I don't believe that there is sufficient evidence that it gives a better regularizer than dropout/normalization/etc.  I also think that it will be much harder to tune than these other methods (discussed in my rebuttal reply).  ------------------------------------Summary: If I understand correctly, this paper proposes to take the ""bottleneck"" term from variational autoencoders which pulls the latent variable towards a noise prior (like N(0,1)) and apply it in a supervised learning context where the reconstruction term log(p(x|z)) is replaced with the usual supervised cross-entropy objective.  ----------------The argument is that this is an effective regularizer and increases robustness to adversarial attacks.  ----------------Pros: -----------------The presentation is quite good and the paper is easy to follow.  -----------------The idea is reasonable and the relationship to previous work is well described.  -----------------The robustness to adversarial examples experiment seems convincing, though I'm not an expert in this area.  Is there any way to compare to an external quantitative baseline on robustness to adversarial examples?  This would help a lot, since I'm not sure how the method here compares with other regularizers in terms of combatting adversarial examples.  For example, if one uses a very high dropout rate, does this confer a comparable robustness to adversarial examples (perhaps at the expense of accuracy)?  ----------------Cons: -----------------MNIST accuracy results don't seem very strong, unless I'm missing something.  The Maxout paper from ICML 2013 listed many permutation invariant MNIST results with error rates below 1%.  So the 1.13% error rate listed here doesn't necessarily prove that the method is a competitive regularizer.  I also suspect that tuning this method to make it work well is harder than other regularizers like dropout.  -----------------There are many distinct architectural choices with this method, particularly in how many hidden layers come before and after z.  For example, the output could directly follow z, or there could be several layers between z and the output.  As far as I can tell the paper says that p(y | z) is a simple logistic regression (i.e. one weight matrix followed by softmax), but it's not obvious why this choice was made.  Did it work best empirically?  ----------------Other: -----------------I wonder what would happen if you ""trained against"" the discovered adversarial examples while also using the method from this paper.  Would it learn to have a higher variance p(z | x) when presented with an adversarial example?  ",This paper discussses applying an information bottleneck to deep networks using a variational lower bound and reparameterization trick. The paper is well written and the examples are compelling. The paper can be improved with more convincing results on MNIST.
https://openreview.net/forum?id=Sy4tzwqxe,"The authors propose two variational methods based on the theme of posterior approximations which may not have a tractable density. The first is from another ICLR submission on ""amortized SVGD"" (Wang and Liu, 2016), where here the innovation is in using SGLD as the inference network. The second is from a NIPS paper (Ranganath et al., 2016) on minimizing the Stein divergence with a parametric approximating family, where here the innovation is in defining their test functions to be an RKHS, obtaining an analytic solution to the inner optimization problem.----------------The methodology is incremental. Everything up to Section 3.2 is essentially motivation, background, or related work. The notion of a ""wild variational approximation"" was already defined in Ranganath et al. (2016), termed a ""variational program"". It would be useful for the authors to comment on the difference, if any.----------------Section 3.2 is at first interesting because it analytically solves the maximum problem that is faced in Ranganath et al. (2016). However, this requires use of a kernel which will certainly not scale in high dimensions, so it is then equivalent in practice to having chosen a very simple test function family. To properly scale to high dimensions would require a deeper kernel and also learning its parameters; this is not any easier than parameterizing the test function family as a neural network to begin with, which Ranganath et al. (2016) do.----------------Section 4 introduces a Langevin inference network, which essentially chooses the variational approximation as an evolving sequence of Markov transition operators as in Salimans et al. (2015). I had trouble understanding this for a while because I could not understand what they mean by inference network. None of it is amortized in the usual inference network sense, which is that the parameters are given by the output of a neural network. Here, the authors simple define global parameters of the SGLD chain which are used across all the latent variables (which is strictly worse?). (What then makes it an ""inference network""?) Is this not the variational approximation used in Salimans et al. (2015), but using a different objective to train it?----------------The experiments are limited, on a toy mixture of Gaussians posterior and Bayesian logistic regression. None of this addresses the problems one might suspect on high-dimensional and real data, such as the lack of scalability for the kernel, the comparison to Salimans et al. (2015) for the Langevin variational approximation, and any note of runtime or difficulty of training.----------------Minor comments----------------+ It's not clear if the authors understood previous work on expressive variational families or inference networks. For example, they argue Rezende & Mohamed, 2015b; Tran et al., 2015; Ranganath et al., 2015 require handcrafted inference networks. However, all of them assume use of any neural network for amortized inference. None of them even require an inference network. Perhaps the authors mean handcrafted posterior approximations, which to some extent is true; however, the three mentioned are all algorithmic in nature: in Rezende & Mohamed (2015), the main decision choice is the flow length; Tran et al. (2015), the size of the variational data; Ranganath et al. (2015), the flow length on the auxiliary variable space. Each works well on different problems, but this is also true of variational objectives which admit intractable q (as the latter two consider, as does Salimans et al. (2015)). The paper's motivation could be better explained, and perhaps the authors could be clearer on what they mean by inference network.--------+ I also recommend the authors not term a variational inference method based on the class of approximating family. While black box variational inference in Ranganath et al. (2014) assumes a mean-field family, the term itself has been used in the literature to mean any variational method that imposes few constraints on the model class.","This paper is both time and topical in that it forms part of the growing and important literature of ways of representing approximate posterior distributions for variational inference that do need need a known or tractable density. The reviewers have identified a number of areas that when addressed will improve the paper greatly. These include: more clearer and structured introduction of methods, with the aim of highlighting how this work adds to the existing literature; careful explanation of the term wild variational inference, especially in context of alternative terms and more on relation to existing work; experiments on higher-dimensional data, much greater than 54 dimensions, to help better understand the advantages and disadvantages of this approach. It if for these reasons that the paper at this point is not yet ready for acceptance at the conference."
https://openreview.net/forum?id=SkYbF1slg,"This is an 18 page paper plus appendix which presents a mathematical derivation for infomax for an actual neural population with noise.  The original Bell & Sejnowski infomax framework only considered the no noise case.  Results are shown for natural image patches and the mnist dataset, which qualitatively resemble results obtained with other methods.----------------This seems like an interesting and potentially more general approach to unsupervised learning.  However the paper is quite long and it was difficult for me to follow all the twists and turns.  For example the introduction of the hierarchical model was confusing and it took several iterations to understand where this was going.  'Hierarchical' is probably not the right terminology here because it's not like a deep net hierarchy, it's just decomposing the tuning curve function into different parts.  I would recommend that the authors try to condense the paper so that the central message and important steps are conveyed in short order, and then put the more complete mathematical development into a supplementary document.----------------Also, the authors should look at the work of Karklin & Simoncelli 2011 which is highly related.  They also use an infomax framework for a noisy neural population to derive on and off cells in the retina, and they show the conditions under which orientation selectivity emerges.","The reviewers were not completely happy with the presentation, but it seems the theory is solid and interesting enough. I think ICLR needs more papers like this, which have convincing mathematical theory instead of merely relying on empirical results."
https://openreview.net/forum?id=HJGODLqgx,"This paper presents a novel model for unsupervised segmentation and classification of time series data.  A recurrent hidden semi-markov model is proposed.  This extends regular hidden semi-markov models to include a recurrent neural network (RNN) for observations.  Each latent class has its own RNN for modeling observations for that category.  Further, an efficient training procedure based on a variational approximation.  Experiments demonstrate the effectiveness of the approach for modeling synthetic and real time series data.----------------This is an interesting and novel paper.  The proposed method is a well-motivated combination of duration modeling HMMs with state of the art observation models based on RNNs.  The combination alleviates shortcomings of standard HSMM variants in terms of the simplicity of the emission probability.  The method is technically sound and demonstrated to be effective.----------------It would be interesting to see how this method compares quantitatively against CRF-based methods (e.g. Ammar, Dyer, and Smith NIPS 2014).  CRFs can model more complex data likelihoods, though as noted in the response phase there are still limitations.  Regardless, I think the merits of using RNNs for the class-specific generative models are clear.","This paper is a by-the-numbers extension of the hidden semi-Markov model to include nonlinear observations, and neural network-based inference. The paper is fairly clear, although the English isn't great. The experiments are thorough.    Where this paper really falls down is on originality. In particular, in the last two years there have been related works that aren't cited (and unfortunately weren't mentioned by the reviewers) that produce similar models. In particular, Johnson et al's 2016 NIPS paper develops almost the same inference strategy in almost the same model class.     http://stat.columbia.edu/~cunningham/pdf/GaoNIPS2016.pdf  https://arxiv.org/abs/1511.05121  https://arxiv.org/abs/1603.06277    This paper is borderline, but I think makes the cut by virtue of having experiments on real datasets, and by addressing a timely problem (how to have interpretable structure in neural network latent variable models)."
https://openreview.net/forum?id=BJbD_Pqlg,"This paper compares the performance, in terms of sensitivity to perturbations, of multilayer neural networks to human vision.  In many of the tasks tested, multilayer neural networks exhibit similar sensitivities as human vision.  ----------------From the tasks used in this paper one may conclude that multilayer neural networks capture many properties of the human visual system.  But of course there are well known adversarial examples in which small, perceptually invisible perturbations cause catastrophic errors in categorization, so against that backdrop it is difficult to know what to make of these results.  That the two systems exhibit similar phenomenologies in some cases could mean any number of things, and so it would have been nice to see a more in depth analysis of why this is happening in some cases and not others.  For example, for the noise perturbations described in the the first section, one sees already that conv2 is correlated with human sensitivity.  So why not examine how the first layer filters are being combined to produce this contextual effect?  From that we might actually learn something about neural mechanisms.----------------Although I like and am sympathetic to the direction the author is taking here, I feel it just scratches the surface in terms of analyzing perceptual correlates in multilayer neural nets.  ","I think the reviewers evaluated this paper very carefully and were well balanced. The reviewers all agree that the presented comparison between human vision and DNNs is interesting. At the same time, none of the reviewers would strongly defend the paper. As it stands, this work seems a little too premature for publication as the analysis does not go too much beyond what we already know. We encourage the authors to deepen the investigation and resubmit."
https://openreview.net/forum?id=Hku9NK5lx,"This work introduces a number of techniques to compress fully-connected neural networks while maintaining similar performance, including a density-diversity penalty and associated training algorithm. The core technique of this paper is to explicitly penalize both the overall magnitude of the weights as well as diversity between weights. This approach results in sparse weight matrices comprised of relatively few unique values. Despite introducing a more efficient means of computing the gradient with respect to the diversity penalty, the authors still find it necessary to apply the penalty with some low probability (1-5%) per mini-batch.----------------The approach achieves impressive compression of fully connected layers with relatively little loss of accuracy. I wonder if the cost of having to sort weights (even for only 1 or 2 out of 100 mini-batches) might make this method intractable for larger networks. Perhaps the sparsity could help remove some of this cost?----------------I think the biggest fault this paper has is the number of different things going on in the approach that are not well explored independently. Sparse initialization, weight tying, probabilistic application of density-diversity penalty and setting the mode to 0, and alternating schedule between weight tied standard training and diversity penalty training. The authors don't provide enough discussion of the relative importance of these parts. Furthermore, the only quantitative metric shown is the compression rate which is a function of both sparsity and diversity such that they cannot be compared on their own. I would really like to see how each component of the algorithm affects diversity, sparsity, and overall compression. ----------------A quick verification: Section 3.1 claims the density-diversity penalty is applied with a fixed probability per batch while 3.4 implies structured phases alternating between application of density-diversity and weight tied standard cross entropy. Is this scheme in 3.4 only applying the density-diversity penalty probabilistically when it is in the density-diversity phase?----------------Preliminary rating:--------I think this is an interesting paper but lacks sufficient empirical evaluation of its many components. As a result, the algorithm has the appearance of a collection of tricks that in the end result in good performance without fully explaining why it is effective.----------------Minor notes:--------Please resize equation 4 to fit within the margins (\resizebox{\columnwidth}{!}{ blah } works well in latex for this)",The reviewers unanimously recommended accepting the paper.
https://openreview.net/forum?id=Byj72udxe,"This paper proposes augmenting RNN-based language models with a pointer network in order to deal better with rare words. The pointer network can point to words in the recent context, and hence the prediction for each time step is a mixture between the usual softmax output and the pointer distribution over the recent words. The paper also introduces a new language modelling dataset, which overcomes some of the shortcomings of previous datasets.----------------The reason for the score I gave for this paper is that I find the proposed model a direct application of the previous work Gulcehre et al., which follows a similar approach but for machine translation and summarization. The main differences I find is that Gulcehre et al. use an encoder-decoder architecture, and use the attention weights of the encoder to point to locations of words in the input, while here an RNN is used and a pointer network produces a distribution over the full vocabulary (by summing the softmax probabilities of words in the recent context). The context (query) vector for the pointing network is also different, but this is also a direct consequence of having a different application.----------------While the paper describes the differences between the proposed approach and Gulcehre et al.’s approach, I find some of the claims either wrong or not that significant. For example, quoting from Section 1:--------“Rather than relying on the RNN hidden state to decide when to use the pointer, as in the recent work of Gulcehre et al. (2016), we allow the pointer component itself to decide when to use the softmax vocabulary through a sentinel.”--------As far as I can tell, your model also uses the recent hidden state to form a query vector,  which is matched by the pointer network to previous words. Can you please clarify what you mean here?----------------In addition, quoting from section 3 which describes the model of Gulcehre et al.:--------“Rather than constructing a mixture model as in our work, they use a switching network to decide which component to use”--------This is not correct. The model of Gulcehre is also a mixture model, where an MLP with sigmoid output (switching network) is used to form a mixture between softmax prediction and locations of the input text.----------------Finally, in the following quote, also from section 3: --------“The pointer network is not used as a source of information for the switching network as in our model.” --------It is not clear what the authors mean by “source of information” here. Is it the fact that the switching probability is part of the pointer softmax? I am wondering how significant this difference is.----------------With regards to the proposed dataset, there are also other datasets typically used for language modelling, including The Hutter Prize Wikipedia (enwik8) dataset (Hutter, 2012) and e Text8 dataset (Mahoney, 2009). Can you please comment on the differences between your dataset and those as well?----------------I would be happy to discuss with the authors the points I raised, and I am open to changing my vote if there is any misunderstanding on my part.","The reviewers liked this paper quite a bit, and so for this reason it is a perfectly fine paper to accept. However, it should be noted that the area chair was less enthusiastic. The area chairs mentions that the model appears to be an extension of Gulcehre et al. and the Penn Treebank perplexity experiments are too small scale to be taken seriously in 2017. Instead of experimenting on other known large-scale language modeling setups, the authors introduce their own new dataset (which is 1 order of magnitude smaller than the 1-Billion LM dataset by Chelba et al). The new dataset might be a good idea, but the area chair doesn't understand why the authors do not run public available systems as baselines. This should have been fairly easy to do and would have significantly strengthen the result of this work. The PCs thus encourage to authors to take into account this feedback and consider updating their paper accordingly."
https://openreview.net/forum?id=r1aGWUqgg,"This paper is about learning unsupervised state representations using multi-task reinforcement learning.  The authors propose a novel approach combining gated neural networks with multitask learning with robotics priors. They evaluated their approach on two simulated datasets and showed promising results. The paper is clearly written and is theoretically sound.----------------Positives:--------+ Gating to enable learning a joint representation--------+ Multi-task learning extended from a single task in prior work--------+ Combining multiple types of losses to learn a strong representation (Coherence, Proportionality, Causality, Repeatability, Consistency and Separation)----------------Negatives:--------- Parameters choice is arbitrary (w parameters)--------- Limiting the multi-task learning to be different to individual tasks rather than sharing and transferring knowledge between tasks--------- The experiments could have been conducted using a standardized simulation tool such as OpenAI Gym to make it easy to compare.----------------I would recommend that the authors consider a more standardized way of picking the model parameters and evaluate on a more standard and high-dimensional datasets.","The authors propose to explore an important problem -- learning state representations for reinforcement learning. However, the experimental evaluation raised a number of concerns among the reviewers. The method is tested on only a single relatively easy domain, with some arbitrarily choices justified in unconvincing ways (e.g. computational limitations as a reason to not fix aliasing). The evaluation also compares to extremely weak baselines. In the end, there is insufficient evidence to convincingly show that the method actually works, and since the contribution is entirely empirical (as noted by the reviewers in regard to some arbitrary parameter choices), the unconvincing evaluation makes this method unsuitable for publication at this time. The authors would be encouraged to evaluate the method rigorously on a wide range of realistic tasks."
https://openreview.net/forum?id=rJLS7qKel,"Deep RL (using deep neural networks for function approximators in RL algorithms) have had a number of successes solving RL in large state spaces. This empirically driven work builds on these approaches. It introduces a new algorithm which performs better in novel 3D environments from raw sensory data and allows better generalization across goals and environments. Notably, this algorithm was the winner of the Visual Doom AI competition.----------------The key idea of their algorithm is to use additional low-dimensional observations (such as ammo or health which is provided by the game engine) as a supervised target for prediction. Importantly, this prediction is conditioned on a goal vector (which is given, not learned) and the current action. Once trained the optimal action for the current state can be chosen as the action that maximises the predicted outcome according the goal. Unlike in successor feature representations, learning is supervised and there is no TD relationship between the predictions of the current state and the next state.----------------There have been a number of prior works both in predicting future states as part of RL and goal driven function approximators which the authors review in section 2. The key contributions of this work are the focus on Monte Carlo estimation (rather than TD), the use of low-dimensional ‘measurements’ for prediction, the parametrized goals and, perhaps most importantly, the empirical comparison to relevant prior work.----------------In addition to the comparison with Visual Doom AI, the authors show that their algorithm is able to learn generalizable policies which can respond, without further training, to limited changes in the goal.----------------The paper is well-communicated and the empirical results compelling and will be of significant interest.----------------Some minor potential improvements:--------There is an approximation in the supervised training as it is making an on-policy assumption but it learns from a replay buffer (with the Monte Carlo regression the expectation of the remainder of the trajectory is assumed to follow the current policy, but is being sampled from episodes generated by prior versions of the policy). This should be discussed.--------The algorithm uses additional metadata (the information about which parts of the sensory input are worth predicting) that the compared algorithms do not. I think this, and the limitations of this approach (e.g. it may not work well in a sensory environment if such measurements are not provided) should be mentioned more clearly.","This paper details the approach that won the VizDoom competition - an on-policy reinforcement learning approach that predicts auxiliary variables, uses intrinsic motivation, and is a special case of a universal value function. The approach is a collection of different methods, but it yields impressive empirical results, and it is a clear, well-written paper."
https://openreview.net/forum?id=S1JG13oee,"In this paper, the authors extend the f-GAN by using Bregman divergences for density ratio matching. The argument against f-GAN (which is a generalization of the regular GAN) is that the actual objective optimized by the generator during training is different from the theoretically motivated objective due to gradient issues with the theoretically motivated objective. In b-GANS, the discriminator is a density ratio estimator (r(x) = p(x) / q(x)), and the generator tries to minimize the f-divergence between p and q by writing p(x) = r(x)q(x).----------------My main problem with this paper is that it is unclear why any of this is useful. The connection to density estimation is interesting, but any derived conclusions between the two seem questionable. For example, in previous density estimation literature, the Pearson divergence is more stable. The authors claim that the same holds for GANS and try to show this in their experiments. Unfortunately, the experiments section is very confusing with unilluminating figures. Looking at the graph of density ratios is not particularly illuminating. They claim that for the Pearson divergence and modified KL-divergence, ""the learning did not stop"" by looking at the graph of density ratios. This is completely hand-wavey and no further evidence is given to back this claim. Also, why was the normal GAN objective not tried in light of this analysis? Furthermore, it seems that despite criticizing normal GANs for using a heuristic objective for the generator, multiple heuristics objectives and tricks are used to make b-GAN work.----------------I think this paper would be much improved if it was rewritten in a clear fashion. As it stands, it is difficult to understand the motivation or intuition behind this work.","The paper may have an interesting contribution, but at present its motivation, and the presentation in general, are not clear enough. After a re-write, the paper might become quite interesting and should be submitted to some other forum."
https://openreview.net/forum?id=BybtVK9lg,"The authors propose NVI for LDA variants. The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI. The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?)----------------In general, I like the direction of this paper and NVI looks promising for LDA. The experimental results however confound model vs inference which makes it hard to understand the significance of the results. Furthermore, the authors don't discuss hyper-parameter selection which is known to significantly impact performance of topic models. This makes it hard to understand when the proposed method can be expected to work. ----------------Can you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters?----------------Figure 1: Is this prior or posterior? The text talks about sparsity whereas the y-axis reads ""log p(topic proportions)"" which is a bit confusing.----------------Section 3.2: it is not clear what you mean by unimodal in softmax basis. Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha<1 makes it multimodal. Isn't the softmax basis still multimodal?----------------None of the numbers include error bars. Are the results statistically significant?------------------------Minor comments:----------------Last term in equation (3) is not ""error""; reconstruction accuracy or negative reconstruction error perhaps?----------------The idea of using an inference network is much older, cf. Helmholtz machine. ","The reviewers agree that the approach is interesting and the paper presents useful findings. They also raise enough questions and suggestions for improvements that I believe the paper will be much stronger after further revision, though these seem straightforward to address."
https://openreview.net/forum?id=BJFG8Yqxl,"The paper proposes the group sparse autoencoder that enforces sparsity of the hidden representation group-wise, where the group is formed based on labels (i.e., supervision). The p-th group hidden representation is used for reconstruction with group sparsity penalty, allowing learning more discriminative, class-specific patterns in the dataset. The paper also propose to combine both group-level and individual level sparsity as in Equation (9). ----------------Clarity of the paper is a bit low. --------- Do you use only p-th group's activation for reconstruction? If it is true, then for Equation (9) do you use all individual hidden representation for reconstruction or still using the subset of representation corresponding to that class only? --------- In Equation (7), RHS misses the summation over p, and wondering it is a simple typo.--------- Is the algorithm end-to-end trainable? It seems to me that the group sparse CNN is no more than the GSA whose input data is the feature extracted from sequential CNNs (or any other pretrained CNNs).----------------Other comments are as follows:--------- Furthermore the group sparse autoencoder is (semi-) supervised method since it uses label information to form a group, whereas the standard sparse autoencoder is fully unsupervised. That being said, it is not surprising that group sparse autoencoder learns more class-specific pattern whereas sparse autoencoder doesn't. I think the fair comparison should be to autoencoders that combines classification for their objective function.--------- Although authors claim that GSA learns more group-relevant features, Figure 3 (b) is not convincing enough to support this claim. For example, the first row contains many filters that doesn't look like 1 (e.g., very last column looks like 3).--------- Other than visual inspection, do you observe improvement in classification using proposed algorithm on MNIST experiments?--------- The comparison to the baseline model is missing. I believe the baseline model shouldn't be the sequential CNN, but the sequential CNN + sparse autoencoder. In addition, more control experiment is required that compares between the Equation (7)-(9), with different values of \alpha and \beta.----------------Missing reference:--------Shang et al., Discriminative Training of Structured Dictionaries via Block Orthogonal Matching Pursuit, SDM 2016 - they consider block orthgonal matching pursuit for dictionary learning whose blocks (i.e., projection matrices) are constructed based on the class labels for discirminative training.","This paper proposes to use a group sparsity penalty to train an autoencoder on question answering. The idea to leverage hierarchies of categories in labels is an appealing one. However the paper has problems:  - it is not clear. In particular, some of the equations do not make sense. This has been pointed by reviewers and not corrected.  - the choice of the particular group sparsity penalty is not well justified or empirically validated with ablation experiments: experiments lack key comparisons and simply compare to unrelated baselines.  In its current form, the paper cannot be recommended for acceptance."
https://openreview.net/forum?id=rkYmiD9lg,This paper proposes to use the tensor train (TT) decomposition to represent the full polynomial linear model. The TT form can reduce the computation complexity in both of inference and model training. A stochastic gradient over a Riemann Manifold has been proposed to solve the TT based formulation. The empirical experiments validate the proposed method.----------------The proposed approach is very interesting and novel for me. I would like to vote acceptance on this paper. My only suggestion is to include the computational complexity per iteration.,"Modeling nonlinear interactions between variables by imposing Tensor-train structure on parameter matrices is certainly an elegant and novel idea. All reviewers acknowledge this to be the case. But they are also in agreement that the experimental section of this paper is somewhat weak relative to the rest of the paper, making this paper borderline. A revised version of this paper that takes reviewer feedback into account is invited to the workshop track."
https://openreview.net/forum?id=S1jE5L5gl,"The authors describe the concrete distribution, a continuous approximation to--------discrete distributions parameterized by a vector of continuous positive numbers--------proportional to the probability of each discrete result. The concrete--------distribution is obtained by using the softmax function to approximate the--------argmax operator. The paper is clearly written, original and significant.--------The experiments clearly illustrate the advantages of the proposed method.----------------Some minor questions:----------------""for the general n-ary case the Gumbel is a crucial 1 and the Gumbel-Max trick cannot be generalized--------for other additive noise distributions""----------------What do you mean by this? Can you be more specific?----------------What is the temperature values used to obtain Table 1 and the table in Figure 4.","This paper proposes a neat general method for relaxing models with discrete softmax choices into closely-related models with continous random variables. The method is designed to work well with the reparameterization trick used in stochastic variational inference. This work is likely to have wide impact.    Related submissions at ICLR:  ""Categorical Reparameterization with Gumbel-Softmax"" by Jang et al. contains the same core idea. ""Discrete variational autoencoders"", by Rolfe, contains an alternative relaxation for autoencoders with discrete latents, which I personally find harder to follow."
https://openreview.net/forum?id=rJ0-tY5xe,"This paper investigates a set of tasks that augment the basic bAbI problems. In particular, some of the people and objects in the scenarios are replaced with unknown variables. Some of these variables must be known to solve the question, thus the agent must learn to query for the values of these variables. Interestingly, one can now measure both the performance of the agent in correctly answering the question, and its efficiency in asking for the values of the correct unknown variables (and not variables that are unnecessary to answer the question). This inferring of unknown variables goes beyond what is required for the vanilla version of the bAbI tasks, which are now more or less solved.----------------The paper is well-written, and the contributions are clear. Due to the very limited vocabulary and structure of the bAbI problems in general, I think these tasks (and variants on them) should be viewed more as basic reasoning tasks than natural language understanding. I’m not convinced by the claim of the paper that this really tests the ‘interaction’ capabilities of agents – while the task is phrased as a kind of interaction, I think it’s more aptly described by simply ‘inferring important unknown variables’, which (while important) is more related to reasoning. I’m not sure whether the connection of this ability to ‘interaction’ is more a superficial one.----------------That being said, it is certainly true that conversational agents will need basic reasoning abilities to converse meaningfully with humans. I sympathise with the general goal of the bAbI tasks, which is to test these reasoning abilities in synthetic environments, that are just complicated enough (but not more) to drive the construction of interesting models. I am convinced by the authors that their extension to these tasks are interesting and worthy of future investigation, and thus I recommend the acceptance of the paper.","The program committee appreciates the authors' response to concerns raised in the reviews. While there are some concerns with the paper that the authors are strongly encouraged to address for the final version of the paper, overall, the work has contributions that are worth presenting at ICLR."
https://openreview.net/forum?id=HkNKFiGex,"UPDATE: The rewritten paper is more focused and precise than the previous version. The ablation studies and improved evaluation of the IAN model help to the make clear the relative contributions of the proposed MDC and orthogonal regularization. Though the paper is much improved, in my opinion there is still too much emphasis on the photo-editing interface. In addition, the MDC blocks are used in the generator of the model but their efficacy is measured via discriminative experiments. All-in-all I am updating my score to a 5.----------------==========----------------This paper proposes a hybridization of a VAE and a GAN whereby the generator both generates random samples and produces reconstructions of the real data, and the discriminator attempts to classify each true data point, sample, and reconstruction as being real, fake, or reconstructed. It also proposes a user-facing interface with an interactive image editing algorithm along with various modifications to standard generative modeling architectures.----------------Pros:--------+ The IAN model itself is interesting as standard GAN-based approaches do not simultaneously train an autoencoder.----------------Cons:--------- The writing is unclear at times and the mathematical formulation of the IAN is not very precise.--------- Many different ideas are proposed in the paper without sufficient empirical validation to characterize their individual contributions.--------- The photo editing interface, though interesting, is probably better suited for a conference with more of an HCI focus.----------------* Section 2: The gradient descent step procedure seems to be quite similar to the approach proposed by [2]. More elaboration on the differences would be helpful.--------* Section 3: It is unclear whether the discriminator has three binary outputs or if there is a softmax over the three possible labels. The paper does not mention whether L_G and L_D are minimized or maximized. Presumably they are both minimized, but in that case it is counter-intuitive that the generator attempts to decrease the probability that the discriminator assigns the ""real"" label to the generated samples and reconstructions. In addition, in the minimization scenario L_D maximizes the probability of the correct labels being assigned to X_gen and \hat{X} but minimizes the probability of the ""real"" label being assigned to X.--------* Section 3.2: It is odd that not training MADE leads to better performance, as training MADE should lead to a better variational approximation to the true posterior. More exploration seems warranted here.--------* Section 3.3.2: One drawback of autoregressive approaches is that sampling is slow. How do you reconcile this with its use in an interactive application, where speed is important?--------* Figure 7: The Inception score is typically expressed as an exponentiated KL-divergence. It is odd that the scores are being presented here as percents.--------* Section 4.1: The Inception score's direct application to non-natural images is indeed problematic. One potential workaround is to compute exponentiated KL for a discriminative net trained specifically for the dataset, e.g. to predict binary attributes on CelebA. ----------------Overall, the paper attempts to simultaneously do too many things. It could be made much stronger by focusing on the primary contribution, the IAN, and performing a comparison against similar approaches such as [1]. The other techniques, such as IAF with randomized MADE, MDC blocks, and orthogonal regularization are potentially interesting in their own right but the current results are not conclusive as to their specific benefits.----------------[1] Larsen, Anders Boesen Lindbo, Søren Kaae Sønderby, and Ole Winther. ""Autoencoding beyond pixels using a learned similarity metric."" arXiv preprint arXiv:1512.09300 (2015).--------[2] J.-Y. Zhu, P. Krähenbühl, E. Shechtman, and A. A. Efros, “Generative Visual Manipulation on the Natural Image Manifold,” ECCV 2016.","Here is a summary of strengths and weaknesses as per the reviews:    Strengths  Work/application is exciting (R3)  Enough detail for reproducibility (R3)  May provide a useful analysis tool for generative models (R1)    Weaknesses  Clarity of the IAN model - presentation is scattered and could benefit from empirical analysis to tease out which parts are most important (R3,R2,R1); AC comments that the paper was revised in this regard and R3 was satisfied, updating their score  Lack of focus: is it the visualization/photo manipulation technique, or is it the generative model? (R3, R2)  Writing could use improvement (R2)  Mathematical formulation of IAN not precise (R2)    The authors provided a considerable overhaul of the paper, re-organizing/re-focusing it in response to the reviews and adding additional experiments.     This, in turn, resulted in R1, R2 and R3 upgrading their scores. The paper is more clearly in accept territory, in the ACÕs opinion. The AC recommends acceptance as a poster."
https://openreview.net/forum?id=Skn9Shcxe,"This paper provides a new perspective to understanding the ResNet and Highway net. The new perspective assumes that the blocks inside the networks with residual or skip-connection are groups of successive layers with the same hidden size, which performs to iteratively refine their estimates of the same feature instead of generate new representations. Under this perspective, some contradictories with the traditional representation view induced by ResNet and Highway network and other paper can be well explained.----------------The pros of the paper are:--------1. A novel perspective to understand the recent progress of neural network is proposed.--------2. The paper provides a quantitatively experimentals to compare ResNet and Highway net, and shows contradict results with several claims from previous work. The authors also give discussions and explanations about the contradictories, which provides a good insight of the disadvantages and advantages between these two kind of networks.----------------The main cons of the paper is that the experiments are not sufficient. For example, since the main contribution of the paper is to propose the “unrolled iterative estimation"" and the stage 4 of Figure 3 seems not follow the assumption of ""unrolled iterative estimation"" and the authors says: ""We note that stage four (with three blocks) appears to be underestimating the representation values, indicating a probable weak link in the architecture."". Thus, it would be much better to do experiments to show that under some condition, the performance of stage 4 can follow the assumption. ----------------Moreover, the paper should provide more experiments to show the evidence of ""unrolled iterative estimation"", not comparing ResNet with Highway Net. The lack of experiments on this point is the main concern from myself.","The paper provides interesting new interpretations of highway and residual networks, which should be of great interest to the community."
https://openreview.net/forum?id=BkV4VS9ll,"I did enjoy reading some of the introductions and background, in particular that of reminding readers of popular papers from the late 1980s and early 1990s. The idea of the proposal is straight forward: remove neurons based on the estimated change in the loss function from the packpropagation estimate with either first or second order backpropagation. The results are as expected that the first order method is worse then the second order method which in turn is worse than the brute force method.----------------However, there are many reasons why I think that this work is not appropriate for ICLR. For one, there is now a much stronger comprehension of weight decay algorithms and their relation to Bayesian priors which has not been mentioned at all. I would think that any work in this regime would require at least some comments about this. Furthermore, there are many statements in the text that are not necessarily true, in particular in light of deep networks with modern regularization methods. For example, the authors state that the most accurate method is what they call brute-force. However, this assumes that the effects of each neurons are independent which might not be the case. So the serial order of removal is not necessarily the best. ----------------I also still think that this paper is unnecessarily long and the idea and the results could have been delivered in a much compressed way. I also don’t think just writing a Q&A section is not enough, and the points should be included in the paper.","The paper does not seem to have enough novelty, and the contribution is not clear enough due to presentation issues."
https://openreview.net/forum?id=Bkfwyw5xg,"This paper evaluates how different context types affect the quality of word embeddings on a plethora of benchmarks.----------------I am ambivalent about this paper. On one hand, it continues an important line of work in decoupling various parameters from the embedding algorithms (this time focusing on context); on the other hand, I am not sure I understand what the conclusion from these experiments is. There does not appear to be a significant and consistent advantage to any one context type. Why is this? Are the benchmarks sensitive enough to detect these differences, if they exist?----------------While I am OK with this paper being accepted, I would rather see a more elaborate version of it, which tries to answer these more fundamental questions.","Reviewers agree that the findings are not clear enough to be of interest, though the effort to do a controlled study is appreciated."
https://openreview.net/forum?id=Sks9_ajex,"The paper presented a modified knowledge distillation framework that minimizes the difference of the sum of statistics across the a feature map between the teacher and the student network. The authors empirically demonstrated the proposed methods outperform the fitnet style distillation baseline. ----------------Pros:--------+ The author evaluated the proposed methods on various computer vision dataset --------+ The paper is in general well-written----------------Cons:  --------- The method seems to be limited to the convolutional architecture--------- The attention terminology is misleading in the paper. The proposed method really just try to distill the summed squared(or other statistics e.g. summed lp norm) of  activations in a hidden feature map.--------- The gradient-based attention transfer seems out-of-place. The proposed gradient-based methods are never compared directly to nor are used jointly with the ""attention-based"" transfer. It seems like a parallel idea added to the paper that does not seem to add much value.--------- It is also not clear how the induced 2-norms in eq.(2) is computed. Q is a matrix \in \mathbb{R}^{H \times W}  whose induced 2-norm is its largest singular value. It seems computationally expensive to compute such cost function. Is it possible the authors really mean the Frobenius norm?----------------Overall, the proposed distillation method works well in practice but the paper has some organization issues and unclear notation.  ","Important task (attention models), interesting distillation application, well-written paper. The authors have been responsive in updating the paper, adding new experiments, and being balanced in presenting their findings. I support accepting this paper."
https://openreview.net/forum?id=ry7O1ssex,"This paper presents a bridging of energy-based models and GANs, where -- starting from the energy-based formalism -- they derive an additional entropy term that is not present in the original GAN formulation and is also absent in the EBGAN model of Zhao et al (2016, cited work). The relation between GANs and energy-based models was, as far as I know, first described in Kim and Bengio (2016, cited work) who also introduced the entropy regularization. It is also discussed in another ICLR submission (Dai et al. ""Calibrating Energy-based Generative Adversarial Networks""). There are two novel contribution of this paper: (1) VGAN: the introduction of a novel entropy approximation; (2) VCD: variational contrastive divergence is introduced as a  novel learning algorithm (however, in fact, a different model). The specific motivation for this second contribution is not particularly clear. ----------------The two contributions offered by the authors are quite interesting and are well motivated in the sense that the address the important problem of avoiding dealing with the generally intractable entropy term. However, unfortunately the authors present no results directly supporting either contribution. For example, it is not at all clear what role, if any, the entropy approximation plays in the samples generated from the VGAN model. Especially in the light of the impressive samples from the EBGAN model that has no corresponding term. The results provided to support the VCD algorithm, come in the form of a comparison of samples and reconstructions. But the samples provided here strike me as slightly less impressive compared to either the VGAN results or the SOTA in the literature - this is, of course, difficult to evaluate. ----------------The results the authors do present include qualitative results in the form of reasonably compelling samples from the model trained on CIFAR-10, MNIST and SVHN datasets. They also present quantitative results int he form of semi-supervised learning tasks on the MNIST and SVHN. However these--------quantitative results are not particularly compelling as they show limited improvement over baselines. Also, there is no reference to the many--------existing semi-supervised results on these datasets. ----------------Summary: The authors identify an important problem and offer two novel and intriguing solutions. Unfortunately the results to not sufficiently support either--------contribution.","This paper is timely since it addresses the connections between energy-based models, GANS, and the general space of generative models. The two principal concerns about the paper are: lack of clarity and coherence in the paper; inability to effectively judge the effectiveness of the method. The responses of the authors address these concern to some extent, but these concerns remain. The paper will have much higher-impact after further introspection, but at present, the paper is not yet ready for acceptance at the conference."
https://openreview.net/forum?id=ByvJuTigl,"Summary:------------------------The authors propose a histogram based state representation with differentiable motion models and observation updates for state tracking from observations. Linear model with Gaussian noise is used as the motion model, while a neural network is used to learn the measurement model. They track robot states in: (1) 1-D hallway, and (2) a 2D arena.------------------------Positives:--------------------------1. Show how to encode prior knowledge about state-transitions in the architecture.--------2. No assumptions about the observation model, which is learned purely from data.--------3. Better accuracy than baselines with limited training data.----------------Negatives:--------------------------1. The motion model is too simplistic. The authors in their response to earlier questions say that a generic feed-forward neural network could be used to model more complicated motions. However, then the novelty of their framework is not clear -- as then the proposed model would just be a couple of neural networks to learn the motion and observation models.----------------2. The observation model again is too simplistic (e.g., one dimensional observations), and is proposed to be a generic feed-forward network. Here again, the technical novelty is not clear.----------------3. The histogram based representation is not scalable as also highlighted by the authors. Hence, the proposed approach as it is, cannot be applied to more complicated settings.----------------4. In Figure 5(a,b), where they compare the state-estimation accuracy with other baselines (i.e., LSTMs), it is clear that the accuracy of the LSTM has not saturated, while that of their model has. They should do larger scale experiments with more training data (e.g., 10k,100k,500k samples). --------Note that while sample efficiency is a desirable property (also discussed in Section 6.2), we do expect models with prior knowledge to work better for small number of samples than models which do not assume any structure. Experiments with larger number of samples would be insightful.","In many respects, this is a strong paper, in my opinion better than the reviews thus far in the system suggest. The idea of learning the parameters of a state estimation system, even if it is a simple example like a histogram filter, is an interesting idea from both the ML and robotics perspectives.    However, I think it's also fairly clear (from the reviews if nothing else), that substantial additional work needs to be done if this paper is to convey these ideas clearly and impactfully to the ICLR community. The value of the histogram filter may be obvious to the robotics community (though even there it seems like it would be worth pursuing the extension of these ideas to the case e.g. of particle filters), as these have historically been an important conceptual milestone in robotic perception, but the examples shown in this paper are extremely simplistic. Given that histogram filters inherently scale poorly with dimension, it's not clear from this paper itself why the techniques show particular promise for scaling up to realistic domains in the future.    Pros:  + Interesting idea of using training a state estimator end-to-end for robotics tasks    Cons:  - Histogram filters, while well-motivated historically from the robotics standpoint, are really toy examples at this point except in a very small number of settings.  - The results aren't all that compelling, showing modest improvement (over seemingly not-very-tuned alternatively approaches), on fairly simple domains."
https://openreview.net/forum?id=Bks8cPcxe,"The paper presents DeepDSL, a ""domain specific language (DSL) embedded in--------Scala, that compiles deep networks written in DeepDSL to Java source code"". It--------introduces its syntax and the key concepts which differentiate it--------from other existing frameworks, such as Torch7, Theano, Caffe, TensorFlow,--------CNTK, Chainer and MXNet. It also benchmarks speed and memory usage against--------TensorFlow and Caffe on a variety of convolutional neural network architectures.----------------The paper is clear and well written and it does a good job of presenting DeepDSL--------in the context of existing deep learning frameworks.----------------However, I don't think ICLR is the right venue for this type of work. Some of--------the ideas it presents are interesting, but overall the paper lacks novelty and--------potential impact and stays firmly within the realm of deep learning framework--------whitepapers such as [1,2,3,4], which to my knowledge don't have a precedent of--------being accepted at venues like ICLR.----------------[1]: Bergstra, James, et al. ""Theano: A CPU and GPU math compiler in Python.""--------Proc. 9th Python in Science Conf. 2010.----------------[2]: Bastien, Frédéric, et al. ""Theano: new features and speed improvements.""--------arXiv preprint arXiv:1211.5590 (2012).----------------[3]: Abadi, Martın, et al. ""Tensorflow: Large-scale machine learning on--------heterogeneous distributed systems."" arXiv preprint arXiv:1603.04467 (2016).----------------[4]: The Theano Development Team et al. ""Theano: A Python framework for fast--------computation of mathematical expressions."" arXiv preprint arXiv:1605.02688--------(2016).----------------UPDATE: The rating has been revised to a 6 following the authors' reply.",All reviewers find value in the contributions. 
https://openreview.net/forum?id=rJsiFTYex,"The paper proposes and analyses three methods applied to traditional LSTMs: Monte Carlo test-time model averaging, average pooling, and residual connections. It shows that those methods help to enhance traditional LSTMs on sentiment analysis. ----------------Although the paper is well written, the experiment section is definitely its dead point. Firstly, although it shows some improvements over traditional LSTMs, those results are not on par with the state of the art. Secondly, if the purpose is to take those extensions as strong baselines for further research, the experiments are not adequate: the both two datasets which were used are quite similar (though they have different statistics). I thus suggest to carry out more experiments on more diverse tasks, like those in ""LSTM: A Search Space Odyssey""). ----------------Besides, those extensions are not really novel.",The paper attempts to perform an interesting exploration (how to combine different tricks for LSTM training) but does not take it far enough.     Pros:  - interesting attempt at studying different techniques to improve LSTM training results  Cons:  - not very strong baselines  - limited set of domains were explored  - low in novelty (which wouldn't be a problem if the comparison was more thorough -- see above 2 points).
https://openreview.net/forum?id=B16dGcqlx,"This paper proposed a novel adversarial framework to train a model from demonstrations in a third-person perspective, to perform the task in the first-person view. Here the adversarial training is used to extract a novice-expert (or third-person/first-person) independent feature so that the agent can use to perform the same policy in a different view point.----------------While the idea is quite elegant and novel (I enjoy reading it), more experiments are needed to justify the approach. Probably the most important issue is that there is no baseline, e.g., what if we train the model with the image from the same viewpoint? It should be better than the proposed approach but how close are they? How the performance changes when we gradually change the viewpoint from third-person to first-person? Another important question is that maybe the network just blindly remembers the policy, in this case, the extracted feature could be artifacts of the input image that implicitly counts the time tick in some way (and thus domain-agonistic), but can still perform reasonable policy. Since the experiments are conduct in a synthetic environment, this might happen. An easy check is to run the algorithm on multiple viewpoint and/or with blurred/differently rendered images, and/or with random initial conditions.----------------Other ablation analysis is also needed. For example, I am not fully convinced by the gradient flipping trick used in Eqn. 5, and in the experiments there is no ablation analysis for that (GAN/EM style training versus gradient flipping trick). For the experiments, Fig. 4,5,6 does not have error bars and is not very convincing.","pros:  - new problem  - huge number of experimental evaluations, based in part on open-review comments    cons:  - the main critiques related to not enough experiments being run; this has been addressed in the revised version    The current reviewer scores do not yet reflect the many updates provided by the authors.  I therefore currently learn in favour of seeing this paper accepted."
https://openreview.net/forum?id=Bkul3t9ee,"The paper explores a simple approach to learning reward functions for reinforcement learning from visual observations of expert trajectories for cases were only little training data is available. To obtain descriptive rewards even under such challenging conditions the method re-uses a pre-trained neural network as feature extractor (this is similar to a large body of work on task transfer with neural nets in the area of computer vision) and represents the reward function as a weighted distance to features for automatically extracted ""key-frames"" of the provided expert trajectories.----------------The paper is well written and explains all involved concepts clearly while also embedding the presented approach in the literature on inverse reinforcement learning (IRL). The resulting algorithm is appealing due to its simplicity and could prove useful for many real world robotic applications. I have three main issues with the paper in its current form, if these can be addressed I believe the paper would be significantly strengthened:--------1) Although the recursive splitting approach for extracting the ""key-frames"" seems reasonable and the feature selection is well motivated I am missing two baselines in the experiments:--------   - what happens if the feature selection is disabled and the distance between all features is used ? will this immediately break the procedure ? If not, what is the trade-off here ? --------   - an even simpler baseline than what is proposed in the paper would be the following procedure: simply use all frames of the recorded trajectories, calculate the distance to them in feature space and weights them according to their time as in the approach proposed in the paper. How well would that work ?--------2) I understand the desire to combine the extracted reward function with a simple RL method but believe the used simple controller could potentially introduce a significant bias in the experiments since it requires initialization from an expert trajectory. As a direct consequence of this initialization the RL procedure is already started close to a good solution and the extracted reward function is potentially only queried in a small region around what was observed in the initial set of images (perhaps with the exception of the human demonstrations). Without an additional experiment it is thus unclear how well the presented approach will work in combination with other RL methods for training the controller.--------3) I understand that the low number of available images excludes training a deep neural net directly for the task at hand but one has to wonder how other baselines would do. What happens if one uses a random projection of the images to form a feature vector? How well would a distance measure using raw images (e.g. L2 norm of image differences) or a distance measure based on the first principal components work? It seems that occlusions etc. would exclude them from working well but without empirical evidence it is hard to confirm this.----------------Minor issues:--------- Page 1: ""make use of ideas about imitation"" reads a bit awkwardly--------- Page 3: ""We use the Inception network pre-trained ImageNet"" -> pre-trained for ImageNet classification--------- Page 4: the definition of the transition function for the stochastic case seems broken--------- Page 6: ""efficient enough to evaluate"" a bit strangely written sentence----------------Additional comments rather than real issues:--------- The paper is mainly of empirical nature, little actual learning is performed to obtain the reward function and no theoretical advances are needed. This is not necessarily bad but makes the empirical evaluation all the more important. --------- While I liked the clear exposition the approach -- in the end -- boils down to computing quadratic distances to features of pre-extracted ""key-frames"", it is nice that you make a connection to standard IRL approaches in Section 2.1 but one could argue that this derivation is not strictly necessary.","Quality, Clarity:    The work is well motivated and clearly written -- no issues there.    Originality, Significance:    The idea is simple and well motivated, i.e., the learning of reward functions based on feature selection from identified subtasks in videos.    pros:  - the problem is difficult and relevant: good solutions would have impact    cons:  - the benefit with respect to other baselines for various choices, although the latest version does contain updated baselines  - the influence of the initial controller on the results  - the work may gain better appreciation at a robotics conference    I am very much on the fence for this paper.  It straddles a number of recent advances in video segmentation, robotics, and RL, which makes the specific technical contributions harder to identify. I do think that a robotics conference would be appreciative of the work, but better learning of reward functions is surely a bottleneck and therefore of interest to ICLR.  Given the lukewarm support for this paper by reviewers, the PCs decided not to accept the paper, but invite the authors to present it in the workshop track."
https://openreview.net/forum?id=SJttqw5ge,"Description:----------------This paper presents a reinforcement learning architecture where, based on ""natural-language"" input, a meta-controller chooses subtasks and communicates them to a subtask controller that choose primitive actions, based on the communicated subtask. The goal is to scale up reinforcement learning agents to large-scale tasks.----------------The subtask controller embeds the subtask definition (arguments) into vectors by a multi-layer perceptron including an ""analogy-making"" regularization. The subtask vectors are combined with inputs at each layer of a CNN. CNN outputs (given the observation and the subtask) are then fed to one of two MLPs; one to compute action probabilities in the policy (exponential falloff of MLP outputs) and the other to compute termination probability (sigmoid from MLP outputs).----------------The meta controller takes a list of sentences as instructions embeds them into a sequence of subtask arguments (not necessarily a one-to-one mapping). A context vector is computed by a CNN from the observation, the previous sentence embedding, the previous subtask and its completion state. The subtask arguments are computed from the context vector through further mechanisms involving instruction retrieval from memory pointers, and hard/soft decisions whether to update the subtask or not.----------------Training involves policy distillation+actor-critic training for the subtask controller, and actor-critic training for the meta controller keeping the subtask controller frozen.----------------The system is tested in a grid world where the agent moves and interacts with (picks up/transforms) various item/enemy types.--------It is compared to a) a flat controller not using a subtask controller, and b) subtask control by mere concatenation of the subtask embedding to the input with/without the analogy-making regularization.------------------------Evaluation:----------------The proposed architecture seems reasonable, although it is not clear why the specific way of combining subtask embeddings in the subtask controller would be the ""right"" way to do it.----------------I do not feel the grid world here really represents a ""large-scale task"": in particular the 10x10 size of the grid is very small. This is disappointing since this was a main motivation of the work.----------------Moreover, the method is not compared to any state of the art alternatives. This is especially problematic because the test is not on established benchmarks. It is not really possible, based on the shown results, to put the performance in context of other works.","The paper looks at how natural language instructions can be decomposed into sub-tasks for as-yet-unseen new tasks  hence the zero-shot generalization, which is considered to be the primary challenge to be solved.   The precise problem being solved by the original paper is not clearly expressed in the writing. This left some reviewers asking for comparisons, while the authors note that for the specific nature of the problem being solved, they could not seen any particular methods that is known to be capable of tackling this kind of problem. The complexity of the system and simplicity of the final examples were also found to be contradictory by a subset of the reviewers, although this is again related to the understanding of the problem being solved.    With scores of 3/4/5/7, the ideas in this paper were appreciated by a subset of reviewers.   At the time of the writing of this metareview, the authors have posted a fairly lengthy rebuttal (Jan 18) and significant revisions (Jan 18), with no further responses from reviewers as of yet. However, it is difficult for reviewers to do a full re-evaluation of the paper on such short notice.     While the latest revisions are commendable, it is unfortunately difficult to argue strongly in favor of this paper at present."
https://openreview.net/forum?id=HJ7O61Yxe,"Because the authors did not respond to reviewer feedback, I am maintaining my original review score.-------------------------------------This paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach: they design a flexible parametric (but not generative) model with Gaussian latent factors and fit it using a rich training objective including terms for reconstruction (of observed time series) error, smoothness in the latent state space (via a KL divergence term encouraging neighbor states to be similarly distributed), and a final regularizer that encourages related time series to have similar latent state trajectories. Relations between trajectories are hard coded based on pre-existing knowledge, i.e., latent state trajectories for neighboring (wind speed) base stations should be similar. The model appears to be fit using gradient simple descent. The authors propose several elaborations, including a nonlinear transition function (based on an MLP) and a reconstruction error term that takes variance into account. However, the model is restricted to using a linear decoder. Experimental results are positive but not convincing.----------------Strengths:--------- The authors target a worthwhile and challenging problem: incorporating the modeling of uncertainty over hidden states with the power of flexible neural net-like models.--------- The idea of representing relationships between hidden states using KL divergence between their (distributions over) corresponding hidden states is clever. Combined with the Gaussian distribution over hidden states, the resulting regularization term is simple and differentiable.--------- This general approach -- focusing on writing down the problem as a neural network-like loss function -- seems robust and flexible and could be combined with other approaches, including variants of variational autoencoders.----------------Weaknesses:--------- The presentation is a muddled, especially the model definition in Sec. 3.3. The authors introduce four variants of their model with different combinations of decoder (with and without variance term) and linear vs. MLP transition function. It appears that the 2,2 variant is generally better but not on all metrics and often by small margins. This makes drawing a solid conclusions difficult: what each component of the loss contributes, whether and how the nonlinear transition function helps and how much, how in practice the model should be applied, etc. I would suggest two improvements to the manuscript: (1) focus on the main 2,2 variant in Sec. 3.3 (with the hypothesis that it should perform best) and make the simpler variants additional ""baselines"" described in a paragraph in Sec. 4.1; (2) perform more thorough experiments with larger data sets to make a stronger case for the superiority of this approach.--------- The authors only allude to learning (with references to gradient descent and ADAM during model description) in this framework. Inference gets its one subsection but only one sentence that ends in an ellipsis (?).--------- It's unclear what is the purpose of introducing the inequality in Eq. 9.--------- Experimental results are not convincing: given the size of the data, the differences vs. the RNN and KF baselines is probably not significant, and these aren't particularly strong baselines (especially if it is in fact an RNN and not an LSTM or GRU).--------- The position of this paper is unclear with respect to variational autoencoders and related models. Recurrent variants of VAEs (e.g., Krishnan, et al., 2015) seem to achieve most of the same goals as far as uncertainty modeling is concerned. It seems like those could easily be extended to model relationships between time series using the simple regularization strategy used here. Same goes for Johnson, et al., 2016 (mentioned in separate question).----------------This is a valuable research direction with some intriguing ideas and interesting preliminary results. I would suggest that the authors restructure this manuscript a bit, striving for clarity of model description similar to the papers cited above and providing greater detail about learning and inference. They also need to perform more thorough experiments and present results that tell a clear story about the strengths and weaknesses of this approach.","The reviews of this paper seem to be very aligned: many of the ideas presented in the paper are interesting, the problem is important, and the results encouraging but preliminary. R2 thought the paper could be improved in terms of clarity and offered several specific suggestions to this end. R2 and R1 mentioned the limitations of the linear decoder; which is not a critical flaw, in my opinion, but as R1 points out, many recent works have explored nonlinear decoders and these could be at least discussed, if not compared. All of the reviewers have worked in this area and expressed high-confidence reviews.    I was surprised that the authors did not provide feedback or revise the paper at least with reference to the clarity/presentation suggestions. It seems this may have had an impact on the perception of the reviewers. I encourage the authors to revise the paper in light of the reviews and re-submit to another venue."
https://openreview.net/forum?id=HJV1zP5xg,"[ Summary ]----------------This paper presents a new modified beam search algorithm that promotes diverse beam candidates. It is a well known problem —with both RNNs and also non-neural language models— that beam search tends to generate beam candidates that are very similar with each other, which can cause two separate but related problems: (1) search error: beam search may not be able to discover a globally optimal solution as they can easily fall out of the beam early on, (2) simple, common, non-diverse output: the resulting output text tends to be generic and common.----------------This paper aims to address the second problem (2) by modifying the search objective function itself so that there is a distinct term that scores diversity among the beam candidates. In other words, the goal of the presented algorithm is not to reduce the search error of the original objective function. In contrast, stack decoding and future cost estimation, common practices in phrase-based SMT, aim to address the search error problem.----------------[ Merits ]----------------I think the Diverse Beam Search (DBS) algorithm proposed by the authors has some merits. It may be useful when we cannot rely on traditional beam search on the original objective function either because the trained model is not strong enough, or because of the search error, or because the objective itself does not align with the goal of the application.----------------[ Weaknesses ]----------------It is however not entirely clear how the proposed method compares against more traditional approaches like stack decoding and future cost estimation, on tasks like machine translation, as the authors compare their algorithm mainly against L&J’s diverse LM models and simple beam search.----------------In fact, modification to the objective function has been applied even in the neural MT context. For example, see equation (14) in page 12 of the following paper:----------------""Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"" (https://arxiv.org/pdf/1609.08144v2.pdf)----------------where the attention coverage term serves a role similar to stack decoding (though unlike stack decoding, the objective term is entirely re-defined, more similarly to DBS proposed in this work), and the length penalty may have an effect that indirectly promotes more informative (thus more likely diverse) responses.----------------Comparison against these existing algorithms would make the proposed work more complete.----------------Also, I have a mixed feeling about computing and reporting only *oracle* BLUE, CIDEr, METEOR, etc. Especially given how these oracle scores are very close to each other, and that developing a high performing ranking has not been addressed in this work (and that doing so must be not all that trivial), I’m somewhat skeptical how much of DBS results make a practical difference.----------------------------------------**** [Update after the author responses] ****----------------The authors addressed some of my concerns by adding a new baseline comparison against Wu et al. 2016. Thus I will raise my score to 6. ","Unfortunately, even after the reviewers adjusted their scores, this paper remains very close to the decision boundary. It presents a thorough empirical evaluation, but the improvements are fairly models. The area chair is also not convinced the idea itself will be very influential and change the way people do decoding since it feels a bit ad hoc (as pointed out by reviewer 1). Overall, this paper did not meet the bar for acceptance at ICLR this year."
https://openreview.net/forum?id=BJ_MGwqlg,"The paper provides a first study of customized precision hardware for large convolutional networks, namely alexnet, vgg and googlenet. It shows that it is possible to achieve larger speed-ups using floating-point precision (up to 7x) when using fewer bits, and better than using fixed-point representations. ----------------The paper also explores predicting custom floating-point precision parameters directly from the neural network activations, avoiding exhaustive search, but i could not follow this part. Only the activations of the last layer are evaluated, but on what data ? On all the validation set ? Why would this be faster than computing the classification accuracy ?----------------The results should be useful for hardware manufacturers, but with a catch. All popular convolutional networks now use batch normalization, while none of the evaluated ones do. It may well be that the conclusions of this study will be completely different on batch normalization networks, and fixed-point representations are best there, but that remains to be seen. It seems like something worth exploring.----------------Overall there is not a great deal of novelty other than being a useful study on numerical precision trade-offs at neural network test time. Training time is also something of interest. There are a lot more researchers trying to train new networks fast than trying to evaluate old ones fast. ----------------I am also no expert in digital logic design, but my educated guess is that this paper is marginally below the acceptance threshold.","The reviewers feel that this is a well written paper on floating and fixed point representations for inference with several state of the art deep learning architectures. At the same time, in order for results to be more convincing, they recommend using 16-bit floats as a more proper baseline for comparison, and to analyze tradeoffs in overall workload speedup, i.e broader system-level issues surrounding the implementation of custom floating point units."
https://openreview.net/forum?id=S1j4RqYxg,"The paper is beyond my expertise. I cannot give any solid review comments regarding the techniques that are better than an educated guess.----------------However, it seems to me that the topic is not very relevant to the focus of ICLR. Also the quality of writing requires improvement, especially literature review and experiment analysis. ","The approach/problem seems interesting, and several reviewers commented on this. However, the experimental evaluation is quite preliminary and the paper would be helped a lot with a connection to a motivating application. All of the reviewers pointed out that the work is not written in the usual in the scope of ICLR papers, and putting these together at this time it makes sense to reject the paper."
https://openreview.net/forum?id=BJ6oOfqge,"This work explores taking advantage of the stochasticity of neural network outputs under randomized augmentation and regularization techniques to provide targets for unlabeled data in a semi-supervised setting. This is accomplished by either applying stochastic augmentation and regularization on a single image multiple times per epoch and encouraging the outputs to be similar (Π-model) or by keeping a weighted average of past epoch outputs and penalizing deviations of current network outputs from this running mean (temporal ensembling). The core argument is that these approaches produce ensemble predictions which are likely more accurate than the current network and are thus good targets for unlabeled data. Both approaches seem to work quite well on semi-supervised tasks and some results show that they are almost unbelievably robust to label noise.----------------The paper is clearly written and provides sufficient details to reproduce these results in addition to providing a public code base. The core idea of the paper is quite interesting and seems to result in higher semi-supervised accuracy than prior work. I also found the attention to and discussion of the effect of different choices of data augmentation to be useful. ----------------I am a little surprised that a standard supervised network can achieve 30% accuracy on SVHN given 90% random training labels. This would only give 19% correctly labeled data (9% by chance + 10% unaltered). I suppose the other 81% would not provide a consistent training signal such that it is possible, but it does seem quite unintuitive. I tried to look through the github for this experiment but it does not seem to be included. ----------------As for the resistance of Π-model and temporal ensembling to this label noise, I find that somewhat more believable given the large weights placed on the consistency constraint for this task. The authors should really include discussion of w(t) in the main paper. Especially because the tremendous difference in w_max in the incorrect label tolerance experiment (10x for Π-model and 100x for temporal ensembling from the standard setting).----------------Could the authors comment towards the scalability for larger problems? For ImageNet, you would need to store around 4.8 gigs for the temporal ensembling method or spend 2x as long training with Π-model.----------------Can the authors discuss sensitivity of this approach to the amount and location of dropout layers in the architecture? ----------------Preliminary rating:--------I think this is a very interesting paper with quality results and clear presentation. ----------------Minor note:--------2nd paragraph of page one 'without neither' -> 'without either'","The reviewers all agree that this is a strong, well-written paper that should be accepted to the conference. The reviewers would like to see the authors extend the analysis to larger data sets and extend the variety of augmentations. Two of the reviewers seem to suggest that some of the experiments seem too good to be true. Please consider releasing code so that others can reproduce experiments and build on this in future work."
https://openreview.net/forum?id=r17RD2oxe,"This paper introduces a hierarchical clustering method using learned CNN features to build 'the tree of life'. The assumption is that the feature similarity indicates the distance in the tree. The authors tried three different ways to construct the tree: 1) approximation central point 2) minimum spanning tree and 3) multidimensional scaling based method. Out of them, MDS works the best. It is a nice application of using deep features. However, I lean toward rejecting the paper because the following reasons:----------------1) All experiments are conducted in very small scale. The experiments include 6 fish species, 11 canine species, 8 vehicle classes. There are no quantitative results, only by visualizing the generated tree versus the wordNet tree. Moreover, the assumption of using wordNet is not quite valid. WordNet is not designed for biology purpose and it might not reflect the true evolutionary relationship between species. --------2) Limited technical novelty. Most parts of the pipeline are standard, e.g. use pretrained model for feature extraction, use previous methods to construct hierarchical clustering. I think the technical contribution of this paper is very limited. ","The reviewers agree that the paper provides a creative idea of using Computer Vision in Biology by building ""the tree of life"". However, they also agree that the paper in its current form is not ready for publication due to limited novelty and unclear impact/application. The authors did not post a rebuttal to address the concerns. The AC agrees with the reviewers', and encourages the authors to improve their manuscript as per reviewers' suggestions, and submit to a future conference."
https://openreview.net/forum?id=SkxKPDv5xl,"Pros:--------The authors are presenting an RNN-based alternative to wavenet, for generating audio a sample at a time.--------RNNs are a natural candidate for this task so this is an interesting alternative. Furthermore the authors claim to make significant improvement in the quality of the produces samples.--------Another novelty here is that they use a quantitative likelihood-based measure to assess them model, in addition to the AB human comparisons used in the wavenet work.----------------Cons:--------The paper is lacking equations that detail the model. This can be remedied in the camera-ready version.--------The paper is lacking detailed explanations of the modeling choices:--------- It's not clear why an MLP is used in the bottom layer instead of (another) RNN.--------- It's not clear why r linear projections are used for up-sampling, instead of feeding the same state to all r samples, or use a more powerful type of transformation. --------As the authors admit, their wavenet implementation is probably not as good as the original one, which makes the comparisons questionable. ----------------Despite the cons and given that more modeling details are provided, I think this paper will be a valuable contribution. ",The reviewers were unanimous in their agreement about accepting this paper.  Pros   - novel formulation that don't require sample by sample prediction  - interesting results    Cons  - lack of details / explanation in the mathematical formulation / motivations for the model.
https://openreview.net/forum?id=HyGTuv9eg,"The paper investigates a simple extension of Gatys et al. CNN-based texture descriptors for image generation. Similar to Gatys et al., the method uses as texture descriptor the empirical intra-channel correlation matrix of the CNN feature response at some layer of a deep network. Differently from Gatys et al., longer range correlations are measured by introducing a shift between the correlated feature responses, which translates in a simple modification of the original architecture.----------------The idea is simple but has interesting effects on the generated textures and can be extended to transformations other than translation. While longer range correlations could be accounted for by considering the response of deeper CNN features in the original method by Gatys et al., the authors show that modelling them explicitly using shallower features is more effective, which is reasonable.----------------An important limitation that this work shares with most of its peers is the lack of a principled quantitative evaluation protocol, such that judging the effectiveness of the approach remains almost entirely a qualitative affair. While this should not be considered a significant drawback of the paper due to the objective difficulty of solving this open issue, nevertheless it is somewhat limiting that no principled evaluation method could be devised and implemented. The authors suggest that, as future work, a possible evaluation method could be based on a classification task -- this is a potentially interesting approach that merits some further investigation.","The program committee appreciates the authors' response to concerns raised in the reviews. While there are some concerns about the computational speed of the approach as well as its advantage over existing methods for some textures, reviewers are excited by the ability of this work to produce structured texture that requires long-range interactions. Overall, the work has contributions that are worth presenting at ICLR."
https://openreview.net/forum?id=SkhU2fcll,"The paper proposed a nice framework leveraging Tucker and Tensor train low-rank tensor factorization to induce parameter sharing for multi-task learning.----------------The framework is nice and appealing. ----------------However, MTL is a very well studied problem and the paper considers simple task for different classification, and it is not clear if we really need ``Deep Learning"" for these simple datasets. A comparison with existing shallow MTL is necessary to show the benefits of the proposed methods (and in particular being deep) on the dataset. The authors ignore them on the basis of speculation and it is not clear if the proposed framework is really superior to simple regularizations like the nuclear norm. The idea of nuclear norm regularization can also be extended to deep learning as gradient descent are popular in all methods. ","The reviews for this paper were quite mixed, with one strong accept and a marginal reject. A fourth reviewer with strong expertise in multi-task learning and deep learning was brought in to read the latest manuscript. Due to time constraints, this fourth review was not entered in the system but communicated through personal communication.     Pros:  - Reviewers in general found the paper clear and well written.  - Multi-task learning in deep models is of interest to the community  - The approach is sensible and the experiments show that it seems to work    Cons:  - Factorization methods have been used extensively in deep learning, so the reviewers may have found the approach incremental  - One reviewer was not convinced that the proposed method would work better than existing multi-task learning methods  - Not all reviewers were convinced by the experiments  - The fourth reviewer found the approach very sensible but was not excited enough to champion the paper    The paper was highly regarded by at least one reviewer and two thought it should be accepted. The PCs also agree that this paper deserves to appear at the conference."
https://openreview.net/forum?id=HkcdHtqlx,"Summary:----------------The authors propose a multi-hop ""gated attention"" model, which models the interactions between query and document representations, for answering cloze-style questions. The document representation is attended to sequentially over multiple-hops using similarity with the query representation (using a dot-product) as the scoring/attention function. --------The proposed method improves upon (CNN, Daily Mail, Who-Did-What datasets) or is comparable to (CBT dataset) the state-of-the-art results.------------------------Pros:----------------1. Nice idea on heirarchical attention for modulating the context (document) representation by the task-specific (query) representation.--------2. The presentation is clear with thorough experimental comparison with the latest results.------------------------Comments:----------------1. The overall system presents a number of architectural elements: (1) attention at multiple layers (multi-hop), (2) query based attention for the context (or gated attention), (3) encoding the query vector at each layer independently.--------It is important to breakdown the gain in performance due to the above factors: the ablation study presented in section 4.4 helps establish the importance of Gated Attention (#2 above). However, it is not clear:----------------  (1) how much multiple-hops of gated-attention contribute to the performance.--------  (2) how important is it to have a specialized query encoder for each layer.----------------Understanding the above better, will help simplify the architecture.------------------------2. The tokens are represented using L(w) and C(w). It is not clear if C(w) is crucial for the performance of the proposed method.--------There is a significant performance drop when C(w) is absent (e.g. in ""GA Reader--""; although there are other changes in ""GA Reader--"" which could affect the performance). Hence, it is not clear how much does the main idea, i.e., gated attention contributes towards the superior performance of the proposed method.","The paper proposes several extensions to popular attention-enhanced models for cloze-style QA. The results are near state of the art, and an ablation study hows that the different features (multiplicative interaction, gating) contribute to the model's performance. The main concern is the limited applicability of the model to other machine reading problems. The authors claim that unpublished results show applicability to other problems, but that is not sufficient defence against these concerns in the context of this paper. That said, the authors have addressed most of the other concerns brought up by the reviewers (e.g. controlling for number of hops) in the revised version. Overall however, the PCs believe that this contribution is not broad and not novel enough; we encourage the authors to resubmit."
https://openreview.net/forum?id=H1Heentlx,"7----------------Summary:--------This paper describes the use of variational autoencoders for multi-view representation learning as an alternative to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE). Two variants of variational autoencoders (which the authors call VCCA and VCCA-private) are investigated. The method’s performances are compared on a synthetic MNIST dataset, the XRMB speech-articulation dataset, and the MIR-Flickr dataset.----------------Review:--------Variational autoencoders are widely used and their performance for multi-view representation learning should be of interest to the ICLR community. The paper is well written and clear. The experiments are thorough. It is interesting that the performance of MVAE and VCCA is quite different given the similarity of their objective functions. I further find the analyses of the effects of dropout and private variables useful.----------------As the authors point out, “VCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA”. It would have been interesting to discuss the differences of a linear variant of VCCA and linear CCA, and to compare it quantitatively. While it might not make sense to use variational inference in the linear case, it would nevertheless help to understand the differences better.----------------The derivations in Equation 3 and Equation 13 seem unnecessarily detailed given that VCCA and VCCA-p are special cases of VAE, only with certain architectural constraints. Perhaps move to the Appendix?----------------In Section 3 the authors claim that “if we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data”. This is not correct, a model which hasn’t learned a thing can have perfectly realistic samples (see Theis et al., 2016). Please remove or revise the sentence.----------------Minor:--------In the equation between Equation 8 and 9, using notation N(x; g(z, theta), I) as in Equation 6 would make it clearer.","The application of VAEs to multiview settings, snd the general problem of learning representations over multiple modalities is of increasing importance. After discussion and reviewing the rebuttals, the reviewers felt that the paper is not yet ready. The application is strong, but that more can be done in terms of praticality, generalisation using other posteriors, and the comparisons made. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings."
https://openreview.net/forum?id=S1AG8zYeg,"The main contribution of this paper is to introduce a new neural network model for sentence ordering. This model is presented as an encoder-decoder, and uses recent development for neural network (pointer networks and order invariant RNNs).----------------The first processing step of the model is to encode each sentence using a word level LSTM recurrent network. An order invariant encoder (based on Vinyals et al. (2015)) is then used to obtain a representation of the set of sentences. The input at each time step of this encoder is a ""bag-of-sentences"", over which attention probabilities are computed. The last hidden representation of the encoder is then used to initialize the decoder. This decoder is a pointer network, and is used to predict the order of the input sentences.----------------The model introduced in this paper is then compared to standard method for sentence ordering, as well as a model with the decoder only. The encoder-decoder approach outperforms the other methods on the task of ordering a given set of sentences. The authors also show that the model learns relatively good sentence representations (e.g. compared to Skip-thought vectors).----------------This paper is relatively well written, and easy to follow. The model described in the paper does not really introduce anything new, but is a natural combination of existing techniques to solve the task of sentence ordering. In particular, it uses an order-invariant encoder to get a representation of the set of sentences, and a pointer network to predict the ordering. While I am not entirely convinced by the task of sentence ordering, this approach seems promising to learn sentence representations.----------------Overall, this is a pretty solid and well executed paper.----------------pros:-------- - sound model for sentence ordering-------- - strong experimental results--------cons:-------- - might be a bit incremental-------- - usefulness of sentence ordering","Sentence ordering is central to a number of NLP tasks (e.g., summarization), and this paper introduces a very sensible application of existing techniques, experimentations is also very solid. However, the technical contributions seems quite limited.    Positive:    -- An important NLP problem (I would disagree here with Reviewer1), a good match between the problem and the methods  -- Solid experiments   -- A well-written paper    Negative:    -- A straightforward application of existing methods    Unfortunately, the latter, I believe, is a problem. Though application papers are welcome, I believe they should go beyond direct applications and provide extra insights to the representation learning community."
https://openreview.net/forum?id=SyQq185lg,"Interesting paper which proposes jointly learning automatic segmentation of words to sub words and their acoustic models.----------------Although the training handles the word segmentation as hidden variable which depends also on the acoustic representations, during the decoding only maximum approximation is used.----------------The authors present nice improvements over character based results, however they did not compare results with word segmentation which does not assume the dependency on acoustic.----------------Obviously, only text based segmentation would result in two (but simpler) independent tasks. In order to extract such segmentation several publicly open tools are available and should be cited. Some of those tools can also exploit the unigram probabilities of the words to perform their segmentations.----------------It looks that the improvements come from the longer acoustical units - longer acoustical constraints which could lead to less confused search -, pointing towards full word models. In another way, less tokens are more probable due to less multiplication of probabilities. As a thought experiment for an extreme case: if all the possible segmentations would be possible (mixture of all word fragments, characters, and full-words), would the proposed model use word fragments at all? (WSJ is a closed vocabulary task). It would be good to show that the sub word model could outperform even a full-word model (no segmentation).--------Your model estimates p(z_t|x,z<t;\theta), but during training both p(z_t|x,y,z<t;\theta) and p(z_t|x,z<t;\theta) are needed. Their connections and calculations, in general Section 2 should be more detailed. The experimental setup should be presented in a replicable way.----------------The authors use one sample of Z during the gradient calculation. Would not it be more consistent then to use the maximum instead of sampling? Or is even getting the maximum is computationally too expensive? Considering the left-to-right approximation, this might be possible and more consistent with your decoding, please address this issue in details.","This work proposes a method for segmenting target generation sequence that is learned as part of the model. Generally all reviewers found this paper novel and interesting.    Pros:  - Quality: The paper is both containing ""with solid theoretically justified"" and ""present(s) nice improvements over character based result"". One reviewer asked for the ""the experimental study could be more solid.""  - Impact: methods like ""BPE"" are now somewhat standard hacks in seq2seq modeling. This type of model could be potentially impactful at disrupting.   - Clarity: Reviewers found the work to be a ""clearly written paper""     Cons:  - Some of the reviewers were not as enthuthiastic about this work compared to other papers. There were several comments asking for further experimental results. However I found that the author's responses clearly explained these away and provided clear justificition for why they were not necessary, already included, or explained by previous results. I feel that this takes care of the major issues, and warrants a small raise in score."
https://openreview.net/forum?id=Bk2TqVcxe,"This paper proposes a relation network (RN) to model relations between input entities such as objects.  The relation network is built in two stages.  First a lower-level structure analyzes a pair of input entities.  All pairs of input entities are fed to this structure.  Next, the output of this lower-level structure is aggregated across all input pairs via a simple sum.  This is used as the input to a higher-level structure.  In the basic version, these two structures are each multi-layer perceptrons (MLPs).----------------Overall, this is an interesting approach to understanding relations among entities.  The core idea is clear and well-motivated -- pooling techniques that induce invariance can be used to learn relations.  The idea builds on pooling structures (e.g. spatial/temporal average/max pooling) to focus on pairwise relations.  The current pairwise approach could potentially be extended to higher-order interactions, modulo scaling issues.----------------Experiments on scene descriptions and images verify the efficacy of relation networks.  The MLP baselines used are incapable of modeling the structured dependencies present in these tasks.  It would be interesting to know if pooling operators (e.g. across-object max pooling in an MLP) or data augmentation via permutation would be effective for training MLPs at these tasks.  Regardless, the model proposed here is novel and effective at handling relations and shows promise for higher-level reasoning tasks.","This paper proposes RNs, relational networks, for representing and reasoning about object relations. Experiments show interesting results such as the capability to disentangling scene descriptions. AR3 praises the idea and the authors for doing this nice and involved analysis. AR1 also liked the paper. Indeed, taking a step back and seeing whether we are able to learn meaningful relations is needed in order to build more complex systems.    However, AR2 raised some important issues: 1) the paper is extremely toy; RN has a very simplistic structure, and is only shown to work on synthetic examples that to some extent fit the assumptions of the RN. 2) there is important literature that addresses relation representations that has been entirely overlooked by the authors. The reviewer implied missed citations from a field that is all about learning object relations. In its current form, the paper does not have a review of related work. The AC does not see any citations in a discussion nor in the final revision. This is a major letdown. The reviewer also mentioned the fact that showing results on real datasets would strengthen the paper, which was also brought up by AR3. This indeed would have added value to the paper, although it is not a deal breaker.    The reviewer AR2 did not engage in discussions, which indeed is not appropriate. The AC does weigh this review less strongly.    Give the above feedback, we recommend this paper for the workshop. The authors are advised to add a related work section with a thorough review over the relevant fields and literature."
https://openreview.net/forum?id=H13F3Pqll,"In this work, the authors propose to use a (perhaps deterministic) retrieval function to replace uniform sampling over the train data in training the discriminator of a GAN.--------Although I like the basic idea, the experiments are very weak.  There are essentially no quantitative results, no real baselines, and only a small amount of not especially convincing qualititative results.   It is honestly hard to review the paper- there isn't any semblance of normal experimental validation.----------------Note:  what is happening with the curves in fig. 6?",All three reviewers point to significant deficiencies (particularly the lack of quantitative evaluation and clarity problems). No response or engagement from the authors. I see no basis for supporting this paper.
https://openreview.net/forum?id=HyAddcLge,"The paper claim that, when supported by a number of backup workers, synchronized-SGD --------actually works better than async-SGD. The paper first analyze the problem of staled updates--------in async-SGDs, and proposed the sync-SGD with backup workers. In the experiments, the --------authors shows the effectiveness of the proposed method in applications to Inception Net--------and PixelCNN.----------------The idea is very simple, but in practice it can be quite useful in industry settings where --------adding some backup workders is not a big problem in cost. Nevertheless, I think the --------proposed solution is quite straightforward to come up with when we assume that --------each worker contains the full dataset and we have budge to add more workers. So, --------under this setting, it seems quite natural to have a better performance with the additional --------backup workers that avoid the staggering worker problem. And, with this assumtion I'm not --------sure if the proposed solution is solving difficult enough problem with novel enough idea. ----------------In the experiments, for fair comparison, I think the Async-SGD should also have a mechanism --------to cut off updates of too much staledness just as the proposed method ignores all the remaining --------updates after having N updates. For example, one can measure the average time spent to --------obtain N updates in sync-SGD setting and use that time as the cut-off threashold in Async-SGD --------so that Async-SGD does not perform so poorly.","Counter to the current wisdom, this work proposes that synchronous training may be advantageous over asynchronous training (provided that ""backup workers"" are available). This is shown empirical and without theoretical results. The contribution is somewhat straightforward and designed for a specific large-scale hardware scenario, but this is often an important bottleneck in the learning process. However, there is some concern about the long-term impact of this work, due to its dependence on the hardware."
https://openreview.net/forum?id=BJ0Ee8cxx,"1. The hierarchical memory is fixed, not learned, and there is no hierarchical in the experimental section, only one layer for softmax layer.--------2. It shows the 10-mips > 100-mips > 1000-mips, does it mean 1-mips is the best one we should adopt?--------3. Approximated k-mips is worse than even original method. Why does it need exact k-mips? It seems the proposed method is not robust.","This paper was reviewed by three experts. While they find interesting ideas in the manuscript, all three point to deficiencies (unconvincing results, etc) and unanimously recommend rejection."
https://openreview.net/forum?id=SJQNqLFgl,"The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems. Essentially, it reads like a review paper about modern CNN architectures. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CIFAR-100, but seem to achieve relatively poor performance on these datasets (Table 1), so their merit is unclear to me.----------------I'm not sure if such a collection of rules extracted from prior work warrants publication as a research paper. It is not a bad idea to try and summarise some of these observations now that CNNs have been the model of choice for computer vision tasks for a few years, and such a summary could be useful for newcomers. However, a lot of it seems to boil down to common sense (e.g. #1, #3, #7, #11). The rest of it might be more suited for an ""introduction to training CNNs"" course / blog post. It also seems to be a bit skewed towards recent work that was fairly incremental (e.g. a lot of attention is given to the flurry of ResNet variants).----------------The paper states that ""it is universal in all convolutional neural networks that the activations are downsampled and the number of channels increased from the input to the final layer"", which is wrong. We already discussed this previously re: my question about design pattern 5, but I think the answer that was given (""the nature of design patterns is that they only apply some of the time"") does not excuse making such sweeping claims. This should probably be removed.----------------""We feel that normalization puts all the layer's input samples on more equal footing, which allows backprop to train more effectively"" (section 3.2, 2nd paragraph) is very vague language that has many possible interpretations and should probably be clarified. It also seems odd to start this sentence with ""we feel"", as this doesn't seem like the kind of thing one should have an opinion about. Such claims should be corroborated by experiments and measurements. There are several other instances of this issue across the paper.----------------The connection between Taylor series and the proposed Taylor Series Networks seems very tenuous and I don't think the name is appropriate. The resulting function is not even a polynomial as all the terms represent different functions -- f(x) + g(x)**2 + h(x)**3 + ... is not a particularly interesting object, it is just a nonlinear function of x.----------------Overall, the paper reads like a collection of thoughts and ideas that are not very well delineated, and the experimental results are unconvincing.",The authors agree with the reviewers that this manuscript is not yet ready.
https://openreview.net/forum?id=r1GKzP5xx,"The paper proposes an extension of weight normalization / normalization propagation to recurrent neural networks. Simple experiments suggest it works well.----------------The contribution is potentially useful to a lot of people, as LSTMs are one of the basic building blocks in our field.----------------The contribution is not extremely novel: the change with respect to weight normalization is minor. The experiments are also not very convincing: Layer normalization is reported to have higher test error as it overfits on their example, but in terms of optimization it seems to work better. Also the authors don't seem to use the data dependent parameter init for weight normalization as proposed in that paper.","Paper proposes a modification of batch normalization. After the revisions the paper is a much better read. However it still needs more diverse experiments to show the success of the method.    Pros:  - interesting idea with interesting analysis of the gradient norms  - claims to need less computation    Cons:  - Experiments are not very convincing and only focus on only a small set of lm tasks.  - The argument for computation gain is not convincing and no real experimental evidence is presented. The case is made that in speech domain, with long sequences this should help, but it is not supported.    With more experimental evidence the paper should be a nice contribution."
https://openreview.net/forum?id=SyJNmVqgg,"Final review: The writers were very responsive and I agree the reviewer2 that their experimental setup is not wrong after all and increased the score by one.  But I still think there is lack of experiments and the results are not conclusive. As a reader I am interested in two things, either getting a new insight and understanding something better, or learn a method for a better performance. This paper falls in the category two, but fails to prove it with more throughout and rigorous experiments. In summary the paper lacks experiments and results are inconclusive and I do not believe the proposed method would be quite useful and hence not a conference level publication. --------------------------The paper proposes to train a policy network along the main network for selecting subset of data during training for achieving faster convergence with less data.----------------Pros:--------It's well written and straightforward to follow--------The algorithm has been explained clearly.----------------Cons:--------Section 2 mentions that the validation accuracy is used as one of the feature vectors for training the NDF. This invalidates the experiments, as the training procedure is using some data from the validation set.----------------Only one dataset has been tested on. Papers such as this one that claim faster convergence rate should be tested on multiple datasets and network architectures to show consistency of results. Especially larger datasets as the proposed methods is going to use less training data at each iteration, it has to be shown in much larger scaler datasets such as Imagenet.----------------As discussed more in detail in the pre-reviews question, if the paper is claiming faster convergence then it has to compare the learning curves with other baselines such Adam. Plain SGD is very unfair comparison as it is almost never used in practice. And this is regardless of what is the black box optimizer they use. The case could be that Adam alone as black box optimizer works as well or better than Adam as black box + NDF.","The authors propose a meta-learning algorithm which uses an RL agent to selectively filter training examples in order to maximise a validation loss. There was a lot of discussion about proper training/validation/test set practices. The author's setup seems to be correct, but the experiments are quite limited. Pro - interesting idea, very relevant for ICLR. Con - insufficient experiments. This is a cool idea which could be a nice workshop contribution."
https://openreview.net/forum?id=Hy3_KuYxg,"I was holding off on this review hoping to get the missing details from the code at https://github.com/alexnowakvilla/DP, but at this time it's still missing. After going over this paper couple of times I'm still missing the details necessary to reproduce the experiments. I think this would be a common problem for readers of this paper, so the paper needs to be improved, perhaps with a toy example going through all the stages of learning.----------------As an example of the difficulty, take section 4.3. It talks about training ""split block"" which is a function that can assign each element to either partition 0 or partition 1. At this point I'm looking at it as a binary classification problem and looking for the parameters, loss, and how this loss is minimized. Instead I get a lot of unexpected information, such as ""we must create artificial targets at every node of the generated tree from the available final target partition"". What are these artificial targets, and how do they relate to the problem of training the splitter? An example that explicitly goes through this construction would help with understanding.","The area chair agrees with the reviewers that this paper is not ready for ICLR yet. There are significant issues with the writing, making it difficult to follow the technical details. Writing aside, the technique seems somewhat limited in its applicability. The authors also promised an updated version, but this version was never delivered (latest version is from Nov 13)."
https://openreview.net/forum?id=BysZhEqee,"The authors pointed out some limitations of existing deep architectures, in particular hard to optimize on small or mid size datasets, and proposed to stack marginal fisher analysis (MFA) to build deep models. The proposed method is tested on several small to mid size datasets and compared with several feature learning methods. The authors also applied some existing techniques in deep learning, such as backprop, denoising and dropout to improve performance. ----------------The new contribution of the paper is limited. MFA has long been proposed. The authors fail to theoretically or empirically justify the stacking of MFAs. The authors did not include any deep architectures that requires backprop over multiple layers in the comparison, which the authors set out to address, instead all the methods compared were learned layer by layer. Will a randomly initialized deep model such as DBN or CNN perform poorly on these datasets? It is also not clear how the authors came up with each particular model architecture and hyper-parameters used in the different datasets. The writing of the paper needs to be significantly improved. A lot of details were omitted, for example, how is dropout applied in the MFA. ",The reviewers unanimously recommend rejection.
https://openreview.net/forum?id=rJ8Je4clg,"This paper proposes an improvement to the q-learning/DQN algorithm using constraint bounds on the q-function, which are implemented using quadratic penalties in practice. The proposed change is simple to implement and remarkably effective, enabling both significantly faster learning and better performance on the suite of Atari games.----------------I have a few suggestions for improving the paper:--------The paper could be improved by including qualitative observations of the learning process with and without the proposed penalties, to better understand the scenarios in which this method is most useful, and to develop a better understanding of its empirical performance.----------------It would also be nice to include zoomed-out versions of the learned curves in Figure 3, as the DQN has yet to converge. Error bars would also be helpful to judge stability over different random seeds.----------------As mentioned in the paper, this method could be combined with D-DQN. It would be interesting to see this combination, to see if the two are complementary. Do you plan to do this in the final version?----------------Also, a couple questions:--------- Do you think the performance of this method would continue to improve after 10M frames?--------- Could the ideas in this paper be extended to methods for continuous control like DDPG or NAF?","The authors have proposed an improvement to q-learning which imposes upper and lower constraint bounds on the q-function, which are implemented using quadratic penalties. This speeds up learning, at least for a limited set of Atari games. The reviewers are strongly divided on the merits of this paper. The authors have worked to be responsive to the reviewers' concerns, however, and the paper has been improved, so I side with the positives."
https://openreview.net/forum?id=HkxAAvcxx,"Paper Summary--------This paper makes two contributions ---------(1) A model for next step prediction, where the inputs and outputs are in the--------space of affine transforms between adjacent frames.--------(2) An evaluation method in which the quality of the generated data is assessed--------by measuring the reduction in performance of another model (such as a--------classifier) when tested on the generated data.----------------The authors show that according to this metric, the proposed model works better--------than other baseline models (including the recent work of Mathieu et al. which--------uses adversarial training).----------------Strengths--------- This paper attempts to solve a major problem in unsupervised learning--------  with videos, which is evaluating them.--------- The results show that using MSE in transform space does prevent the blurring--------  problem to a large extent (which is one of the main aims of this paper).--------- The results show that the generated data reduces the performance of the C3D--------  model on UCF-101 to a much less extent than other baselines.--------- The paper validates the assumption that videos can be approximated to quite a--------  few time steps by a sequence of affine transforms starting from an initial--------frame.----------------Weaknesses--------- The proposed metric makes sense only if we truly just care about the performance--------  of a particular classifier on a given task. This significantly narrows the--------scope of applicability of this metric because arguably, one the important--------reasons for doing unsupervised learning is to come up a representation that is--------widely applicable across a variety of tasks. The proposed metric would not help--------evaluate generative models designed to achieve this objective.----------------- It is possible that one of the generative models being compared will interact--------  with the idiosyncrasies of the chosen classifier in unintended ways.--------Therefore, it would be hard to draw strong conclusions about the relative--------merits of generative models from the results of such experiments. One way to--------ameliorate this would be to use several different classifiers (C3D,--------dual-stream network, other state-of-the-art methods) and show that the ranking--------of different generative models is consistent across the choice of classifier.--------Adding such experiments would help increase certainty in the conclusions drawn--------in this paper.----------------- Using only 4 or 8 input frames sampled at 25fps seems like very little context--------  if we really expect the model to extrapolate the kind of motion seen in--------UCF-101. The idea of working in the space of affine transforms would be much--------more appealing if the model can be shown to really generated non-trivial motion--------patterns. Currently, the motion patterns seem to be almost linear--------extrapolations.----------------- The model that predicts motion does not have access to content at all. It only--------  gets access to previous motion. It seems that this might be a disadvantage--------because the motion predictor cannot use any cues like object boundaries, or--------decide what to do when two motion fields collide (it is probably easier to argue--------about occlusions in content space).----------------Quality/Clarity--------The paper is clearly written and easy to follow. The assumptions are clearly--------specified and validated. Experimental details seem adequate.----------------Originality--------The idea of generating videos by predicting motion has been used previously.--------Several recent papers also use this idea. However the exact implementation in--------this paper is new. The proposed evaluation protocol is novel.----------------Significance--------The proposed evaluation method is an interesting alternative, especially if it--------is extended to include multiple classifiers representative of different--------state-of-the-art approaches. Given how hard it is to evaluate generative models--------of videos, this paper could help start an effort to standardize on a benchmark--------set.----------------Minor comments and suggestions----------------(1) In the caption for Table 1: ``Each column shows the accuracy on the test set--------when taking a different number of input frames as input"" - ``input"" here refers--------to the input to the classifier (Output of the next step prediction model). However--------in the next sentence ``Our approach maps 16 \times 16 patches into 8 \times 8--------with stride 4, and it takes 4 frames at the input"" - here ``input"" refers to--------the input to the next step prediction model. It might be a good idea to rephrase--------these sentences to make the distinction clear.----------------(2) In order to better understand the space of affine transform--------parameters, it might help to include a histogram of these parameters in the--------paper. This can help us see at a glance, what is the typical range of these--------6 parameters, should we expect a lot of outliers, etc.----------------(3) In order to compare transforms A and B, instead of ||A - B||^2, one--------could consider A^{-1}B being close to identity as the metric. Did the authors--------try this ?----------------(4) ""The performance of the classifier on ground truth data is an upper bound on--------the performance of any generative model."" This is not *strictly* true. It is--------possible (though highly unlikely) that a generative model might make the data--------look cleaner, sharper, or highlight some aspect of it which could improve the--------performance of the classifier (even compared to ground truth). This is--------especially true if the the generative model had access to the classifier, it--------could then see what makes the classifier fire and highlight those discriminative--------features in the generated output.----------------Overall--------This paper proposes future prediction in affine transform space. This does--------reduce blurriness and makes the videos look relatively realistic (at least to the--------C3D classifier). However, the paper can be improved by showing that the model can--------predict more non-trivial motion flows and the experiments can be strengthened by--------adding more classifiers besides than C3D.","There were serious concerns raised about the originality of the work in regard to prior methods. The authors' responses to numerous reviewer questions on this matter were also unsatisfactory. In the end, it is difficult to tell what the contribution in regard to the video prediction model is. The proposed evaluation metric is interesting, but also raised serious concerns. Finally, several reviewers raised concerns about the quality of the results."
https://openreview.net/forum?id=rJEgeXFex,"This is a well-conducted and well-written study on the prediction of medication from diagnostic codes. The authors compared GRUs, LSTMs, feed-forward networks and random forests (making a case for why random forests should be used, instead of SVMs) and analysed the predictions and embeddings.----------------The authors also did address the questions of the reviewers.----------------My only negative point is that this work might be more relevant for a data science or medical venue rather than at ICLR.","This paper applies RNNs to predict medications from billing costs. While this paper does not have technical novelty, it is well done and well organized. It demonstrates a creative use of recent models in a very important domain, and I think many people in our community are interested and inspired by well-done applications that branch to socially important domains. Moreover, I think an advantage to accepting it at ICLR is that it gives our ""expert"" stamp of approval -- I see a lot of questionable / badly applied / antiquated machine learning methods in domain conferences, so I think it would be helpful for those domains to have examples of application papers that are considered sound."
https://openreview.net/forum?id=Sy2fzU9gl,"The paper is attacking a classical and very hard problem: non-linear PCA i.e. learning the principal components a.k.a independent factors, a.k.a. orthogonal geodesics in the highly non-linear latent manifold of a given data-set. Moreover, the paper is hoping for these factors to be humanly-interpretable. The problem is so hard that is likely unsolvable in unsupervised fashion for real-life datasets. After all, even we humans are able to zero-in the the type of, say, legs of chairs (Fig 3 in the paper), only after we have identified the object as a chair and have rotated and translated it. The importance of this problem is hard to overstate, so we hope the paper is accepted, on the grounds of asking so fundamental a question.----------------The paper correctly points out the inability of VAE to disentangle important factors even on toy data-sets. This of course has been known for awhile, e.g., Fig 4. on reference [1], from almost 2 years ago. On the back of so much hope and expectation built-up in the introduction, the solution put forward in the paper strikes us as a curious but a hardly useful toy. Slapping a large multiplicative factor on the generative error term of a VAE is not going to work for any real-life datasets. Generation without reconstruction leads to schizophrenia, as every respectable psychiatrist will  testify. Physicists have attacked this problem considerably earlier than DeepMind and their scalable and conceptual solutions have been discussed at length in references [1], section 1.6, section 4, and most of reference [2]. In short, for spatial, color, time symmetries, symmetry statistics are produced by smaller specialized nets like spatial transformers and used to augment the latent variables, to aid the decoder. As demonstrated in reference [2], Figures 3,4, this is indispensable in order to handle distortion, e.g., spatial transformation. Note that this approach is completely unsupervised. ----------------This of course does not handle complex factors like ""type of legs"" but that is a hell of an ambitious goal, and while we hope to be proven wrong, probably way beyond reach of machines and even many humans (for real-life data-sets that is!)----------------By complete luck, the authors may have hit upon something else of fundamental importance: the letter ""beta"" for their multiplicative coefficient is of course reserved in statistical physics for the inverse of the temperature. This requires the separation of generative ""energy"" and ""entropy"" and is  partially addressed in section 2.9 of reference [1]. The correct definition of ""generative temperature"" is not published yet, but used extensively in experiments and can be privately communicated upon request. When worked out correctly, the beta does not multiply the generative error, as in this paper, so it would be very interesting to repeat the toy experiments here with the ""correct"" physical model instead.----------------A question to the authors: creating a new metric (""disentanglement"") as u do is commendable but after hundred years of PCA, ICA, etc , is there no some proxy that can be used instead? Also, could you not for example simply distort the dataset along  different factors and then look for the quality of the reconstructed images (as in the Introduction of reference [2])?----------------[1] https://arxiv.org/pdf/1508.06585v5.pdf--------[2] https://arxiv.org/pdf/1511.02841v3.pdf","This paper proposes a modification of the variational ELBO in encourage 'disentangled' representations, and proposes a measure of disentanglement. The main idea and is presented clearly enough and explored through experiments. This whole area still seems a little bit conceptually confused, but by proposing concrete metrics and methods, this paper makes several original contributions."
https://openreview.net/forum?id=H1zJ-v5xl,"This paper points out that you can take an LSTM and make the gates only a function of the last few inputs  - h_t = f(x_t, x_{t-1}, ...x_{t-T}) - instead of the standard - h_t = f(x_t, h_{t-1}) -, and that if you do so the networks can run faster and work better. You're moving compute from a serial stream to a parallel stream and also making the serial stream more parallel. Unfortunately, this simple, effective and interesting concept is somewhat obscured by confusing language.----------------- I would encourage the authors to improve the explanation of the model. --------- Another improvement might be to explicitly go over some of the big Oh calculations, or give an example of exactly where the speed improvements are coming from. --------- Otherwise the experiments seem adequate and I enjoyed this paper.----------------This could be a high value contribution and become a standard neural network component if it can be replicated and if it turns out to work reliably in multiple settings.","The paper is well written and easy to follow. It has strong connections to other convolutional models such as pixel cnn and bytenet that use convolutional only models with little or no recurrence. The method is shown to be significantly faster than using RNNs, while not losing out on the accuracy.    Pros:  - Fast model  - Good results    Cons:  - Because of its strong relationship to other models, the novelty is incremental."
https://openreview.net/forum?id=r1WUqIceg,"The paper demonstrates a semi-automatic learning rate schedule for the Adam optimizer, called Eve. Originality is somehow limited but the method appears to have a positive effect on neural network training. The paper is well written and illustrations are appropriate.----------------Pros:----------------- probably a more sophisticated scheduling technique than a simple decay term--------- reasonable results on the CIFAR dataset (although with comparably small neural network)----------------Cons:----------------- effect of momentum term would be of interest--------- the Adam reference doesn't point to the conference publications but only to arxiv--------- comparison to Adam not entirely conclusive","The authors propose a simple strategy that uses function values to improve the performance of Adam. There is no theoretical analysis of this variant, but there is an extensive empirical evaluation. A disadvantage of the proposed approach is that it has 3 parameters to tune, but the same parameters are used across experiments. Overall however, the PCs believe that this paper doesn't quite reach the level expected for ICLR and thus cannot be accepted."
https://openreview.net/forum?id=SJttqw5ge,"The paper presents a hierarchical DRL algorithm that solves sequences of navigate-and-act tasks in a 2D maze domain. During training and evaluation, a list of sub-goals represented by text is given to the agent and its goal is to learn to use pre-learned skills in order to solve a list of sub-goals. The authors demonstrate that their method generalizes well to sequences of varying length as well as to new combinations of sub-goals (i.e., if the agent knows how to pick up a diamond and how to visit an apple, it can also visit the diamond). ----------------Overall, the paper is of high technical quality and presents an interesting and non-trivial combination of state-of-the-art advancements in Deep Learning (DL) and Deep Reinforcement Learning (DRL). In particular, the authors presents a DRL agent that is hierarchical in the sense that it can learn skills and plan using them. The skills are learned using a differential temporally extended memory networks with an attention mechanism. The authors also make a novel use of analogy making and parameter prediction. ----------------However, I find it difficult to understand from the paper why the presented problem is interesting and why hadn't it bee solved before. Since the domain being evaluated is a simple 2D maze, using deep networks is not well motivated. Similar problems have been solved using simpler models. In particular, there is a reach literature about planning with skills that had been ignored completely by the authors. Since all of the skills are trained prior to the evaluation of the hierarchical agent, the problem that is being solved is much more similar to supervised learning than reinforcement learning (since when using the pre-trained skills the reward is not particularly delayed). The generalization that is demonstrated seems to be limited to breaking a sentence (describing the subtask) into words (item, location, action). ----------------The paper is difficult to read, it is constantly switching between describing the algorithm and giving technical details. In particular, I find it to be overloaded with details that interfere with the general understanding of the paper. I suggest moving many of the implementation details into the appendix. The paper should be self-contained, please do not assume that the reader is familiar with all the methods that you use and introduce all the relevant notations. ----------------I believe that the paper will benefit from addressing the problems I described above and will make a better contribution to the community in a future conference. ","The paper looks at how natural language instructions can be decomposed into sub-tasks for as-yet-unseen new tasks  hence the zero-shot generalization, which is considered to be the primary challenge to be solved.   The precise problem being solved by the original paper is not clearly expressed in the writing. This left some reviewers asking for comparisons, while the authors note that for the specific nature of the problem being solved, they could not seen any particular methods that is known to be capable of tackling this kind of problem. The complexity of the system and simplicity of the final examples were also found to be contradictory by a subset of the reviewers, although this is again related to the understanding of the problem being solved.    With scores of 3/4/5/7, the ideas in this paper were appreciated by a subset of reviewers.   At the time of the writing of this metareview, the authors have posted a fairly lengthy rebuttal (Jan 18) and significant revisions (Jan 18), with no further responses from reviewers as of yet. However, it is difficult for reviewers to do a full re-evaluation of the paper on such short notice.     While the latest revisions are commendable, it is unfortunately difficult to argue strongly in favor of this paper at present."
https://openreview.net/forum?id=SJx7Jrtgl,"This submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario. First the model has to be adapted (with a Gaussian mixture as a prior) and then the inference has to become consistent (by introducing a regularization term). ----------------A general positive point about this paper is that the model construction is kept simple. Indeed, the assumption about the mixture prior is simple but reasonable, and the inference follows the VAE framework where appropriate with only changing parts that do not conform with the clustering task. These changes are also motivated by some analysis. The presentation is also kept simple: the linking to VAE and related methods is made in an clear and honest way, so that it's easy to follow the paper and understand how everything fits together. ----------------Also, the regularization term is a well motivated and reasonable addition. Given the VAE context in this paper, I'd be interested in seeing a discussion on the variance of the samples in (6).----------------A negative issue of this paper is that all crucial regularizations rely upon ad-hoc parameters that control their strength, namely \eta (eq. (3)) and \alpha (eq. 7). According to the authors adjusting these parameters is crucial, and there seems to be no principled way of adjusting them. It also seems that these two parameters interact, since they both regularize z in different ways. This makes the search space over them grow multiplicatively, since the tuning problem now becomes combinatorial. The authors mention that they tune the trade-off between these two regularizers, but I'd be interested in a comment concerning how this is done (what's the space of parameters to search on). In practical clustering applications, high sensitivity to tuned parameters is undesirable, since one also needs to cross-validate values of K at the same time.----------------I really liked the experiments section. It is not very exhaustive in terms of comparison, but it is very exploratory in terms of demonstrating the model components, strengths and weaknesses. This is much more useful than reporting unintuitive percentage improvements relative to arbitrarily selected baselines and datasets.------------------------Overall, I am a little concerned about the practicality of this approach, given the tuning it requires. However, I am in favor of accepting this paper because it makes its strong and weak points very clear through good explanation and demonstration. Therefore, I expect further research to be built on top of this paper, so that the aforementioned issues will hopefully be alleviated in the future. Finally, the theoretical intuitions given in the paper (and author comments) improve its usefulness as a scientific manuscript.","The reviewers have looked through both the responses, updates, and had much discussion. We agree that the paper is well executed and exposes ideas that are of value and interest. At the same same, the extent to which the methods can be applied in practice and scaled to different types of problems remains unclear, especially in high-dimensional settings. For this reason, we feel that the paper is not yet ready for acceptance in this years conference."
https://openreview.net/forum?id=By5e2L9gl,"This paper presents a novel layer-wise optimization approach for learning CNN with piecewise linear nonlinearities.  The proposed approach trains piecewise linear CNNs layer by layer and reduces the sub-problem into latent structured SVM, which has been well-studied in the literature. In addition, the paper presents improvements of the BCFW algorithm used in the inner procedure. Overall, this paper is interesting. However, unfortunately, the experiment is not convincing. ----------------Pros:----------------- To my best knowledge, the proposed approach is novel, and the authors provide nice theoretical analysis.--------- The paper is well-written and easy to follow. ----------------Cons:----------------- Although the proposed approach can be applied in general structured prediction problem, the experiments only conduct on a simple multi-class classification task. This makes this work less compelling. -------- --------- The test accuracy performance on CIFAR-10 reported in the paper doesn't look right. The accuracy of the best model reported in this paper is 70.2% while existing work often reports 90+%. For example, https://arxiv.org/pdf/1412.6806.pdf showed an accuracy of 91% without data augmentation. Also, CIFAR-10 is a relatively small dataset, ----------------Other comments:----------------- If I understand correctly, BCFW only guarantees monotonically increasing in the dual objective and does not have guarantees on the primal objective. Especially, in practice, the inner optimization process often stops pretty early (i.e., stops when duality gap is still large). Therefore, when putting them together, the CCCP procedure may not monotonically decrease as the inner procedure is only solved approximately. The authors should add this note when they discuss the properties of their algorithm.","The authors present a novel layer-wise optimization approach for learning convolutional neural networks with piecewise linear nonlinearities. The proposed approach trains piecewise linear ConvNets layer by layer, reduces the sub-problem into latent structured SVM.     Reviewers mainly expressed concerns about the experimental results, which the authors have diligently addressed in their revised versions. While the reviewers haven't updated explicitly their reviews, I believe the changes made should have been sufficient for them to do so.    Thus, I recommend this paper be accepted."
https://openreview.net/forum?id=HJ0NvFzxl,"The paper proposes an extension of the Gated Graph Sequence Neural Network by including in this model the ability to produce complex graph transformations. The underlying idea is to propose a method that will be able build/modify a graph-structure as an internal representation for solving a problem, and particularly for solving question-answering problems in this paper. The author proposes 5 different possible differentiable transformations that will be learned on a training set, typically in a supervised fashion where the state of the graph is given at each timestep. A particular occurence of the model is presented that takes a sequence as an input a iteratively update an internal graph state to a final prediction, and which can be applied for solving QA tasks (e.g BaBi) with interesting results.----------------The approach  in this paper is really interesting since the proposed model is able to maintain a representation of its current state as a complex graph, but still keeping the property of being differentiable and thus easily learnable through gradient-descent techniques. It can be seen as a succesfull attempt to mix continuous and symbolic representations. It moreover seems more general that the recent attempts made to add some 'symbolic' stuffs in differentiable models (Memory networks, NTM, etc...) since the shape of the state is not fixed here and can evolve. My main concerns is about the way the model is trained i.e by providing the state of the graph at each timestep which can be done for particular tasks (e.g Babi) only, and cannot be the solution for more complex problems. My other concern is about the whole content of the paper that would perhaps best fit a journal format and not a conference format, making the article still difficult to read due to its density. ","The idea of building a graph-based differentiable memory is very good. The proposed approach is quite complex, but it is likely to lead to future developments and extensions. The paper has been much improved since the original submission. The results could be strengthened, with more comparisons to existing results on bAbI and baselines on the experiments here. Exploring how it performs with less supervision, and different types of supervision, from entirely labeled graphs versus just node labels, would be valuable."
https://openreview.net/forum?id=SJqaCVLxx,"Unfortunately, this paper is very difficult to understand.  The current version of this paper seems improved compared to the initial version, but still far from a finished level.  I'd encourage the authors to keep editing over the language and presentation.----------------I also think it would be good to also try answering some of the following questions very clearly in the paper:----------------- What is the advantage, if any, of the proposed algorithm over SGD?  What is the motivation and goal of the work beyond MNIST benchmarking?----------------- Why are few training examples used?  Is this a scenario in which the system might have an advantage?----------------- Concretely describe the genetic algorithms terminology used in the algorithm descriptions, and what each term means in the context of the convolutional network.----------------- Try to make sure that the method, as described, can be understood by a reader without much prior background on genetic algorithms.----------------- A single experiment on MNIST is too small to adequately describe the algorithm performance.  Consider using a second or third dataset and/or experimental application.----------------Much work is still needed on the paper's writing before it can be understood well enough.  I hope that some of this might be useful in helping to improve. I would encourage the authors to try to find outside readers, preferably fluent in English, to work with on a frequent basis before resubmitting to another venue.","This paper is unfortunately quite unclear and unreadable and nowhere near ready for any conference.  I would advise the authors to 1) restructure their paper to present first some type of context and identify a problem that they are trying to solve, 2) explain what novel method they propose to solve the identified problem and why this method is promising and how it relates to existing methods, 3) explain what their experiments are trying to do and what the results of the experiments are, 4) enlist someone fluent in English to help with writing and re-reading.  A way to do this is to find a set of well-cited papers in the same domain with similar ideas and see how they are structured, then try to follow similar outlines."
https://openreview.net/forum?id=BJC8LF9ex,"The authors propose a RNN-method for time-series classification with missing values, that can make use of potential information in missing values. It is based on a simple linear imputation of missing values with learnable parameters. Furthermore, time-intervals between missing values are computed and used to scale the RNN computation downstream. The authors demonstrate that their method outperforms reasonable baselines on (small to mid-sized) real world datasets. The paper is clearly written.--------IMO the authors propose a reasonable approach for dealing with missing values for their intended application domain, where data is not abundant and requires smallish models. I’m somewhat sceptical if the benefits would carry over to big datasets, where more general, less handcrafted multi-layer RNNs are an option. ","This paper presents a modification of GRU-RNNs to handle missing data explicitly, allowing them to exploit data not missing at random. The method is presented clearly enough, but the reviewers felt that the claims were overreaching. It's also unsatisfying that the method depends on specific modifications of RNN architectures for a particular domain, instead of being a more general approach."
https://openreview.net/forum?id=HJV1zP5xg,"This paper considers the problem of decoding diverge solutions from neural sequence models. It basically adds an additional term to the log-likelihood of standard neural sequence models, and this additional term will encourage the solutions to be diverse. In addition to solve the inference, this paper uses a modified beam search.----------------On the plus side, there is not much work on producing diverse solutions in RNN/LSTM models. This paper represents one of the few works on this topic. And this paper is well-written and easy to follow.----------------The novel of this paper is relatively small. There has been a lot of prior work on producing diverse models in the area of probailistic graphical models. Most of them introduce an additional term in the objective function to encourage diversity. From that perspective, the solution proposed in this paper is not that different from previous work. Of course, one can argue that most previous work focues on probabilistic graphical models, while this paper focuses on RNN/LSTM models. But since RNN/LSTM can be simply interpreted as a probabilistic model, I would consider it a small novelty.----------------The diverse beam search seems to straightforward, i.e. it partitions the beam search space into groups, and does not consider the diversity within group (in order to reduce the search space). To me, this seems to be a simple trick. Note most previous work on diverse solutions in probabilistic graphical models usually involve developing some nontrivial algorithmic solutions, e.g. in order to achieve efficiency. In comparison, the proposed solution in this paper seems to be simplistic for a paper.----------------The experimental results how improvement over previous methods (Li & Jurafsky, 2015, 2016). But it is hard to say how rigorous the comparisons are, since they are based on the authors' own implementation of (Li & Jurasky, 2015, 2016).---------------------------------------update: given that the authors made the code available (I do hope the code will remain publicly available), this has alleviated some of my concerns about the rigor of the experiments. I will raise my rate to 6.","Unfortunately, even after the reviewers adjusted their scores, this paper remains very close to the decision boundary. It presents a thorough empirical evaluation, but the improvements are fairly models. The area chair is also not convinced the idea itself will be very influential and change the way people do decoding since it feels a bit ad hoc (as pointed out by reviewer 1). Overall, this paper did not meet the bar for acceptance at ICLR this year."
https://openreview.net/forum?id=B1gtu5ilg,"This paper proposes a model to learn across different views of objects.  The key insight is to use a triplet loss that encourages two different views of the same object to be closer than an image of a different object.  The approach is evaluated on object instance and category retrieval and compared against baseline CNNs (untrained AlexNet and AlexNet fine-tuned for category classification) using fc7 features with cosine distance.  Furthermore, a comparison against human perception on the ""Tenenbaum objects” is shown.----------------Positives: Leveraging a triplet loss for this problem may have some novelty (although it may be somewhat limited given some concurrent work; see below).  The paper is reasonably written.----------------Negatives: The paper is missing relevant references of related work in this space and should compare against an existing approach.----------------More details:----------------The “image purification” paper is very related to this work:----------------[A] Joint Embeddings of Shapes and Images via CNN Image Purification. Hao Su*, Yangyan Li*, Charles Qi, Noa Fish, Daniel Cohen-Or, Leonidas Guibas. SIGGRAPH Asia 2015.----------------There they learn to map CNN features to (hand-designed) light field descriptors of 3D shapes for view-invariant object retrieval.  If possible, it would be good to compare directly against this approach (e.g., the cross-view retrieval experiment in Table 1 of [A]).  It appears that code and data is available online (http://shapenet.github.io/JointEmbedding/).----------------Somewhat related to the proposed method is recent work on multi-view 3D object retrieval:----------------[B] Multi-View 3D Object Retrieval With Deep Embedding Network. Haiyun Guo, Jinqiao Wang, Yue Gao, Jianqiang Li, and Hanqing Lu. IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 25, NO. 12, DECEMBER 2016.----------------There they developed a triplet loss as well, but for multi-view retrieval (given multiple images of the same object).  Given the similarity of the developed approach, it somewhat limits the novelty of the proposed approach in my view.----------------Also related are approaches that predict a volumetric representation of an input 2D image (going from image to canonical orientation of 3D shape):----------------[C] R. Girdhar, D. Fouhey, M. Rodriguez, A. Gupta. Learning a Predictable and Generative Vector Representation for Objects. ECCV 2016.----------------[D] Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling. Jiajun Wu*, Chengkai Zhang*, Tianfan Xue, William T. Freeman, and Joshua B. Tenenbaum. NIPS 2016.----------------For the experiments, I would like to see a comparison using different feature layers (e.g., conv4, conv5, pool4, pool5) and feature comparison (dot product, Eucllidean).  It has been shown that different layers and feature comparisons perform differently for a given task, e.g.,----------------[E] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. Conference on Computer Vision and Pattern Recognition (CVPR), 2016.----------------[F] Understanding Deep Features with Computer-Generated Imagery. Mathieu Aubry and Bryan C. Russell. IEEE International Conference on Computer Vision (ICCV), 2015.","The paper proposes a model for multi-view learning that uses a triplet loss to encourage different views of the same object to be closer together then the views of two different objects. The technical novelty of the model is somewhat limited, and the reviewers are concerned that experimental evaluations are done exclusively on synthetic data. The connections to human perception appear interesting. Earlier issues with missing references to prior work and comparisons with baseline models appear to have been substantially addressed in revisions of the paper. We strongly encourage the authors to further revise their paper to address the remaining outstanding issues mentioned above."
https://openreview.net/forum?id=ByldLrqlx,The paper presents a technique to combine deep learning style input-output training with search techniques to match the input of a program to the provided output. Orders of magnitude speedup over non-augmented baselines are presented.----------------Summary:--------———--------The proposed search for source code implementations based on a rather small domain specific language (DSL) is compelling but also expected to some degree----------------Quality: The paper is well written.--------Clarity: Some of the derivations and intuitions could be explained in more detail but the main story is well described.--------Originality: The suggested idea to speed up search based techniques using neural nets is perfectly plausible.--------Significance: The experimental setup is restricted to smaller scales but the illustrated improvements are clearly apparent.----------------Details:--------————--------1. The employed test set of 100 programs seems rather small. in addition the authors ensure that the test set programs are semantically disjoint from the training set programs. Could the authors provide additional details about the small size of the test set and how to the disjoint property is enforced?----------------2. The length of the programs is rather small at this point in time. A more detailed ablation regarding the runtime seems useful. The search based procedure is probably still the computationally most expensive part. Hence the neural net provides some additional prior information rather than tackling the real task.,"This is a well written paper that attempts to craft a practical program synthesis approach by training a neural net to predict code attributes and exploit these predicted attributes to efficiently search through DSL constructs (using methods developed in programming languages community). The method is sensible and appears to give consistent speedups over baselines, though its viability for longer programs remains to be seen. There is potential to improve the paper. One of the reviewers would have liked more analysis on what type of programs are difficult and how often the method fails, and how performance depends on training set size etc. The authors should improve the paper based on reviewer comments."
https://openreview.net/forum?id=ByG4hz5le,"1) Summary----------------This paper proposes a video captioning model based on a 3D (space+time) convnet (C3D) encoder and a LSTM decoder. The authors investigate the benefits of using attention mechanisms operating both at the spatio-temporal and layer (feature abstraction) levels.----------------2) Contributions----------------+ Well motivated and implemented attention mechanism to handle the different shapes of C3D feature maps (along space, time, and feature dimensions).--------+ Convincing quantitative and qualitative experiments on three challenging datasets (Youtube2Text, M-VAD, MSR-VTT) showing clearly the benefit of the proposed attention mechanisms.--------+ Interesting comparison of soft vs hard attention showing a slight performance advantage for the (simpler) soft attention mechanism in this case.----------------3) Suggestions for improvement----------------Hypercolumns comparison:--------As mentioned during pre-review questions, it would be interesting to compare to the hypercolumns of https://arxiv.org/abs/1411.5752, as they are an alternative to the proposed attention mechanisms, with the same purpose of leveraging different feature abstraction levels.----------------Minor clarifications in the text and figures as agreed with the authors in our pre-review discussions.----------------4) Conclusion----------------Although the novelty with existing video captioning approaches is limited, the paper is relevant to ICLR, as the proposed simple but efficient implementation and benefits of spatio-temporal + feature abstraction attention are clearly validated in this work.","Reviewers feel the work is well executed and that the model makes sense, but two of the reviewers were not convinced that the proposed method contains enough novelty in light of prior work. The comparison of the soft vs hard attention model variations is perhaps one of the more novel aspects of the work; however, the degree of novelty within these formulations and the insights obtained from their comparison were not perceived as being enough to warrant higher ratings. We would like to invite the authors to submit this paper to the workshop track."
https://openreview.net/forum?id=S1oWlN9ll,"This paper proposed a proximal (quasi-) Newton’s method to learn binary DNN. The main contribution is to combine pre-conditioning with binarization in a proximal framework. It is interesting to have a proximal Newton’s method to interpret the different DNN binarization schemes. This gives a new interpretation of existing approaches. However, the theoretical analysis is not very convincing or useful. The formulated optimization problem (3)-(4) is essentially a mixed integer programming. Even though the paper treats the integer part as a constraint and address it in proximal operators, the constraint set is still discrete and there is no guarantee that the proximal Newton algorithm could converge under practically useful conditions. In practice it is hard to verify the assumption [d_t^t]_k > \beta in Theorem 3.1. This relation could be hard to hold in DNN as the loss surface could be extremely complicated.","It's a simple contribution supported by empirical and theoretical analyses. After some discussion, all reviewers viewed the paper favourably."
https://openreview.net/forum?id=HJtN5K9gx,"This paper investigates deep generative models with multiple stochastic nodes and gives them meaning by semi-supervision. From a methodological point of view, there is nothing fundamentally novel (it is very similar to the semi-supervised work of Kingma et al; although this work has sometimes more than two latent nodes, it is not a complex extension). There is a fairly classical auxiliary variable trick used to make sure the inference network for y is trained over all data points (by supposing y is in fact is a latent variable with an observation \tilde y; the observation is y if y is observed, or uninformative for unobserved y). Alternatively, one can separate the inference used to learn the generative model (which throws out inference over y if it is observed), from an inference used to 'exercise' the model (approximate the complex p(y|x) in the model by a simpler q(y|x) - effectively inferring the target p(y|x) for the data where only x is collected). Results are strong, although on simple datasets. Overall this is a well written, interesting paper, but lacking in terms of methodological advances.----------------Minor:--------- I feel the title is a bit too general for the content of the paper. I personally don't agree with the strong contrast made between deep generative models and graphical models (deep generative models are graphical models, but they are more typically learned and un-interpretable than classical graphical models; and having multiple stochastic variables is not exclusive to graphical models, see DRAW, Deep Kalman Filter, Recurrent VAE, etc.). The word 'structure' is a bit problematic; here, the paper seems more concerned with disentangling and semanticizing the latent representation of a generative model by supervision. It is debatable whether the models themselves have structure.","The paper is a clearly presented application of deep generative models in the semi-supervised setting. After reviewing the discussion and responses, the reviewers felt that the paper while interesting, is limited in scope, and unfortunately not yet ready for inclusion in this year's proceeding."
https://openreview.net/forum?id=HkSOlP9lg,"This paper proposes the RIMs that unrolls variational inference procedure. ----------------The author claims that the novelty lies in the separation of the model and inference procedure, making the MAP inference as an end-to-end approach. The effectiveness is shown in image restoration experiments.----------------While unrolling the inference is not new, the author does raise an interesting perspective towards the `model-free' configuration, where model and inference are not separable and can be learnt jointly. ----------------However I do not quite agree the authors' argument regarding [1] and [2]. Although both [1] and [2] have pre-defined MAP inference problem. It is not necessarily that a separate step is required. In fact, both do not have either a pre-defined prior model or an explicit prior evaluation step as shown in Fig. 1(a). I believe that the implementation of both follows the same procedure as the proposed, that could be explained through Fig. 1(c). That is to say, the whole inference procedure eventually becomes a learnable neural network and the energy is implicitly defined through learning the parameters.  ----------------Moreover, the RNN block architecture (GRU) and non-linearity (tanh) restrict the flexibility and implicitly form the inherent family of variational energy and inference algorithm. This is also similar with [1] and [2].----------------Based on that fact, I have the similar feeling with R1 that the novelty is somewhat limited. Also some discussions should be added in terms of the architecture and nonlinearity that you have chosen. ","This paper presents an approach for learning both a model and inference procedure at the same time using RNNs. The reviewers agree that the idea is interesting, but discussion and considering the responses to the reviews, still felt that more is needed to motivate and contextualise the work, and to make the methods presented more convincing. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings."
https://openreview.net/forum?id=BylSPv9gx,"Summary: The paper presents a technique to convert a dense to sparse network for RNNs. The algorithm will increasingly set more weights to zero during the RNN training phase. This provides a RNN model with less storage requirement and higher inference rate. ----------------Pros:--------Proposes a pruning method that doesn’t need re-training and doesn’t affect the training phase of RNN. The method achieves 90% sparsity, and hence less number of parameters.----------------Cons & Questions:--------Judiciously choosing hyper parameters for different models and different applications wouldn’t be cumbersome? In equation 1, is q the sparsity of final model? Is there a formula to know what is sparsity, number of parameters and accuracy of final model given a set of hyper parameters, before going through training? (Questions answered)----------------In table3, we see a trade-off between number of units and sparsity to achieve better number of parameters or accuracy, or in table5 better speed. Good, but where are the results for GRU sparse big? I mean, accuracy must be similar and still get decent compression rate and speed up. Just like RNN Sparse medium compared with RNN Dense. I can’t see much advantage of pruning and getting high speed-up if you are sacrificing so much accuracy. (Issue fixed with updated data)----------------Why sparsity for table3 and table5 are different? In text: “average sparsity of 88%” but in table5 is 95%? Are the models used in table3 different from table5? (Issue fixed)","Here is a summary of the reviews:    Strengths  Experiments are done on state-of-the-art networks, on a real speech recognition problem (R3, R1)  Networks themselves are of a very large size (R3)  Computational gains are substantial (R3, R4)  Paper is clear (R1)    Weaknesses  Experiments are all done on a private dataset (R3)  No comparison to other pruning approaches (e.g. Han et al.) (R3); AC notes that reviewers added new results which compare to an existing pruning method  No comparison to distillation techniques (R1)  Paper doesn't present much novelty in terms of ideas (R3)    The AC encouraged feedback from the reviewers following author rebuttal and paper improvements. Reviewers stated that the improvements made to the paper made it publishable but was still closer to the threshold. R1 who had originally rated the paper 3: a ""clear reject"" updated the score to 6 (just above acceptance).    Considering the reviews and discussions, the AC thinks that this paper is a poster accept. There are no serious flaws, the improvements made to the paper during the discussion paper have satisfied the reviewers, and this is an important topic with practical benefits; evaluated on a real large-scale problem."
https://openreview.net/forum?id=rJY3vK9eg,"This paper proposes to use RNN and reinforcement learning for solving combinatorial optimization problems. The use of pointer network is interesting as it enables generalization to arbitrary input size. The proposed method also ""fintunes"" on test examples with active search to achieve better performance.----------------The proposed method is theoretically interesting as it shows that RNN and RL can be combined to solve combinatorial optimization problems and achieve comparable performance to traditional heuristic based algorithms.----------------However, the lack of complexity comparison against baselines make it impossible to tell whether the proposed method has any practical value. The matter is further complicated by the fact that the proposed method runs on GPU while baselines run on CPU: it is hard to even come up with a meaningful unit of complexity. Money spent on hardware and electricity per instance may be a viable option.----------------Further more, the performance comparisons should be taken with a grain of salt as traditional heuristic based algorithms can often give better performance if allowed more computation, which is not controlled across algorithms.","This was one of the more controversial submissions to this area, and there was extensive discussion over the merits and contributions of the work. The paper also benefitted from ICLRs open review system as additional researchers chimed in on the paper and the authors resubmitted a draft. The authors did a great job responding and updating the work and responding to criticisms. In the end though, even after these consideration, none of the reviewers strongly supported the work and all of them expressed some reservations.     Pros:  - All agree that the work is extremely clear, going as far as saying the work is ""very well written"" and ""easy to understand"".   - Generally there was a predisposition to support the work for its originality particularly due to its ""methodological contributions"", and even going so far as a saying it would generally be a natural accept.    Cons:  - There was a very uncommonly strong backlash to the claims made by the paper, particularly the first draft, but even upon revisions. One reviewer even saying this was an ""excellent example of hype-generation far before having state-of-the-art results"" and that it was ""doing a disservice to our community since it builds up an expectation that the field cannot live up to"" . This does not seem to be an isolated reviewer, but a general feeling across the reviews. Another faulting ""the toy-ness of the evaluation metric"" and the way the comparisons were carried out.  - A related concern was a feeling that the body of work in operations research was not fully taken account in this work, noting ""operations research literature is replete with a large number of benchmark problems that have become standard to compare solver quality"". The authors did fix some of these issues, but not to the point that any reviewer stood up for the work."
https://openreview.net/forum?id=rJ0JwFcex,"This paper sets out to tackle the program synthesis problem: given a set of input/output pairs discover the program that generated them. The authors propose a bipartite model, with one component that is a generative model of tree-structured programs and the other component an input/output pair encoder for conditioning. They consider applying many variants of this basic model to a FlashFill DSL. The experiments explore a practical dataset and achieve fine numbers. The range of models considered, carefulness of the exposition, and basic experimental setup make this a valuable paper for an important area of research. I have a few questions, which I think would strengthen the paper, but think it's worth accepting as is.----------------Questions/Comments:----------------- The dataset is a good choice, because it is simple and easy to understand. What is the effect of the ""rule based strategy"" for computing well formed input strings?----------------- Clarify what ""backtracking search"" is? I assume it is the same as trying to generate the latent function? ----------------- In general describing the accuracy as you increase the sample size could be summarize simply by reporting the log-probability of the latent function. Perhaps it's worth reporting that? Not sure if I missed something.","There is a bit of a spread in the reviewer scores and unfortunately it wasn't possible to entice the reviewers to participate in a discussion. The area chair therefore discounts the late review of reviewer3, who seems to have had a misunderstanding that was successfully rebutted by the authors. The other reviewers are supportive of the paper."
https://openreview.net/forum?id=HksioDcxl,"This paper proposes an RNN-based model for recommendation which takes into account temporal dynamics in user ratings and reviews. Interestingly, the model infers time-dependant user/item vectors by applying an RNN to previous histories. Those vectors are used to predict ratings, in a similar fashion to standard matrix factorization methods, and also bias a conditional RNN language model for reviews. The paper is well written and the architectural choices make sense. The main shortcomings of the paper are in the experiments:----------------1) The full model (rating+text) is only applied to one and relatively small dataset. Applying the model on multiple datasets with more data, e.g. Amazon reviews dataset (https://snap.stanford.edu/data/web-Amazon.html) would be more convincing.----------------2) While modelling order in review text seems like the right choice, previous papers (e.g. Almahairi et al. 2015) have shown that for rating prediction, modelling order in reviews might not be useful. A comparison with a similar model, but with bag-of-words reviews model would be nice in order to show the importance of the RNN-based review model, especially given previous literature.----------------Finally, this paper is an application paper applying well-established deep learning techniques, and I do not feel the paper offers new insights which the ICLR community in general would benefit from. This is not to undermine the importance of this paper, but I would like the authors to comment on why they think ICLR is a good avenue for their work.","The paper has some nice ideas, but requires a bit to push it over the acceptance threshold. I agree with the reviewers who ask for comparisons with other rating-review methods, and that other evaluation metrics more appropriate to the recommendation tasks should be reported. More analysis of the model, and the factors that contribute to its performance, would greatly improve the paper."
https://openreview.net/forum?id=H1GEvHcee,"The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM.  A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported.---------------- It's interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work.-------- 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed.-------- 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly.-------- 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.","This paper identifies a joint distribution for an RBM variant based on leaky-ReLU activations. It also proposes using a sequence of distributions, both as an annealing-based training method, or to estimate log(Z) with AIS.    This paper was borderline. While there is an interesting idea, the reviewers weren't generally as excited by the work as for other papers. One limitation is that unit-variance Gaussian RBMs aren't a strong baseline for comparison, although that is the focus of the main body of the paper. An update to the paper has results for binary visibles in an appendix, although I'm not sure exactly what was done, if the results are comparable, or if there is a large cost of projection here."
https://openreview.net/forum?id=B1gtu5ilg,"On one hand this paper is fairly standard in that it uses deep metric learning with a Siamese architecture. On the other, the connections to human perception involving persistence is quite interesting. I'm not an expert in human vision, but the comparison in general and the induced hierarchical groupings in particular seem like something that should interest people in this community. The experimental suite is ok but I was disappointed that it is 100% synthetic. The authors could have used a minimally viable real dataset such as ALOI http://aloi.science.uva.nl .----------------In summary, the mechanics of the proposed approach are not new, but the findings about the transfer of similarity judgement to novel object classes are interesting.","The paper proposes a model for multi-view learning that uses a triplet loss to encourage different views of the same object to be closer together then the views of two different objects. The technical novelty of the model is somewhat limited, and the reviewers are concerned that experimental evaluations are done exclusively on synthetic data. The connections to human perception appear interesting. Earlier issues with missing references to prior work and comparisons with baseline models appear to have been substantially addressed in revisions of the paper. We strongly encourage the authors to further revise their paper to address the remaining outstanding issues mentioned above."
https://openreview.net/forum?id=BkUDvt5gg,"This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. ----------------The approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations.----------------You are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting.----------------The submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption.----------------Prior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced.----------------What do you mean by transition ""scalars""?----------------I do not repeat further comments here, which were already given in the pre-review period.----------------Minor comments:-------- - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly--------   End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check)-------- - Sec. 2.3: Bayse -> Bayes-------- - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper).-------- - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural)-------- - Sec. 2.4, first line: threholding -> thresholding (spell check..)-------- - Figure 4: mention the corpus used here - dev?","Without revisions to this paper or a rebuttal from the authors, it is hard to accept this paper. The main contribution of the paper is removing the blank from CTC to create a somewhat different criterion, but this is not particularly novel (see, for example, http://www.isca-speech.org/archive/Interspeech_2016/pdfs/1446.PDF). None of the reviewers were willing to argue for acceptance in the deliberation phase, so unfortunately the recommendation must be to reject this paper."
https://openreview.net/forum?id=SJDaqqveg,"The paper presents a nice application of actor-critic method for conditional sequence prediction. The critic is trained conditional to target sequence output, while the actor is conditional on input sequence. The paper presents a number of interesting design decisions in order to tackle non-standard RL problem with actor-critic (conditional sequence generation with sequence-level reward function, large action space, reward at final step) and shows encouraging results for applying RL in sequence prediction. ----------------The interaction of actor and critic is an interesting aspect of this paper. Each has different pieces of information (input sequence, target output sequence), and effectively the actor gets target label information only through greedy optimization of the critic. Letting the critic having access to information only available at train time is interesting and may be applicable to other applications that tie RL with supervised learning. Pre-review discussion on Q-learning vs actor-critic has been good, and indeed I agree that making the critic having access to structured output label may be quite useful. ----------------The pros include reasonable improvement over prior attempts at using RL to fine-tune sequence models. One possible con is that the actor-critic is likely more unstable than simpler prior methods, thus requiring a number of tricks to alleviate, and it would be nice to see discussion on stability and hyper-parameter sensitivity. Another possible con is that this is an application paper, but it explores a non-traditional approach in a widely applicable field. ","Originality, Significance:    The paper proposes to use an actor-critic RL methods to train sequence to sequence tasks, as applied to NLP tasks.   A key aspect is that the critic network can be conditioned on the ground-truth output of the actor network. The idea is quite novel.    Quality, Clarity:    The major concern is with respect to the evaluation, specifically with respect to baseline results for other state-of-the-art methods for BLEU-scored translation tasks. The final rebuttal tackles many of these issues, although the reviewers have not commented on the rebuttal.     I believe that the method demonstrates significant promise, given the results that can be achieved with quite a different approach from previous work."
https://openreview.net/forum?id=r1LXit5ee,"This is a very interesting and timely paper, with multiple contributions. --------- it proposes a setup for dealing with combinatorial perception and action-spaces that generalizes to an arbitrary number of units and opponent units,--------- it establishes some deep RL baseline results on a collection of Starcraft subdomains,--------- it proposes a new algorithm that is a hybrid between black-box optimization REINFORCE, and which facilitates consistent exploration.------------------------As mentioned in an earlier comment, I don’t see why the “gradient of the average cumulative reward” is a reasonable choice, as compared to just the average reward? This over-weights late rewards at the expense of early ones, so the updates are not matching the measured objective. The authors state that they “did not observe a large difference in preliminary experiments” -- so if that is the case, then why not choose the correct objective?----------------DPQ is characterized incorrectly: despite its name, it does not “collect traces by following deterministic policies”, instead it follows a stochastic behavior policy and learns off-policy about the deterministic policy. Please revise this. ----------------Gradient-free optimization is also characterized incorrectly (“it only scales to few parameters”), recent work has shown that this can be overcome (e.g. the TORCS paper by Koutnik et al, 2013). This also suggests that your “preliminary experiments with direct exploration in the parameter space” may not have followed best practices in neuroevolution? Did you try out some of the recent variants of NEAT for example, which have been applied to similar domains in the past?----------------On the specific results, I’m wondering about the DQN transfer from m15v16 to m5v5, obtaining the best win rate of 96% in transfer, despite only reaching 13% (the worst) on the training domain? Is this a typo, or how can you explain that?","The paper presents an approach to structured exploration in StarCraft micromanagement policies (essentially small tasks in the game). From an application standpoint, this is a notable advance, since the authors are tackling a challenging domain and in doing so develop a novel exploration algorithm that seems to work quite well here.    The main downside of the paper is that the proposed algorithm does seem fairly ad-hoc: certain approximations in the algorithm, like ignoring the argmax term in computing the backprop, are not really justified except in that it ""seems to work well in practice"" (a quote from the paper), so it's really unclear whether these represent general techniques or just a method that happens to work well on the target domain for poorly understood reasons.    Despite this, however, I think the strength of the application is sufficient here. Deep learning has been alternatively pushed forward by more algorithmic/mathematical advances and more applied advances, with many of the major breakthroughs coming from seemingly ad-hoc strategies applied to challenging problems. This paper falls in that later category: the ZO algorithm may or may not lead to something slightly more disciplined in the future, but for now the compelling results on StarCraft are I believe enough to warrant accepting the paper.    Pros:  + Substantial performance improvement (over basic techniques like Q-learning) on a challenging task  + Nice intuitive justification of a new exploration approach    Cons:  - Proposed algorithm seems rather ad-hoc, making some (admittedly not theoretically justified) approximations simply because they seem to work well in practice"
https://openreview.net/forum?id=H1GEvHcee,"Based on previous work such as the stepped sigmoid units and ReLU hidden units for discriminatively trained supervised models, a Leaky-ReLU model is proposed for generative learning.----------------Pro: what is interesting is that unlike the traditional way of first defining an energy function and then deriving the conditional distributions, this paper propose the forms of the conditional first and then derive the energy function. However this general formulation is not novel to this paper, but was generalized to exponential family GLMs earlier.----------------Con: --------Because of the focus on specifying the conditionals, the joint pdf and the marginal p(v) becomes complicated and hard to compute.----------------On the experiments, it would been nice to see a RBM with binary visbles and leaky ReLu for hiddens. This would demonstrate the superiority of the leaky ReLU hidden units. In addition, there are more results on binary MNIST modeling with which the authors can compare the results to. While the authors is correct that the annealing distribution is no longer Gaussian, perhaps CD-25 or (Faast) PCD experiments can be run to compare agains the baseline RBM trained using (Fast) PCD.----------------This paper is interesting as it combines new hidden function with the easiness of annealed AIS sampling, However, the baseline comparisons to Stepped Sigmoid Units (Nair &Hinton) or other models like the spike-and-slab RBMs (and others) are missing, without those comparisons, it is hard to tell whether leaky ReLU RBMs are better even in continuous visible domain.","This paper identifies a joint distribution for an RBM variant based on leaky-ReLU activations. It also proposes using a sequence of distributions, both as an annealing-based training method, or to estimate log(Z) with AIS.    This paper was borderline. While there is an interesting idea, the reviewers weren't generally as excited by the work as for other papers. One limitation is that unit-variance Gaussian RBMs aren't a strong baseline for comparison, although that is the focus of the main body of the paper. An update to the paper has results for binary visibles in an appendix, although I'm not sure exactly what was done, if the results are comparable, or if there is a large cost of projection here."
https://openreview.net/forum?id=Skn9Shcxe,"The paper describes an alternative view on hierarchical feature representations in deep neural networks. The viewpoint of refining representations is well motivated and is in agreement with the success of recent model structures like ResNets.----------------Pros:----------------- Good motivation for the effectiveness of ResNets and Highway networks--------- Convincing analysis and evaluation----------------Cons:----------------- The effect of this finding of the interpretation of batch-normalization is only captured briefly but seems to be significant--------- Explanation of findings in (Zeiler & Fergus (2014)) using UIE viewpoint missing----------------Remarks:----------------- Missing word in line 223: ""that it *is* valid""","The paper provides interesting new interpretations of highway and residual networks, which should be of great interest to the community."
https://openreview.net/forum?id=r1aGWUqgg,"The paper presents a method to learn a low-dimensional state representations from raw obervation for multi-task setting. In contrast to classic multi-task learning setting where a joint representation is usually learned by exploring the transferable information among different tasks, the method aims to identify individual task and solve them separately. To this end, the authors extend the learning with robotic priors approach by extending the loss function with additional term for task coherence, i.e., a task only changes representation between training episodes. The method has been evaluated on two tasks, multi-task slot-car racing and mobile navigation to prove its efficacy.----------------there were several unclear issues:----------------1. The first question is that if the method is only appealing on the scenario like the slot-car racing, otherwise it should be benchmarked with mutli-task learning. While the author made the argument in the related work, the proposed method is orthogonal to multi-task learning they did admit both explore shared knowledge between tasks. What's the advantage and disadvantage for the proposed method for general mutiple task setting, in particular over the multi-task learning?--------The reply of the authors was not fully satisfactory. The argument did not support the lack of comparison to multi-task joint-learning. It seems they don't plan to include any comparison neither. I think it's important for the fundamental motivation for the work, without such comparison, the method seems to be purely an alternative to multi-task joint-learning without any(or much) practical advantage.----------------2.Following up to the previous question, please clarify the results on the mobile navigation scenario. It's not clear how the plot on the right indicates MT-LRP identifies all tasks as the author claimed and and seems very weak to support the method, in particular compared to the multi-slot car-racing driving experiment, there is too little results to make sound argument (almost no comparison to alternative methods, i.e. no baseline method, is that true for the problem).--------The explanation of the authors did provide more details and more explicit information. ----------------3. The proposed gated neural network architecture seems to be a soft gated structure(correct me if I am wrong), a possible baseline would be a hard gated unit, how would this affect the conclusion. This is particularly interesting as the authors reflect on the constraint that the representation should stay consistent during the training.--------The author simply stated again what they did for the modeling without counter the comparison to hard-gating, but it's probably less an issue compared to Question 1.----------------In summary, while there are remaining concerns about lacking comparisons, the is a weak tendency towards accepting the submission.","The authors propose to explore an important problem -- learning state representations for reinforcement learning. However, the experimental evaluation raised a number of concerns among the reviewers. The method is tested on only a single relatively easy domain, with some arbitrarily choices justified in unconvincing ways (e.g. computational limitations as a reason to not fix aliasing). The evaluation also compares to extremely weak baselines. In the end, there is insufficient evidence to convincingly show that the method actually works, and since the contribution is entirely empirical (as noted by the reviewers in regard to some arbitrary parameter choices), the unconvincing evaluation makes this method unsuitable for publication at this time. The authors would be encouraged to evaluate the method rigorously on a wide range of realistic tasks."
https://openreview.net/forum?id=HJOZBvcel,"This paper proposes a new method for learning graphical models. Combined with a neural network architecture, some sparse edge structure is estimated via sampling methods. In introduction, the authors say that a problem in graphical lasso is model selection. However, the proposed method still implicitly includes model selection. In the proposed method,  is a sparse prior, and should include some hyper-parameters. How do you tune the hyper-parameters? Is this tuning an equivalent problem to model section? Therefore, I do not understand real advantage of this method over previous methods. What is the advantage of the proposed method?----------------Another concern is that this paper is unorganized. In Algorithm 1, first, G_i and \Sigma_i are sampled, and then x_j is sampled from N(0, \Sigma). Here, what is \Sigma? Is it different from \Sigma_i? Furthermore, how do you construct (Y_i, \hat{\Sigma}_i) from (G_i, X_i )? Finally, I have a simple question: Where is input data X (not sampled data) is used in Algorithm 1?----------------What is the definition of the receptive field in Proposition 2 and Proposition 3?","The authors provide a modern twist to the classical problem of graphical model selection. Traditionally, the sparsity priors to encourage selection of specific structures is hand-engineered. Instead, the authors propose using a neural network to train for these priors. Since graphical models are useful in the small-sample regime, using neural networks directly on the training data is not effective. Instead, the authors propose generating data based on the desired graph structures to train the neural network.     While this is a nice idea, the paper is not clear and convincing enough to be accepted to the conference, and instead, recommend it to the workshop track."
https://openreview.net/forum?id=Sks3zF9eg,An interesting study of using Sine as activation function showing successful training of models using Sine. However the scope of tasks this is applied to is a bit too limited to be convincing. Maybe showing good results on more important tasks in addition to current toy tasks would make a stronger case?,The reviewers unanimously recommend rejecting the paper.
https://openreview.net/forum?id=rkYmiD9lg,"The paper describes how to use a tensor factorization method called Tensor Train for modeling the interactions between features for supervised classification tasks. Tensor Train approximates tensors of any dimensions using low rank products of matrices. The rank is used as a parameter for controlling the complexity of the approximation. Experiments are performed on different datasets for binary classification problems.----------------The core of the paper consists in demonstrating how the TT formalism developed by one of the authors could be adapted for modeling interactions between features. Another contribution is a gradient algorithm that exploits the geometrical structure of the factorization. These ideas are probably new in Machine learning. The algorithm itself is of reasonable complexity for the inference and could probably be adapted to large size problems although this is not the case for the experiments here.----------------The experimental section is not well structured, it is incomplete and could be improved. We miss a description of the datasets characteristics. The performance on the different datasets are not provided. Each dataset has been used for illustrating one aspect of the model, but you could also provide classification performance and a comparison with baselines for all the experiments. The experiments on the 2 UCI datasets show optimization performance on the training set, you could provide the same curves on test sets to show how the algorithm generalizes. The comparison to other approaches (section 8.4) is only performed on artificial data, which are designed with interacting features and are not representative of diverse situations. The same holds for the role of Dropout. The comparison on the Movielens dataset is incomplete. Besides, all the tests are performed on small size problems.----------------Overall, there are original contributions which could be worth a publication. The experiments are incomplete and not conclusive. A more detailed comparison with competing methods, like Factorization Machines, could also improve the paper. ","Modeling nonlinear interactions between variables by imposing Tensor-train structure on parameter matrices is certainly an elegant and novel idea. All reviewers acknowledge this to be the case. But they are also in agreement that the experimental section of this paper is somewhat weak relative to the rest of the paper, making this paper borderline. A revised version of this paper that takes reviewer feedback into account is invited to the workshop track."
https://openreview.net/forum?id=SJk01vogl,"Comments: ----------------""This contrasts to adversarial attacks on classifiers, where any inspection of the inputs will reveal the original bytes the adversary supplied,--------which often have telltale noise""----------------Is this really true?  If it were the case, wouldn't it imply that training ""against"" adversarial examples should easily make a classifier robust to adversarial examples (if they all have a telltale noise)?  ----------------Pros: --------  -The question of whether adversarial examples exist in generative models, and indeed how the definition of ""adversarial example"" carries over is an interesting one.  --------  -Finding that a certain type of generative model *doesn't have* adversarial examples would be a really significant result, finding that generative models have adversarial examples would also be a worth negative result.  --------  -The adversarial examples in figures 5 and 6 seem convincing, though they seem much more overt and noisy than the adversarial examples on MNIST shown in (Szegedy 2014).  Is this because it's actually harder to find adversarial examples in these types of generative models?  ----------------Issues: --------  -Paper is significantly over length at 13 pages.  --------  -The beginning of the paper should more clearly motivate its purpose.  --------  -Paper has ""generative models"" in the title but as far as I can tell the whole paper is concerned with autoencoder-type models.  This is kind of annoying because if someone wanted to consider adversarial attacks on, say, autoregressive models, they might be unreasonably burdened by having to explain how they're distinct from a paper called ""adversarial examples for generative models"".  --------   -I think that the introduction contains too much background information - it could be tightened.  ","The main idea in this paper is interesting, of considering various forms of adversarial examples in generative models. The paper has been considerably revised since the original submission. The results showing the susceptibility of generative models to attacks are interesting, but the demonstrations need to be more solid and on other datasets to be convincing."
https://openreview.net/forum?id=BJrFC6ceg,"It is refreshing that OpenAI has taken the time to resurrect classic heuristics like down-sampling and dropout into PixelCNN. Some sort of AR technique like PixelCNN probably holds the missing keys needed to eventually have decent originally-created images from CIFAR10 or other real-life data-sets. So any engineering streamlining, as in this paper, is welcome to the general public, especially when helping to avoid expensive clusters of GPUs, only DeepMind can afford. In this sense, OpenAI is fulfilling its mission and we are all very grateful! Thus the paper is a welcome addition and we hope it finds its way into what appears to be an experimental CS conference anyway. ----------------On a more conceptual level, our hope is that OpenAI, with so talented a team, will stop competing in these contrived contests to improve by basis points certain obscure log-likelihoods and instead focus on the bigger picture problems. Why for example, almost two years later, the class-conditional CIFAR10 samples, as on the left of Figure 4 in this paper (column 8 - class of horses), are still inferior to, say, the samples on Figure 4 of reference [2]? Forgive the pun, but aren't we beating a dead horse here? Yes, resolution and sharpness have improved, due to good engineering but nobody in the general public will take these samples seriously! Despite the claims put forward by some on the DeepMind team, PixelCNN is not a ""fully-generative"" neural net (as rigorously defined in section 3 of reference [1]), but merely a perturbative net, in the vain of the Boltzmann machine. After the Procrustean experience of lost decades on Boltzmann machines, the time perhaps has come to think more about the fundamentals and less about the heuristics?----------------[1] https://arxiv.org/pdf/1508.06585v5.pdf--------[2] https://arxiv.org/pdf/1511.02841v3.pdf"," The authors acknowledge that the ideas in the paper are incremental, but assert these are not-trivial improvements upon prior work on pixel CNNs. The reviewers tended to agree with this characterization. The paper presents SOTA pixel likelihood results on CIFAR-10. This work is also coupled with a high quality source code contribution, which also appears to have already been well received by the github community. Reviewer 1 made the point that in terms of raw novelty this work is probably a little below the bar for an oral presentation. A public reviewer rated this paper as a strong accept. Given the statistics, quality and originality of the other papers in my AC batch I recommend poster."
https://openreview.net/forum?id=SJiFvr9el,"Overview: This work seems very promising, but I believe it should be compared with more baselines, and more precisely described and explained, from a signal processing point of view.----------------Pros:--------New descriptor--------Fast implementation----------------Cons:--------a) Lack of rigor--------b) Too long accordingly to the content--------c) The computational gain of the algorithm is not clear--------d) The work is not compared with its most obvious baseline: a scattering transform----------------I will detail each cons.----------------a) Section 1:--------The author  motivates the use of scattering transform because it defines a contraction of the space that relies on geometric features.--------"" The nonlinearity used in the scattering network is the complex modulus which is piecewise linear.""--------A real modulus is piecewise linear. A complex modulus has a shape of bell when interpreting C as R^2. Could you clarify?--------\Omega is not introduced.----------------Could you give a precise reference (page+paper) of this claim: “Higher order nonlinearity refers to |x|^2 instead of |x| as it is usually done in the scattering network.” ?----------------Section 2:--------The motivation of the non-linearity is not clear. First, this non-linearity might potentially increase a lot the variance of your architecture since it depends on higher moments(up to 4). I think a fair analysis would be to compute numerically the normalized variance (e.g. divided by the averaged l^2 norm), as a sanity check. Besides, one should prove that the energy is decreasing. It is not possible to argue that this architecture is similar to a scattering transform which has precise mathematical foundations and those results are required, since the setting is different.----------------Permutation is not a relevant variability.----------------The notion of sparsity during the whole paper sometimes refers to the number of 0 value, either the l^1 norm. Mathematically, a small value, even 10^-1000 is still a non 0 value.----------------Did you compute the graph of the figure 4 on the bird dataset? You might use a ratio instead for clarity. ----------------The wavelet that is defined is not a morlet wavelet ( https://en.wikipedia.org/wiki/Morlet_wavelet ). It is close to be a gabor wavelet, and actually it has not a 0 averaging. The measure of the ""sparsity of the filters"" is extremely unclear, is it the ratio between the support of the filter and its size? A good criteria might be for instance to understand the amount of the energy that has been neglected.----------------Besides, a filter with compact support has a bad localisation property. However, this topic is not  reached in the paper. For instance, Cauchy wavelets are not used in many applications.(However in mathematical proofs, they often are)----------------In Subsection 3.4 you write that V^(l)(x)=Sum_{j>l} S^(j)(x), but also that you do compute only the 2 first order coefficients because they can be neglected. Besides, you specifically write that adding the variance coefficients improve the representation, whereas they can be obtained as linear combination of S.----------------You claim you apply only one FFT, whereas you apply several FFTs.----------------b) From ""One of the great..."" to ""iof the--------input."" section 3.1, the text is not clear. The motivation is that a convolution in space is slower that performing a convolution in the Fourier domain. This whole paragraph can be summarised in few sentences.--------The section 3.4 is long and corresponds to implementation details. Maybe it could be removed.----------------c) The table 2 seems to indicate that the generation of the filters is one of the bottleneck of your software. Is this really true?----------------One of the main claim of the paper is that sparse filters and staying in Fourier domain speed up the computations. Let us compare the computation of the first order scattering coefficients at scale j with this setting. One has to compare the complexity to compute sum |x*psi_j| and sum |x*psi_j|^2----------------A downsampling is always performed with wavelets, yet it bears approximation. In a Fourier resolution implementation, one adjust the degree of approximation and speed of computation with the over sampling parameters. Assume a FFT of size N costs C*N*log N, then,----------------Computing \hat x costs in both case C*N*log N. --------ST: Then, the signal is multiplied with the fourier transform of the filter, which has a cost of N. In a fourier multi resolution implementation, one periodises the signal by a factor j, such that its size is N/2^j. Then, the FFT has cost C*N/2^j*log(N/2^j), and modulus has cost say N. Then, one applied the averaging that has complexity N/2^j.----------------Here: The signal is multiplied with the fourier transport of the filter that has a support of N/2^j. Then, you convolve it with itself, that has thanks to the padding and the FFT a cost of C*N/2^(j-1)*log(N/2^(j-1))+N/2^(j-1). And you take the 0 frequency.----------------I might be wrong, since this it not my work to do those calculus, but if you claim that your implementation is theoretically faster, you need to prove it, since I do not know any papers where scattering transform claims to be fastly implemented. Here, one sees that the difference is not that significant. Please correct me if I did a mistake.----------------d) It is essential to compare your work with the representation of a scattering transform. First, in term of speed of computation, with a fair implementation (e.g. not MATLAB) and secondly in term of accuracy on the dataset you did use: it is a natural baseline.","This paper proposes a to use squared modulus nonlinearities within convolutional architectures. Because point-wise squaring can be written as a convolution in the Fourier domain, when doing all the operations in the Fourier this architecture becomes 'dual': convolutions become pointwise operations, and pointwise square-nonlinearities become convolutions.   The authors study this architecture in the context of scattering transforms and produce a complexity analysis that exploits the previous property, along with preliminary numerical experiments.     All reviewers agreed that, while this is an interesting paper with potentially useful outcomes, its exposition and current experimental section are insufficient. The AC agrees with this assessment, and therefore recommends rejection.   I agree that the main unanswered question and a 'show-stopper' is the lack of comparisons with its most immediate baseline, scattering using complex modulus, both in terms of accuracy and computational complexity."
https://openreview.net/forum?id=SkhU2fcll,"The paper proposed a tensor factorization approach for MTL to learn cross task structures for better generalization. The presentation is clean and clear and experimental justification is convincing. ----------------As mentioned, including discussions on the effect of model size vs. performance would be useful in the final version and also work in other fields related to this. ----------------One question on Sec. 3.3, to build the DMTRL, one DNN per-task is trained with the same architecture. How important is this pretraining? Would random initialization also work here? If the data is unbalanced, namely, some classes have very few examples, how would that affect the model?","The reviews for this paper were quite mixed, with one strong accept and a marginal reject. A fourth reviewer with strong expertise in multi-task learning and deep learning was brought in to read the latest manuscript. Due to time constraints, this fourth review was not entered in the system but communicated through personal communication.     Pros:  - Reviewers in general found the paper clear and well written.  - Multi-task learning in deep models is of interest to the community  - The approach is sensible and the experiments show that it seems to work    Cons:  - Factorization methods have been used extensively in deep learning, so the reviewers may have found the approach incremental  - One reviewer was not convinced that the proposed method would work better than existing multi-task learning methods  - Not all reviewers were convinced by the experiments  - The fourth reviewer found the approach very sensible but was not excited enough to champion the paper    The paper was highly regarded by at least one reviewer and two thought it should be accepted. The PCs also agree that this paper deserves to appear at the conference."
https://openreview.net/forum?id=B1YfAfcgl,"__Note__: An earlier version of the review (almost identical to the present one) for an earlier version of the paper (available on arXiV) can be found here: http://www.shortscience.org/paper?bibtexKey=journals/corr/1611.01838#csaba--------The only change concerns relation to previous work.----------------__Problem__: The problem considered is to derive an improved version of SGD for training neural networks (or minimize empirical loss) by modifying the loss optimized to that the solution found is more likely to end up in the vicinity of a minimum where the loss changes slowly (""flat minima"" as in the paper of Hochreiter and Schmidhuber from 1997). ----------------__Motivation__: It is hypothetised that flat minima ""generalize"" better. ----------------__Algorithmic approach__: Let  be the (empirical) loss to be minimized. Modify this to -------- with some  tunable parameters. For --------, the term -------- becomes very small, so effectively the second term is close to a constant times the integral of  over a ball centered at  and having a radius of . This is a smoothened version of , hence one expects that by making this term more important then the first term, a procedure minimizing -------- will be more likely to end up at a flat minima of . Since the gradient is somewhat complicated, an MCMC algorithm is proposed (""stochastic gradient Langevin dynamics"" from Welling and Teh, 2011).----------------__Results__: There is a theoretical result that quantifies the increased smoothness of --------, which is connected to stability and ultimately to generalization through citing a result of Hardt et al. (2015). Empirical results show better validation error on two datasets: MNIST and CIFAR-10 (the respective networks are LeNet and All-CNN-C). The improvement is in terms of reaching the same validation error as with an ""original SGD"" but with fewer ""epochs"".----------------__Soundness, significance__: The proof of the __theoretical result__ relies on an arbitrary assumption that there exists some  such that no eigenvalue of the hessian of  lies in the set  (the reason for the assumption is because otherwise a uniform improvement cannot be shown). For  the improvement of the smoothness (first and second order) is a factor of . The proof uses Laplace's method and is more a sketch than a rigorous proof (error terms are dropped; it would be good to make this clear in the statement of the result).----------------In the experiments the modified procedure did not consistently reach a smaller validation error. The authors did not present running times, hence it is unclear whether the procedure's increased computation cost is offset by the faster convergence. ----------------__Evaluation__: It is puzzling why a simpler smoothing, e.g., -------- (with  being the density of a centered probability distribution) is not considered. The authors note that something ""like this"" may be infeasible in ""deep neural networks"" (the note is somewhat vague). However, under mild conditions, --------, hence, for , -------- is an unbiased estimate of --------, whose calculation is as cheap as that of vanilla SGD. Also, how much the smoothness of  changes when using this approach is quite well understood.----------------__Related work__: It is also strange that the specific modification that appears in this paper was proposed by others (Baldassi et al.), whom this paper also cites, but without giving these authors credit for introducing local entropy as a smoothing technique. ","This paper presents both an analysis of neural net optimization landscapes, and an optimization algorithm that encourages movement in directions of high entropy. The motivation is based on intuitions from physics.    Pros   - the main idea is well-motivated, from a non-standard perspective.   - There are lots of side-experiments supporting the claims for the motivation.  Cons   - The propose method is very complicated, and it sounds like good performance depended on adding and annealing yet another hyperparameter, referred to as 'scoping'.   - The motivating intuition has been around for a long time in different forms. In particular, the proposed method is very closely related to stochastic variational inference, or MCMC methods. Appendix C makes it clear that the two methods aren't identical, but I wish the authors had simply run SVI with their proposed modification, instead of appearing to re-invent the idea of maximizing local volume from scratch. The intuition that good generalization comes from regions of high volume is also exactly what Bayes rule says.    In summary, while there is improvement for the paper, the idea is well-motivated and the experimental results are sound."
https://openreview.net/forum?id=HJTzHtqee,"This paper proposes a compare-aggregate framework that performs word-level matching followed by aggregation with convolutional neural networks. It compares six different comparison functions and evaluates them on four datasets. Extensive experimental results have been reported and compared against various published baselines.----------------The paper is well written overall.----------------A few detailed comments:--------* page 4, line5: including a some -> including some--------* What's the benefit of the preprocessing and attention step? Can you provide the results without it?--------* Figure 2 is hard to read, esp. when on printed hard copy. Please enhance the quality.","This paper proposes a framework whereby, to an attention mechanism relating one text segment to another piecewise, an aggregation mechanism is added to yield an architecture matching words of one segment to another. Different vector comparison operations are explored in this framework. The reviewers were satisfied that this work is relevant, timely, clearly presented, and that the empirical validation was sound. "
https://openreview.net/forum?id=rJJRDvcex,"Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.------------------------Paper summary: this work proposes to use RNNs inside a convolutional network architecture as a complementary mechanism to propagate spatial information across the image. Promising results on classification and semantic labeling are reported.------------------------Review summary:--------The text is clear, the idea well describe, the experiments seem well constructed and do not overclaim. Overall it is not a earth shattering paper, but a good piece of incremental science.------------------------Pros:--------* Clear description--------* Well built experiments--------* Simple yet effective idea--------* No overclaiming--------* Detailed comparison with related work architectures------------------------Cons:--------* Idea somewhat incremental (e.g. can be seen as derivative from Bell 2016)--------* Results are good, but do not improve over state of the art------------------------Quality: the ideas are sound, experiments well built and analysed.------------------------Clarity: easy to read, and mostly clear (but some relevant details left out, see comments below)------------------------Originality: minor, this is a different combination of ideas well known.------------------------Significance: seems like a good step forward in our quest to learn good practices to build neural networks for task X (here semantic labelling and classification).------------------------Specific comments:--------* Section 2.2 “we introduction more nonlinearities (through the convolutional layers and ...”. Convolutional layers are linear operators.--------* Section 2.2, why exactly RNN cannot have pooling operators ? I do not see what would impede it.--------* Section 3 “into the computational block”, which block ? Seems like a typo, please rephrase.--------* Figure 2b and 2c not present ? Please fix figure or references to it.--------* Maybe add a short description of GRU in the appendix, for completeness ?--------* Section 5.1, last sentence. Not sure what is meant. The convolutions + relu and pooling in ResNet do provide non-linearities “between layers” too. Please clarify--------* Section 5.2.1 (and appendix A), how is the learning rate increased and decreased ? Manually ? This is an important detail that should be made explicit. Is the learning rate schedule the same in all experiments of each table ? If there is a human in the loop, what is the variance in results between “two human schedulers” ?--------* Section 5.2.1, last sentence; “we certainly have  a strong baseline”; the Pascal VOC12 for competition 6 reports 85.4 mIoU as best known results. So no, 64.4 is not “certainly strong”. Please tune down the statement.--------* Section 5.2.3 Modules -> modules--------* The results ignore any mention of increased memory usage or computation cost. This is not a small detail. Please add a discussion on the topic.--------* Section 6 “adding multi-scale spatial” -> “adding spatial” (there is nothing inherently “multi” in the RNN)--------* Section 6 Furthermoe -> Furthermore--------* Appendix C, redundant with Figure 5 ?","This paper proposes a hybrid architecture that combines traditional CNN layers with separable RNN layers that quickly increase the receptive field of intermediate features. The paper demonstrates experiments on CIfar-10 and semantic segmentation, both by fine-tuning pretrained CNN models and by training them from scratch, showing numerical improvements.     The reviewers agreed that this paper presents a sound modification of standard CNN architectures in a clear, well-presented manner. They also highlighted the clear improvement of the manuscipt between the first draft and subsequent revisions.   However, they also agreed that the novelty of the approach is limited compared to recent works (e.g. Bell'16), despite acknowledging the multiple technical differences between the approaches. Another source of concern is the lack of large-scale experiments on imagenet, which would potentially elucidate the role of the proposed interleaved lrnn modules in the performance boost and demonstrate its usefulness to other tasks.     Based on these remarks, the AC recommends rejection of the current manuscript, and encourages the authors to resubmit the work once the large-scale experiments are completed."
https://openreview.net/forum?id=Hku9NK5lx,"The method proposes to compress the weight matrices of deep networks using a new density-diversity penalty together with a computing trick (sorting weights) to make computation affordable and a strategy of tying weights.----------------This density-diversity penalty consists of an added cost corresponding to the l2-norm of the weights (density) and the l1-norm of all the pairwise differences in a layer.----------------Regularly, the most frequent value in the weight matrix is set to zero to encourage sparsity.----------------As weights collapse to the same values with the diversity penalty, they are tied together and then updated using the averaged gradient.----------------The training process then alternates between training with 1. the density-diversity penalty and untied weights, and 2. training without this penalty but with tied weights.----------------The experiments on two datasets (MNIST for vision and TIMIT for speech) shows that the method achieves very good compression rates without loss of performance.------------------------The paper is presented very clearly,  presents very interesting ideas and seems to be state of the art for compression. The approach opens many new avenues of research and the strategy of weight-tying may be of great interest outside of the compression domain to learn regularities in data.----------------The result tables are a bit confusing unfortunately.----------------minor issues:----------------p1--------english mistake: “while networks *that* consist of convolutional layers”.----------------p6-p7--------Table 1,2,3 are confusing. Compared to the baseline (DC), your method (DP) seems to perform worse:-------- In Table 1 overall, Table 2 overall FC, Table 3 overall, DP is less sparse and more diverse than the DC baseline. This would suggest a worse compression rate for DP and is inconsistent with the text which says they should be similar or better.--------I assume the sparsity value is inverted and that you in fact report the number of non-modal values as a fraction of the total.",The reviewers unanimously recommended accepting the paper.
https://openreview.net/forum?id=rJxDkvqee,"Pros:--------  Interesting training criterion.--------Cons:--------  Missing proper ASR technique based baselines.----------------Comments:--------  The dataset is quite small.--------  ROC curves for detection, and more measurements, e.g. EER would probably be helpful besides AP.--------  More detailed analysis of the results would be necessary, e.g. precision of words seen during training compared to the detection--------  performance of out-of-vocabulary words.--------  It would be interesting to show scatter plots for embedding vs. orthographic distances.","The paper explores a model that performs joint embedding of acoustic sequences and character sequences. The reviewers agree the paper is well-written, the proposed loss function is interesting, and the experimental evaluation is sufficient. Having said that, there are also concerns that the proxy tasks used in the experiments are somewhat artificial."
https://openreview.net/forum?id=SkgSXUKxx,"Summary--------===--------This paper extends and analyzes the gradient regularizer of Hariharan and--------Girshick 2016. In that paper a regularizer was proposed which penalizes--------gradient magnitudes and it was shown to aid low-shot learning performance.--------This work shows that the previous regularizer is equivalent to a direct penalty--------on the magnitude of feature values weighted differently per example.----------------The analysis goes to to provide two examples where a feature penalty--------favors a better representation. The first example addresses the XOR--------problem, constructing a network where a feature penalty encourages--------a representation where XOR is linearly separable.--------The second example analyzes a 2 layer linear network, showing improved stability--------of a 2nd order optimizer when the feature penalty is added.--------One last bit of analysis shows how this regularizer can be interpreted as--------a Gaussian prior on both features and weights. Since the prior can be--------interpreted as having a soft whitening effect, the feature regularizer--------is like a soft version of Batch Normalization.----------------Experiments show small improvements on a synthetic XOR test set.--------On the Omniglot dataset feature regularization is better than most baselines,--------but is worse than Moment Matching Networks. An experiment on ImageNet similar--------to Hariharan and Girshick 2016 also shows effective low-shot learning.------------------------Strengths--------===----------------* The core proposal is a simple modification of Hariharan and Girshick 2016.----------------* The idea of feature regularization is analyzed from multiple angles--------both theoretically and empirically.----------------* The connection with Batch Normalization could have broader impact.------------------------Weaknesses--------===----------------* In section 2 the gradient regularizer of Hariharan and Girshick is introduced.--------While introducing the concept, some concern is expressed about the motivation:--------""And it is not very clear why small gradients on every sample produces--------good generalization experimentally."" This seems to be the central issue to me.--------The paper details some related analysis, it does not offer a clear answer to--------this problem.------------------------* The purpose and generality of section 2.1 is not clear.----------------The analysis provides a specific case (XOR with a non-standard architecture)--------where feature regularization intuitively helps learn a better representation.--------However, the intended take-away is not clear.----------------The take-away may be that since a feature penalty helps in this case it--------should help in other cases. I am hesitant to buy that argument because of the--------specific architecture used in this section. The result seems to rely on the--------choice of an x^2 non-linearity, which is not often encountered in recent neural--------net literature.----------------The point might also be to highlight the difference between a weight--------penalty and a feature penalty because the two seem to encourage--------different values of b in this case. However, there is no comparison to--------a weight penalty on b in section 2.1.------------------------* As far as I can tell, eq. 3 depends on either assuming an L2 or cross-entropy--------loss. A more general class of losses for which eq. 3 holds is not provided. This--------should be made clear before eq. 3 is presented.------------------------* The Omniglot and ImageNet experiments are performed with Batch Normalization,--------yet the paper points out that feature regularization may be similar in effect--------to Batch Norm. Since the ResNet CNN baseline includes Batch Norm and there are--------clear improvements over that baseline, the proposed regularizer has a clear--------additional positive effect. However, results should be provided without--------Batch Norm so a 1-1 comparison between the two methods can be performed.------------------------* The ImageNet experiment should be more like Hariharan and Girshick.--------In particular, the same split of classes should be used (provided in--------the appendix) and performance should be measured using n > 1 novel examples--------per class (using k nearest neighbors).------------------------Minor:----------------* A brief comparison to Matching Networks is provided in section 3.2, but the--------performance of Matching Networks should also be reported in Table 1.----------------* From the approach section: ""Intuitively when close to convergence, about half--------of the data-cases recommend to update a parameter to go left, while--------the other half recommend to go right.""----------------Could the intuition be clarified? There are many directions in high--------dimensional space and many ways to divide them into two groups.----------------* Is the SGM penalty of Hariharan and Girshick implemented for this paper--------or using their code? Either is acceptable, but clarification would be appreciated.----------------* Should the first equal sign in eq. 13 be proportional to, not equal to?----------------* The work is dense in nature, but I think the presentation could be improved.--------In particular, more detailed derivations could be provided in an appendix--------and some details could be removed from the main version in order to increase--------focus on the results (e.g., the derviation in section 2.2.1).------------------------Overall Evaluation--------===----------------This paper provides an interesting set of analyses, but their value is not clear.--------There is no clear reason why a gradient or feature regularizer should improve--------low-shot learning performance. Despite that, experiments support that conclusion,--------the analysis is interesting by itself, and the analysis may help lead to a--------clearer explanation.----------------The work is a somewhat novel extension and analysis of Hariharan and Girshick 2016.--------Some points are not completely clear, as mentioned above.","The paper extends a regularizer on the gradients recently proposed by Hariharan and Girshick. I agree with the reviewers that while the analysis is interesting, it is unclear why this particular regularizer is especially relevant for low-shot learning. And the experimental validation is not strong enough to warrant acceptance."
https://openreview.net/forum?id=Hyanrrqlg,"The paper presents a method to reduce the memory footprint of a neural network at some increase in the computation cost. This paper is a generalization of HashedNets by Chen et al. (ICML'15) where parameters of a neural network are mapped into smaller memory arrays using some hash functions with possible collisions. Instead of training the original parameters, given a hash function, the elements of the compressed memory arrays are trained using back-propagation. In this paper, some new tricks are proposed including: (1) the compression space is shared among the layers of the neural network (2) multiple hash functions are used to reduce the effects of collisions (3) a small network is used to combine the elements retrieved from multiple hash tables into a single parameter. Fig 1 of the paper describes the gist of the approach vs. HashedNets.----------------On the positive side,--------+ The proposed ideas are novel and seem useful.--------+ Some theoretical justification is presented to describe why using multiple hash functions is a good idea.--------+ All of the experiments suggest that the proposed MFH approach outperforms HashedNets.--------On the negative side,--------- The computation cost seems worse than HashedNets and is not discussed.--------- Immediate practical implication of the paper is not clear given that alternative pruning strategies perform better and should be faster at inference.----------------That said, I believe this paper benefits the deep learning community as it sheds light into ways to share parameters across layers of a neural network potentially leading to more interesting follow-ups. I recommend accept, while asking the authors to address the comments below.----------------More comments:--------- Please discuss the computation cost for both HashedNets and MFH for both fully connected and convolutional layers.--------- Are the experiments only run once for each configuration? Please run multiple times and report average / standard error.--------- For completeness, please add U1 results to Table 1.--------- In Table 1, U4-G3 is listed twice with two different numbers.--------- Some sentences are not grammatically correct. Please improve the writing.","This paper presents some interesting and potentially useful ideas, but multiple reviewers point out that the main appeal of the paper's contributions would be in potential follow-up work and that the paper as-is does not present a compelling use case for the novel ideas. For that reason, the recommendation is to reject the paper. I would encourage the authors to reframe and improve the paper and extend it to cover the more interesting possible empirical investigations brought up in the discussion. Unfortunately, the paper is not sufficiently ground breaking to be a good fit for the workshop track."
https://openreview.net/forum?id=SkxKPDv5xl,"The paper introduces SampleRNN, a hierarchical recurrent neural network model of raw audio. The model is trained end-to-end and evaluated using log-likelihood and by human judgement of unconditional samples, on three different datasets covering speech and music. This evaluation shows the proposed model to compare favourably to the baselines.----------------It is shown that the subsequence length used for truncated BPTT affects performance significantly, but interestingly, a subsequence length of 512 samples (~32 ms) is sufficient to get good results, even though the features of the data that are modelled span much longer timescales. This is an interesting and somewhat unintuitive result that I think warrants a bit more discussion.----------------The authors have attempted to reimplement WaveNet, an alternative model of raw audio that is fully convolutional. They were unable to reproduce the exact model architecture from the original paper, but have attempted to build an instance of the model with a receptive field of about 250ms that could be trained in a reasonable time using their computational resources, which is commendable.----------------The architecture of the Wavenet model is described in detail, but it found it challenging to find the same details for the proposed SampleRNN architecture (e.g. which value of ""r"" is used for the different tiers, how many units per layer, ...). I think a comparison in terms of computational cost, training time and number of parameters would also be very informative.----------------Surprisingly, Table 1 shows a vanilla RNN (LSTM) substantially outperforming this model in terms of likelihood, which is quite suspicious as LSTMs tend to have effective receptive fields of a few hundred timesteps at best. One would expect the much larger receptive field of the Wavenet model to be reflected in the likelihood scores to some extent. Similarly, Figure 3 shows the vanilla RNN outperforming the Wavenet reimplementation in human evaluation on the Blizzard dataset. This raises questions about the implementation of the latter. Some discussion about this result and whether the authors expected it or not would be very welcome.----------------Table 1 and Figure 4 also show the 2-tier SampleRNN outperforming the 3-tier model in terms of likelihood and human rating respectively, which is very counterintuitive as one would expect longer-range temporal correlations to be even more relevant for music than for speech. This is not discussed at all, I think it would be useful to comment on why this could be happening.----------------Overall, this an interesting attempt to tackle modelling very long sequences with long-range temporal correlations and the results are quite convincing, even if the same can't always be said of the comparison with the baselines. It would be interesting to see how the model performs for conditional generation, seeing as it can be more easily be objectively compared to models like Wavenet in that domain.--------------------------------Other remarks:----------------- upsampling the output of the models is done with r separate linear projections. This choice of upsampling method is not motivated. Why not just use linear interpolation or nearest neighbour upsampling? What is the advantage of learning this operation? Don't the r linear projections end up learning largely the same thing, give or take some noise?----------------- The third paragraph of Section 2.1.1 indicates that 8-bit linear PCM was used. This is in contrast to Wavenet, for which an 8-bit mu-law encoding was used, and this supposedly improves the audio fidelity of the samples. Did you try this as well?----------------- Section 2.1 mentions the discretisation of the input and the use of a softmax to model this discretised input, without any reference to prior work that made the same observation. A reference is given in 2.1.1, but it should probably be moved up a bit to avoid giving the impression that this is a novel observation.",The reviewers were unanimous in their agreement about accepting this paper.  Pros   - novel formulation that don't require sample by sample prediction  - interesting results    Cons  - lack of details / explanation in the mathematical formulation / motivations for the model.
https://openreview.net/forum?id=BJAFbaolg,"This paper trains a generative model which transforms noise into model samples by a gradual denoising process. It is similar to a generative model based on diffusion. Unlike the diffusion approach:--------- It uses only a small number of denoising steps, and is thus far more computationally efficient.--------- Rather than consisting of a reverse trajectory, the conditional chain for the approximate posterior jumps to q(z(0) | x), and then runs in the same direction as the generative model. This allows the inference chain to behave like a perturbation around the generative model, that pulls it towards the data. (This also seems somewhat related to ladder networks.)--------- There is no tractable variational bound on the log likelihood.----------------I liked the idea, and found the visual sample quality given a short chain impressive. The inpainting results were particularly nice, since one shot inpainting is not possible under most generative modeling frameworks. It would be much more convincing to have a log likelihood comparison that doesn't depend on Parzen likelihoods.----------------Detailed comments follow:----------------Sec. 2:--------""theta(0) the"" -> ""theta(0) be the""--------""theta(t) the"" -> ""theta(t) be the""--------""what we will be using"" -> ""which we will be doing""--------I like that you infer q(z^0|x), and then run inference in the same order as the generative chain. This reminds me slightly of ladder networks.--------""q*. Having learned"" -> ""q*. [paragraph break] Having learned""--------Sec 3.3:--------""learn to inverse"" -> ""learn to reverse""--------Sec. 4:--------""For each experiments"" -> ""For each experiment""--------How sensitive are your results to infusion rate?--------Sec. 5: ""appears to provide more accurate models"" I don't think you showed this -- there's no direct comparison to the Sohl-Dickstein paper.--------Fig 4. -- neat!","Infusion training is a new, somewhat heuristic, procedure for training deep generative models. It's an interesting novel idea and a good paper, which has also been improved after the authors have been responsive to reviewer feedback."
https://openreview.net/forum?id=Sywh5KYex,"The paper presents a layer architecture where a single parameter is used to  gate the output response of layer to amplify or suppress it. It is shown that such an architecture can ease optimization of a deep network as it is easy to learn identity mappings in layers helping in better gradient propagation to lower layers (better supervision). ----------------Using an introduced SDI metric it shown that gated residual networks can most easily learn identity mappings compared to other architectures. ----------------Although good theoretical reasoning is presented the observed experimental evidence of learned k values does not seem to strongly support the theory given that learned  k values are mostly very small and not varying much across layers. Also, experimental validation of the approach is not quite strong in terms of reported performances and number of large scale experiments.","Although this was a borderline paper, the reviewers ultimately concluded that, given how easy it would be for a practitioner to independently devise the methodological trick of the paper, the paper did not demonstrate that the idea was sufficiently useful to merit acceptance."
https://openreview.net/forum?id=BkbY4psgg,"This paper improves significantly upon the original NPI work, showing that the model generalizes far better when trained on traces in recursive form. The authors show better sample complexity and generalization results for addition and bubblesort programs, and add two new and more interesting tasks - topological sort and quicksort (added based on reviewer discussion). Furthermore, they actually *prove* that the algorithms learned by the model generalize perfectly, which to my knowledge is the first time this has been done in neural program induction.",Has the source code for this paper been released by the authors?
https://openreview.net/forum?id=Bk67W4Yxl,"The paper tests various feedforward network architectures for supervised training to predict a human’s next move, given a board position. It trains on human play data taken from KGX, augmenting the data by considering all 8 rotations/reflections of each board position. ----------------The paper’s presentation is inefficient and muddled, and the results seem incremental.----------------Presentation:----------------The abstract and introduction point out that AlphaGo requires many RL iterations to train, and propose to improve this by swapping out the policy network with one that is more amenable to training. However, the paper only presents supervised learning results, not RL.----------------While it’s not unreasonable to assume that a higher-capacity network that shows improvements in supervised learning will also yield dividends in RL, it’s still unsatisfying to be presented with SL improvements and be asked to assume that the RL improvements will be of a similar magnitude, whatever that may mean. It would’ve been more convincing to train both AlphaGo and this paper's architectures on an equal number of RL self-play iterations, then have them play each other. (Both would be pre-trained using supervised training, as per the AlphaGo paper).----------------It is not until section 3.3 that it is clearly stated this is strictly a supervised-learning paper. This should have been put front and center in the abstract and introduction.----------------Fully 3 pages are spent on giant but simple architecture diagrams. This is both extravagant and muddles the exposition. It seems better to show just the architectures used in the experiments, and spend at most half a page doing so, so that they may be seen alongside one another.----------------The results (Table 1, Figures 7 and 8) are hard to skim, as there is little information in the captions, and the graph axes are poorly labeled. For example, I assume “Examples” in figures 7 and 8 should be “Training examples”, and the number of training examples isn’t 0-50, but some large multiple thereof.----------------Results: ----------------The take-home seems to be that that deeper networks do better, and residual architectures and spatial batch normalization each improve the results in this domain, as they are known to do in others. Furthermore, we are asked to assume that the improvements in an RL setting will be similar to the improvements in SL shown here. These results seem too incremental to justify an ICLR publication. ","The authors suggest alternate feedforward architectures (residual layers) for training a Go policy more efficiently. The authors refer to AlphaGo, but the proposed approach does not use reinforcement learning, which is not stated clearly in the introduction of the paper. The contribution is very incremental, there are no new ideas, and the presentation is muddled."
https://openreview.net/forum?id=BkXMikqxx,"This paper uses an LSTM model to predict what it calls ""open bigrams"" (bigrams of characters that may or may not have letters inbetween) from handwriting data. These open bigrams are subsequently used to predict the written word in a decoding step. The experiments indicate that the system does slightly better than a baseline model that uses Viterbi decoding. I have some major concerns about this paper:----------------- I find the ""cortical inspired"" claim troublesome. If anything, it is psychology/cognitive science inspired, in the sense that open bigrams appear to help for word recognition (Touzet et al. 2014). But the implied cortical characteristics, implicitly referred to e.g. by pointing to analogies between deep neural nets for object recognition and in that case the visual cortex, is unfounded. Is there any direct evidence from neuroscience that open-bigrams constitute a wholly separate layer in the cortex for a handwriting recognition task? Dehaene's work is a proposal, so you'll need to describe more ""findings in cognitive neurosciences [sic] research on reading"" (p. 8) to substantiate those claims. I am further worried by the fact that the authors seem to think that ""deep neural networks are based on a series of about five pairs of neurons [sic] layers"". Unless I misunderstand something, you are specifically referring to Krizhevsky's AlexNet here (which you should probably have cited there)? I hope you don't mean to imply that all deep neural nets need five layers. It is also not true that ten is ""quite close to the number of layers of an efficient deep NN"" -- what network? what task? etc.----------------- The model is not clearly explained. There is a short paragraph in Appendix A.3. that roughly describes the setup, but this does not include e.g. the objective function, or answer why the network output is only considered each two consecutive time steps, rather than at each time step (or so it seems?). This is probably because the paper argues that it ""is focused on the decoder"" (p. 6), rather than on the whole problem. I find this problematic, because in that case we're effectively measuring how easy it is to reconstruct a word from its open bigrams, which has very little to do with handwriting recognition (it could have been evaluated on any text corpus). In fact, as the example on page 4 shows, handwriting is not necessary to illustrate the open bigram hypothesis. Which leads me to wonder why these particular tasks were chosen, if we are only interested in the decoding mechanism?----------------- The comparison is not really fair. The Viterbi decoder only has access to unigrams, as far as I can tell. The only model that does better than that baseline has access to a lot more information, and does not do that much better. Did the Viterbi model have access to the word boundary information (at one point rather confusingly called ""extremities"") that pushed the open bigram model over the edge in terms of performance? Why is there no comparison to e.g. rnn_0,1' (unigram+bigram+boundary markers)? The dataset also appears to be biased in favor of the proposed approach (longer words, only ). I am not convinced that this paper really shows that open bigrams help.----------------I very much like the idea of the paper, but I am simply not convinced by its claims.----------------Minor points:--------- There are quite a few typos. Just a sample: ""independant"" (Fig.1), ""we evaluate an handwritten"", "", hand written words [..], an the results"", ""their approach include"", ""the letter bigrams of a word w is"", ""for the two considered database""--------- Wouldn't it be easy to add how many times a bigram occurs, which would improve the decoding process? You can just normalize over the full counts instead of the binary occurrence counts.--------- The results in Table 5 are the same (but different precision) as the results in Table 2, except that edit distance and SER are added, this is confusing.","There is consistent agreement towards the originality of this work and that the topic here is ""interesting"". Additionally there is consensus that the work is ""clearly written"", and (excepting questions of the word ""cortical"") all would be primed to accept this style of work.     However there is a shared concern about the quality and potential impact of the work, in particularly in terms of the validity of empirical evaluations. Reviewers are generally not inclined to believe that the current empirical evidence validates the conclusions of the word. Suggestions are to: make greater use of a language model, compare to external baselines, or remove the handwriting aspects."
https://openreview.net/forum?id=B1YfAfcgl,"Overview: ----------------This paper introduces a biasing term for SGD that, in theoretical results and a toy example, yields solutions with an approximately equal or lower generalization error. This comes at a computational cost of estimating the gradient of the biasing term for each iteration through stochastic gradient Langevin dynamics, approximating an MCMC sample of the log partition function of a modified Gibbs distribution. The cost is equivalent to adding an inner for-loop to the standard SGD algorithm for each minibatch.----------------Pros:--------- Reviews and distills many results and theorems from past 2 decades that suggest a promising way forward for increasing the generalizability of deep neural networks--------- Generally very well written and well presented results, with interesting discussion of eigenvalues of Hessian as a way to characterize “flat” minima--------- Promising mathematical arguments suggest that E-SGD has generalization error bounded below by SGD, motivating further research in the area----------------Cons / points suggested for a rebuttal:--------(1) One claim of the paper given in the abstract is ”experiments on competitive baselines demonstrate that Entropy-SGD leads to improved generalization and has the potential to accelerate training.“ This does not appear to be supported by the current set of experiments. As the authors comment in the discussion section, “In our experiments, Entropy-SGD results in a comparable generalization error as SGD, but always has a lower cross-entropy loss.” It's not clear to me how to reconcile those two claims.----------------(2) Similarly, the claim of accelerated training is not convincingly supported in the present version of the paper. Vanilla SGD requires a single forward pass through all M minibatches during one epoch for a parameter update, but the new method, E-SGD requires, L*M forward passes during one epoch where L is the number of Langevin updates, which require a minibatch sample each. This could in fact mean that E-SGD has worse computational complexity to reach the same point. In a remark on p.9, the authors note that a single epoch is defined to be “the number of parameter updates required to run through the dataset once.” It’s not clear to me how this answers the objection to a factor of L additional computations required for the inner-loop SGLD iterations. SGLD appears to introduces a potentially costly tradeoff that must be carefully managed by a user of E-SGD.----------------(3) As the previous two points suggest, the paper could use some attention to the magnitude of the claims. For example, the introduction reads “Actively biasing towards wide valleys aids generalization, in fact, we can optimize solely the free energy term to obtain similar generalization error as SGD on the original loss function.“ According the the values reported on pp.9-10, only on MNIST is the generalization error, using only the free energy term (the log partition function of the modified Gibbs distribution), equivalent to using only the SGD loss function. This corresponds to setting rho to 0 in equation (6). On CIFAR-10, rho = 0.01 is used.----------------(4) Another contribution of this paper, the characterization of the optimization landscape in terms of the eigenvalues of the Hessian and low generalization error being associated with flat local extrema, is helpful and interesting. I found the plots clear and useful. As another reviewer has already pointed out, there are high-level similarities to “Flat Minima” by Hochreiter and Schmidhuber (1997). The authors have responded already by adding a paragraph that helpfully explores some differences with H&S 1997. However, the similarities should also be carefully identified and mentioned. H&S 1997 includes detailed theoretical analysis that could be helpful for future work in this area, and has independently discovered a similar approach to training generalizable networks.----------------(5) It's not clear how the assumption about the eigenvalues that were made in section 4.4 / Appendix B affect the application of this result to real-world problems. What magnitude of c>0 needs to be chosen? Does this correspond to a measurable characteristic of the dataset? It's a little mysterious in the current version of the paper.","This paper presents both an analysis of neural net optimization landscapes, and an optimization algorithm that encourages movement in directions of high entropy. The motivation is based on intuitions from physics.    Pros   - the main idea is well-motivated, from a non-standard perspective.   - There are lots of side-experiments supporting the claims for the motivation.  Cons   - The propose method is very complicated, and it sounds like good performance depended on adding and annealing yet another hyperparameter, referred to as 'scoping'.   - The motivating intuition has been around for a long time in different forms. In particular, the proposed method is very closely related to stochastic variational inference, or MCMC methods. Appendix C makes it clear that the two methods aren't identical, but I wish the authors had simply run SVI with their proposed modification, instead of appearing to re-invent the idea of maximizing local volume from scratch. The intuition that good generalization comes from regions of high volume is also exactly what Bayes rule says.    In summary, while there is improvement for the paper, the idea is well-motivated and the experimental results are sound."
https://openreview.net/forum?id=ryxB0Rtxx,"Paper Summary:----------------Authors investigate identity re-parametrization in the linear and the non linear case. ----------------Detailed comments:----------------— Linear Residual Network:----------------The paper shows that for a linear residual network any critical point is a global optimum. This problem is non convex it is interesting that this simple re-parametrization leads to such a result. ---------------- — Non linear Residual Network:----------------Authors propose a construction that maps the points to their labels via a resnet , using an initial random projection, followed by a residual block that clusters the data based on their label, and a last layer that maps the clusters to the label. ----------------1- In Eq 3.4  seems the dimensions are not matching q_j in R^k and e_j in R^r. please clarify ----------------2- The construction seems fine, but what is special about the resnet here in this construction? One can do a similar construction if we did not have the identity? can you discuss this point?--------In the linear case it is clear from a spectral point of view how the identity is helping the optimization. Please provide some intuition.  ----------------3-   Existence of a network in the residual  class that overfits does it give us any intuition on why residual network outperform other architectures? What does an existence result of such a network tell us about its representation power ? --------A simple linear model under the assumption that points can not be too close can overfit the data, and get fast convergence rate (see for instance tsybakov noise condition).----------------4- What does the construction tell us about the number of layers? ----------------5- clustering the activation independently from the label, is an old way to pretrain the network. One could use those centroids as weights for the next layer (this is also related to Nystrom approximation see for instance https://www.cse.ust.hk/~twinsen/nystrom.pdf ). Your clustering is very strongly connected to the label at each residual block.--------I don't think this is appealing or useful since no feature extraction is happening. Moreover the number of layers in this construction--------does not matter. Can you weaken the clustering to be independent to the label at least in the early layers? then one could you use your construction as an initialization in the training. ----------------— Experiments : ----------------- last layer is not trained means the layer before the linear layer preceding the softmax?----------------Minor comments:----------------Abstract:  how  the identity mapping motivated batch normalization?","The paper begins by presenting a simple analysis for deep linear networks. This is more to demonstrate the intuitions behind their derivations, and does not have practical relevance. They then extend to non-linear resnets with ReLU units and demonstrate that they have finite sample expressivity. They formally establish these results. Inspired by their theory, they perform experiments using simpler architectures without any batch norm, dropout or other regularizations and fix the last layer and still attain competitive results. Indeed, they admit that data augmentation is a form of regularization and can replace other regularization schemes.   I think the paper meets the threshold to be accepted."
https://openreview.net/forum?id=SkpSlKIel,"SUMMARY --------This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks with ReLU and threshold units. The main contribution of the paper is to show that approximating a strongly convex differentiable function is possible with much less units when using a network with one more hidden layer. ----------------PROS --------The paper presents an interesting combination of tools and arrives at a nice result on the exponential superiority of depth. ----------------CONS--------The main result appears to address only strongly convex univariate functions. ----------------SPECIFIC COMMENTS ----------------- Thanks for the comments on L. Still it would be a good idea to clarify this point as far as possible in the main part. Also, I would suggest to advertise the main result more prominently. --------I still have not read the revision and maybe you have already addressed some of these points there. ----------------- The problem statement is close to that from [Montufar, Pascanu, Cho, Bengio NIPS 2014], which specifically arrives at exponential gaps between deep and shallow ReLU networks, albeit from a different angle. I would suggest to include that paper it in the overview. ----------------- In Lemma 3, there is an i that should be x----------------- In Theorem 4, ``\tilde f'' is missing the (x). ----------------- Theorem 11, the lower bound always increases with L ? ----------------- In Theorem 11, \bf x\in [0,1]^d? ","The paper makes a solid technical contribution in proving that the deep networks are exponentially more efficient in function approximation compared to the shallow networks. They take the case of piecewise smooth networks, which is practically motivated (e.g. images have edges with smooth regions), and analyze the size of both the deep and shallow networks required to approximate it to the same degree.    The reviewers recommend acceptance of the paper and I am happy to go with their recommendation."
https://openreview.net/forum?id=HJGODLqgx,"Putting the score for now, will post the full review tomorrow.","This paper is a by-the-numbers extension of the hidden semi-Markov model to include nonlinear observations, and neural network-based inference. The paper is fairly clear, although the English isn't great. The experiments are thorough.    Where this paper really falls down is on originality. In particular, in the last two years there have been related works that aren't cited (and unfortunately weren't mentioned by the reviewers) that produce similar models. In particular, Johnson et al's 2016 NIPS paper develops almost the same inference strategy in almost the same model class.     http://stat.columbia.edu/~cunningham/pdf/GaoNIPS2016.pdf  https://arxiv.org/abs/1511.05121  https://arxiv.org/abs/1603.06277    This paper is borderline, but I think makes the cut by virtue of having experiments on real datasets, and by addressing a timely problem (how to have interpretable structure in neural network latent variable models)."
https://openreview.net/forum?id=BJa0ECFxe,"Paper summary--------This paper develops a generalization of dropout using information theoretic--------principles. The basic idea is that when learning a representation z of input x--------with the aim of predicting y, we must choose a z such that it carries the least--------amount of information about x, as long as it can predict y. This idea can be--------formalized using the Information Bottleneck Lagrangian. This leads to an--------optimization problem which is similar to the one derived for variational--------dropout, the difference being that Information dropout allows for a scaling--------factor associated with the KL divergence term that encourages noise. The amount--------of noise being added is made a parameterized function of the data and this--------function is optimized along with the rest of the model. Experimental results on--------CIFAR-10 and MNIST show (small) improvements over binary dropout.----------------Strengths--------- The paper highlights an important conceptual link between probabilistic--------  variational methods and information theoretic methods, showing that dropout--------can be generalized using both formalisms to arrive at very similar models.--------- The presentation of the model is excellent.--------- The experimental results on cluttered MNIST are impressive.----------------Weaknesses--------- The results on CIFAR-10 in Figure 3(b) seem to be on a validation set (unless--------  the axis label is a typo). It is not clear why the test set was not used. This--------makes it hard to compare to results reported in Springenberg et al, as well as--------other results in literature.----------------Quality--------The theoretical exposition is high quality. Figure 2 gives a nice qualitative--------assessment of what the model is doing. However, the experimental results--------section can be made better, for example, by matching the results on CIFAR-10 as--------reported in Springenberg et al. and trying to improve on those using information--------dropout.----------------Clarity--------The paper is well written and easy to follow.----------------Originality--------The derivation of the information dropout optimization problem using IB--------Lagrangian is novel. However, the final model is quite close to variational--------dropout.----------------Significance--------This paper will be of general interest to researchers in representation learning--------because it highlights an alternative way to think about latent variables (as--------information bottlenecks). However, unless the model can be shown to achieve--------significant improvements over simple dropout, its wider impact is likely to be--------limited.----------------Overall--------The paper presents an insightful theoretical derivation and good preliminary--------results. The experimental section can be improved.----------------Minor comments and suggestions ---------- expecially -> especially--------- trough -> through--------- There is probably a minus sign missing in the expression for H(y|z) above Eq (2).--------- Figure 3(a) has error bars, but 3(b) doesn't. It might be a good idea to have those--------for Figure 3(b) as well.--------- Please consider comparing Figure 2 with the activity map of a standard CNN--------  trained with binary dropout, so we can see if similar filtering out is--------happening there already.","The authors all agree that the theory presented in the paper is of high quality and is promising but the experiments are not compelling. The reviewers are concerned that the presented idea and connections to existing methods, while neat, may not be impactful as the promise of the theory does not bear out in practice. One reviewer is concerned that the presented theory is still not useful, stating that the ""information bottleneck thus only becomes meaningful when the capacity of the encoding network is controlled in some measurable way, which is not discussed in the paper"". In general, they seem to agree that the experimental evaluation is still preliminary and unfinished. As such, it would seem that the authors could make the paper far more compelling by demonstrating more compelling improvements on benchmark experiments and submitting to a future conference."
https://openreview.net/forum?id=BysvGP5ee,"This paper proposes a Variational Autoencoder model that can discard information found irrelevant, in order to learn interesting global representations of the data. This can be seen as a lossy compression algorithm, hence the name Variational Lossy Autoencoder. To achieve such model, the authors combine VAEs with neural autoregressive models resulting in a model that has both a latent variable structure and a powerful recurrence structure.----------------The authors first present an insightful Bits-Back interpretation of VAE to show when and how the latent code is ignored. As it was also mentioned in the literature, they say that the autoregressive part of the model ends up explaining all structure in the data, while the latent variables are not used. Then, they propose two complementary approaches to force the latent variables to be used by the decoder. The first one is to make sure the autoregressive decoder only uses small local receptive field so the model has to use the latent code to learn long-range dependency. The second is to parametrize the prior distribution over the latent code with an autoregressive model.----------------They also report new state-of-the-art results on binarized MNIST (both dynamical and statically binarization), OMNIGLOT and Caltech-101 Silhouettes.----------------Review:--------The bits-Back interpretation of VAE is a nice contribution to the community. Having novel interpretations for a model helps to better understand it and sometimes, like in this paper, highlights how it can be improved.----------------Having a fine-grained control over the kind of information that gets included in the learned representation can be useful for a lot of applications. For instance, in image retrieval, such learned representation could be used to retrieve objects that have similar shape no matter what texture they have.----------------However, the authors say they propose two complementary classes of improvements to VAE, that is the lossy code via explicit information placement (Section 3.1) and learning the prior with autoregressive flow (Section 3.2). However, they never actually showed how a VAE without AF prior but that has a PixelCNN decoder performs. What would be the impact on the latent code is no AF prior is used?----------------Also, it is not clear if WindowAround(i) represents only a subset of x_{<i} or it can contain any data other than x_i. The authors mentioned the window can be represented as a small rectangle adjacent to a pixel x_i, must it only contains pixels above and to the left of x_i (similar to PixelCNN)----------------Minor:--------In Equation 8, should there be an expectation over the data distribution?","The reviewers agree that this is a well executed paper, and should be accepted and will make a positive contribution to the conference. In any final version please try to make a connection to the other paper at this conference with the same aims and execution."
https://openreview.net/forum?id=HJ9rLLcxg,"In this paper authors propose a novel data augmentation scheme where instead of augmenting the input data, they augment intermediate feature representations.  Sequence auto-encoder based features are considered, and random perturbation, feature interpolation, and extrapolation based augmentation are evaluated. On three sequence classification tasks and on MNIST and CIFAR-10, it is shown that augmentation in feature space, specifically extrapolation based augmentation, results in good accuracy gains w.r.t. authors baseline.----------------My main questions and suggestions for further strengthening the paper are:----------------a) The proposed data augmentation approach is applied to a learnt auto-encoder based feature space termed ‘context vector’ in the paper.  The context vectors are then augmented and used as input to train classification models. Have the authors considered applying their feature space augmentation idea directly to the classification model during training, and applying it to potentially many layers of the model?  Also, have the authors considered convolutional neural network (CNN) architectures as well for feature space augmentation?  CNNs are now the state-of-the-art in many image and sequence classification task, it would be very valuable to see the impact of the proposed approach in that model.----------------b) When interpolation or extrapolation based augmentation was being applied, did the authors also consider utilizing nearby samples from competing classes as well?  Especially in case of extrapolation based augmentation it will be interesting to check if the extrapolated features are closer to competing classes than original ones.----------------c) With random interpolation or nearest neighbor interpolation based augmentation the accuracy seems to degrade pretty consistently.  This is counter-intuitive.  Do the authors have explanation for why the accuracy degraded with interpolation based augmentation?----------------d) The results on MNIST and CIFAR-10 are inconclusive.  For instance the error rate on CIFAR-10 is well below 10% these days, so I think it is hard to draw conclusions based on error rates above 30%.  For MNIST it is surprising to see that data augmentation in the input space substantially degrades the accuracy (1.093% -> 1.477%).  As mentioned above, I think this will require extending the feature space augmentation idea to CNN based models.","This paper proposes to regularize neural networks by adding synthetic data created by interpolating or extrapolating in an abstract feature space, learning by an autoencoder.    The main idea is sensible, and clearly presented and motivated. Overall this paper is a good contribution. However, the idea seems unlikely to have much impact for two reasons:   - It's unclear when we should expect this method to help vs hurt   - Relatedly, the method has a number of hyperparameters that it's unclear how to set except by cross-validation.    We also want to remark that other regularization methods effectively already do closely related things. Dropout, gradient noise, and Bayesian methods, for instance, effectively produce 'synthetic data' in a similar way when the high-level weights of the network are perturbed."
https://openreview.net/forum?id=S11KBYclx,"The paper addresses the problem of predicting learning curves. The key difference from prior work is that (1) the authors learn a neural network that generalizes across hyperparameter settings and (2) the authors use a Bayesian neural network with SGHMC. --------The authors demonstrate that the proposed approach is effective on extrapolating partially observed curves as well as predicting unobserved learning curves on various architectures (FC, CNN, LR and VAE). This seems very promising for Bayesian optimization, I'd love to see an experiment that evaluates the relative advantage of this proposed method :)----------------Have you thought about ways to handle learning rate decays? Perhaps you could run the algorithm on a random subset of data and extrapolate from that?----------------I was thinking of other evaluation measures in addition to MSE and LL. In practice, we care about the most promising run. Would it make sense to evaluate how accurately each method identified the best run?----------------Minor comments:----------------Fonts are too small and almost illegible on my hard copy. Please increase the font size for legends and axes in the figures.----------------Fig 6: not all figures seem to have six lines. Are the lines overlapping in some cases?","The reviewers agreed that this is a good paper that proposes an interesting approach to modeling training curves. The approach is well motivated in terms of surrogate based (e.g. Bayesian) optimization. They are convinced that a great model of training curves could be used to extrapolate in the future, greatly expediting hyperparameter search methods through early stopping. None of the reviewers championed the paper, however. They all stated that the paper just did not go far enough in showing that the method is really useful in practice. While the authors seem to have added some interesting additional results to this effect, it was a little too late for the reviewers to take into account.     It seems like this paper would benefit from some added experiments as per the authors' suggestions. Incorporating this within a Bayesian optimization methodology is certainly non-trivial (e.g. may require rethinking the modeling and acquisition function) but could be very impactful. The overall pros and cons are as follows:    Pros:  - Proposes a neat model for extrapolating training curves  - Experiments show that the model can extrapolate quite well  - It addresses a strong limitation of current hyperparameter optimization techniques    Cons  - The reviewers were underwhelmed with the experimental analysis  - The paper did not push the ideas far enough to demonstrate the effectiveness of the approach  Overall, the PCs have established that, despite some of its weaknesses, this paper deserved to appear at the conference."
https://openreview.net/forum?id=H1eLE8qlx,"The paper tackles the very important problem of learning options from data. The introduction of the budget constraint is an interesting twist on this problem, which I had not seen before (though other methods apply other constraints.)----------------I must say I’m not very convinced by the need to introduce the Bi-POMDP framework, where the conventional POMDP framework would do.  In discussions, the authors suggest this makes for simpler comparison with RL models, but I find that it rather obscures the link to POMDP models.----------------The proposed method makes an interesting contribution, distinct from the existing literature as far as I know.  The extension to discover a discrete set of options is a nice feature for practical applications.----------------In terms of the algorithm itself, I am actually unclear about lines 4 & 6.  At line 4, I don’t know how \sigma_t is computed. Can you give the precise equation?  At line 6, I don’t know how the new option o_t is generated. Again, can you give the precise procedure?----------------The paper contains several empirical results, on contrasting simulated domains. For some of these domains, such as CartPole, it’s really not clear that options are necessary. In my mind, the lack of comparison to other options learning methods is a limitation of the current draft.","Most of the reviewers agreed that the proposed budgeted options framework was interesting, but there were a number of serious concerns raised about the work. Many of the reviewers found the assumptions of the approach to be somewhat odd, and while the particular formulation in the paper was generally assessed as novel, it has connections to a number of previous works that were not explored in detail. Finally, the experimental evaluation is conducted on simple tasks with few comparisons, so it is very difficult to make concrete conclusions about how well the method works."
https://openreview.net/forum?id=BJC8LF9ex,"This paper proposed a way to deal with supervised multivariate time series tasks involving missing values. The high level idea is still using the recurrent neural network (specifically, GRU in this paper) to do sequence supervised learning, e.g., classification, but modifications have been made to the input and hidden layers of RNNs to tackle the missing value problem. ----------------pros: --------1) the insight of utilizing missing value is critical. the observation of decaying effect in the healthcare application is also interesting;--------2) the experiment seems to be solid; the baseline algorithms and analysis of results are also done properly. ----------------cons:--------1) the novelty of this work is not enough. Adding a decaying smooth factor to input and hidden layers seems to be the main modification of the architecture. --------2) the datasets used in this paper are small. --------3) the decaying effect might not be able to generalize to other domains. ","This paper presents a modification of GRU-RNNs to handle missing data explicitly, allowing them to exploit data not missing at random. The method is presented clearly enough, but the reviewers felt that the claims were overreaching. It's also unsatisfying that the method depends on specific modifications of RNN architectures for a particular domain, instead of being a more general approach."
https://openreview.net/forum?id=HJ1JBJ5gl,"This paper investigates whether the variational inference interpretation of dropout, as introduced in [Gal & Ghahramani (2016), and Kingma et al (2015)], can lead to good estimates of mode uncertainty outside of the training distribution. This is an area of research that indeed warrants more experimental investigation. One very interesting finding is that MC integration leads to much calibration, thus probably much better out-of-sample prediction, than the more usual. -------- --------Critique:--------- As explained in Kingma et al (2015), when using continuous posterior distributions over the weights, the dropout rate can be optimized, leading to better regularization. While the paper is cited in the introduction, this adaptive form of dropout is missing from experiments, without clarification.----------------- Only the dropout rate p=0.5 was used across experiments, while the optimal rate is problem dependent, as found by earlier published work.----------------- No new ideas are presented, and the analysis in the paper is quite limited. As it stands, this would be more appropriate for a workshop.",The reviewers unanimously recommend rejecting this paper.
https://openreview.net/forum?id=rJY0-Kcll,"This work presents an LSTM based meta-learning framework to learn the optimization algorithm of a another learning algorithm (here a NN).--------The paper is globally well written and the presentation of the main material is clear. The crux of the paper: drawing the parallel between Robbins Monroe update rule and the LSTM update rule and exploit it to satisfy the two main desiderata of few shot learning (1- quick acquisition of new knowledge, 2- slower extraction of general transferable knowledge) is intriguing. ----------------Several tricks re-used from (Andrychowicz et al. 2016)  such as parameter sharing and normalization, and novel design choices (specific implementation of batch normalization) are well  motivated. --------The experiments are convincing. This is a strong paper. My only concerns/questions are the following:----------------1. Can it be redundant to use the loss, gradient and parameters as input to the meta-learner? Did you do ablative studies to make sure simpler combinations are not enough.--------2. It would be great if other architectural components of the network can be learned in a similar fashion (number of neurons, type of units, etc.). Do you have an opinion about this?--------3. The related work section (mainly focused on meta learning) is a bit shallow. Meta-learning is a rather old topic and similar approaches have been tried to solve the same problem even if they were not using LSTMs:--------     - Samy Bengio PhD thesis (1989) is all about this ;-)--------     - Use of genetic programming for the search of a new learning rule for neural networks (S. Bengio, Y. Bengio, and J. Cloutier. 1994)--------     - I am convince Schmidhuber has done something, make sure you find it and update related work section.  ----------------Overall, I like the paper. I believe the discussed material is relevant to a wide audience at ICLR.  ","The authors propose a meta-learner to address the problem of few-shot learning. The algorithm is interesting, and results are convincing. It's a very timely paper that will receive attention in the community. All three reviewers recommend an accept, with two being particularly enthusiastic. The authors also addressed some issues raised by the more negative reviewer. The AC also agrees that the writing needs a little more work to improve clarity. Overall, this is a clear accept."
https://openreview.net/forum?id=BydARw9ex,"The authors investigate a variety of existing and two new RNN architectures to obtain more insight about the effectiveness at which these models can store task information in their parameters and activations.----------------The experimental setups look sound. To generalize comparisons between different architectures it’s necessary to consider multiple tasks and control for the effect of the hyperparameters. This work uses multiple tasks of varying complexities, principled hyperparameter tuning methodology and a number of tuning iterations that can currently only be achieved by the computational resources of some of the larger industrial research groups. ----------------The descriptions of the models and the objective where very clear to me. The descriptions of the experiments and presentation of the results were not always clear to me at times, even with the additional details in the appendix available. Most of these issues can easily be resolved by editing the text. For example, in the memory task the scaling of the inputs (and hence also outputs) is not provided so it’s hard to interpret the squared error scores in Figure 2c. It’s not clear to me what the term ‘unrollings’ refers to in Figure 2b. Is this a time lag with additional hidden state updates between the presentation of the input sequence and the generation of the output? Since the perceptron capacity task is somewhat central to the paper, I think a slightly more precise description of how and when the predictions are computed would be helpful. Due to the large number of graphs, it can be somewhat hard to find the most relevant results. Perhaps some of the more obvious findings (like Figure 1(b-d) given Figure 1a) could move to the appendix to make space for more detailed task descriptions.----------------Novelty is not really the aim of this paper since it mostly investigates existing architectures. To use the mutual information to obtain bits per parameter scores in highly non-linear parameterized functions is new to me. The paper also proposed to new architectures that seem to have practical value. The paper adds to the currently still somewhat neglected research effort to employ the larger computational resources we currently have towards a better understanding of architectures which were designed when such resources were not yet available. I’d argue that the paper is original enough for that reason alone.----------------The paper provides some interesting new insights into the properties of RNNs. While observed before, it is interesting to see the importance of gated units for maintaining trainability of networks with many layers. It is also interesting to see a potential new use for vanilla RNNs for simpler tasks where a high capacity per parameter may be required due to hardware constraints. The proposed +RNN may turn out to have practical value as well and the hyperparameter robustness results shed some light on the popularity of certain architectures when limited time for HP tuning is available. The large body of results and hyperparameter analysis should be useful to many researchers who want to use RNNs in the future. All in all, I think this paper would make a valuable addition to the ICLR conference but would benefit from some improvements to the text.----------------Pros:--------* Thorough analysis.--------* Seemingly proper experiments.--------* The way of quantifying capacity in neural networks adds to the novelty of the paper.--------* The results have some practical value and suggest similar analysis of other architectures.--------* The results provide useful insights into the relative merits of different RNN architectures.----------------Cons:--------* It’s hard to isolate the most important findings (some plots seem redundant).--------* Some relevant experimental details are missing.","The reviewers all agreed that this paper should appear at the conference. The experiments seem to confirm interesting intuition about the capacity of recurrent nets and how difficult they are to train, and the reviewers appreciated the experimental rigor. This is certainly of interest and useful to the ICLR community and will lead to fruitful discussion. The reviewers did request more fine details related to the experiments for reproducibility (thank you for adding more detail to the appendix). The authors are recommended to steer clear of making any strong but unsubstantiated references to neuroscience."
https://openreview.net/forum?id=SkJeEtclx,"This paper addresses video captioning with a TEM-HAM architecture, where a HAM module attends over attended outputs of the TEM module when generating the description. This gives a kind of 2-level attention. The model is evaluated on the Charades and MSVD datasets.----------------1. Quality/Clarify: I found this paper to be poorly written and relatively hard to understand. As far as I can tell the TEM module of Section 3.1 is a straight-forward attention frame encoder of Bahdanau et al. 2015 or Xu et al. 2015. The decoder of Section 3.3 is a standard LSTM with log likelihood. The HAM module of Section 3.2 is the novel module but is not very well described. It looks to be an attention LSTM where the attention is over the TEM LSTM outputs, but the attention weights are additionally conditioned on the decoder state. There are a lot of small problems with the description, such as notational discrepancy in using \textbf in equations and then not using it in the text. Also, I spent a long time trying to understand what f_m is. The authors say: ----------------""In order to let the network remember what has been attended before and the temporal--------structure of a video, we propose f_m to memorize the previous attention and encoded version of an--------input video with language model. Using f_m not only enables the network to memorize previous--------attention and frames, but also to learn multi-layer attention over an input video and corresponding--------language.""----------------Where one f_m is bold and the other f_m is not. Due to words such as ""we propose f_m"" assumed this was some kind of a novel technical contribution I couldn't find any details about but it is specified later in Section 3.3 at the end that f_m is in fact just an LSTM. It's not clear why this piece of information is in Section 3.3, which discusses the decoder. The paper is sloppy in other parts. For example in Table 1 some numbers have 1 significant digit and some have 2. The semantics of the horizontal line in Table 2 are not explained in text. ----------------2. Experimental results: The ablation study shows mixed results when adding TEM and HAM to the model. Looking at METEOR which was shown to have the highest correlation to humans in the COCO paper compared to the other evaluation criteria, adding TEM+HAM improves the model from 31.20 to 31.70. It is not clear how significant this improvement is, especially given that the test set is only 670 videos. I have doubts over this result. In Table 2, the METEOR score of Pan et al. 2016a is higher [33.10 vs. 31.80], but this discrepancy is not addressed in text. This is surprising because the authors explicitly claim ""state of the art results"".----------------3. Originality/Significance: The paper introduces an additional layer of attention over a more standard sequence to sequence setup, which is argued to alleviate the burden on the LSTM's memory. This is moderately novel but I don't believe that the experimental results make it sufficiently clear that it is also worth doing. If the paper made the standard model somehow simpler instead of more complex I would be more inclined to judge it favorably.------------------------Minor:--------In response to the author's comment ""not sure what causes to think of RBM. We don't model any part of our architecture using RBM. We'd be appreciated if you please elaborate more about your confusion about figure 1 so we can address it accordingly."", I created a diagram to hopefully make this more clear:  https://imgur.com/a/4MJaG . It is very common to use 2 rows of circles with lines between them pairwise to denote an RBM. The lines indicate undirected edges of the graphical model. A typical RBM diagram can have, for example, 3 circles on top and 4 circles on the bottom joined with edges. Your diagram has 4 circles on the top and 5 circles on the bottom joined with edges. However, not all of your circles from the top and bottom are connected, which further adds to the difficulty. In particular, your last circle on the bottom is only connected to 2 circles on the top. If the authors are trying to denote a neural network I would advise using arrows instead of lines. However, since the HAM module uses an LSTM just like the encoder and the decoder it is unclear why this module has a visually distinct appearance at all.","There appears to be consensus among the reviewers that the paper appears the overstate its contributions: the originality of the proposed temporal modeler (TEM) is limited, and the experimental evaluation (which itself is of good quality!) does not demonstrate clear merits of the TEM architecture. As a result, the impact of this paper is expected to be limited."
https://openreview.net/forum?id=rJY3vK9eg,"This paper is methodologically very interesting, and just based on the methodological contribution I would vote for acceptance. However, the paper's sweeping claims of clearly beating existing baselines for TSP have been shown to not hold, with the local search method LK-H solving all the authors' instances to optimality -- in seconds on a CPU, compared to clearly suboptimal results by the authors' method in 25h on a GPU. ----------------Seeing this clear dominance of the local search method LK-H, I find it irresponsible by the authors that they left Figure 1 as it is -- with the line for ""local search"" referring to an obviously poor implementation by Google rather than the LK-H local search method that everyone uses. For example, at NIPS, I saw this Figure 1 being used in a talk (I am not sure anymore by whom, but I don't think it was by the authors), the narrative being ""RNNs now also clearly perform better than local search"". Of course, people would use a figure like that for that purpose, and it is clearly up to the authors to avoid such misconceptions. ----------------The right course of action upon realizing the real strength of local search with LK-H would've been to make ""local search"" the same line as ""Optimal"", showing that the authors' method is still far worse than proper local search. But the authors chose to leave the figure as it was, still suggesting that their method is far better than local search. Probably the authors didn't even think about this, but this of course will mislead the many superficial readers. To people outside of deep learning, this must look like a sensational yet obviously wrong claim. I thus vote for rejection despite the interesting method. --------------------------------------------------------Update after rebuttal and changes:----------------I'm torn about this paper. ----------------On the one hand, the paper is very well written and I do think the method is very interesting and promising. I'd even like to try it and improve it in the future. So, from that point of view a clear accept.----------------On the other hand, the paper was using extremely poor baselines, making the authors' method appear sensationally strong in comparison, and over the course of many iterations of reviewer questions and anonymous feedback, this has come down to the authors' methods being far inferior to the state of the art. That's fine (I expected that all along), but the problem is that the authors don't seem to want this to be true... E.g., they make statements, such as ""We find that both greedy approaches are time-efficient and just a few percents worse than optimality.""--------That statement may be true, but it is very well known in the TSP community that it is typically quite trivial to get to a few percent worse than optimality. What's hard and interesting is to push those last few percent. --------(As a side note: the authors probably don't stop LK-H once it has found the optimal solution, like they do with their own method after finding a local optimum. LK-H is an anytime algorithm, so even if it ran for a day that doesn't mean that it didn't find the optimal solution after milliseconds -- and a solution a few percent suboptimal even faster).----------------Nevertheless, since the claims have been toned down over the course of the many iterations, I was starting to feel more positive about this paper when just re-reading it. That is, until I got to the section on Knapsack solving. The version of the paper I reviewed was not bad here, as it at least stated two simple heuristics that yield optimal solutions:----------------""Two simple heuristics are ExpKnap, which employs brand-and-bound with Linear Programming bounds (Pisinger, 1995), and MinKnap, which employs dynamic programming with enumerative bounds (Pisinger, 1997). Exact solutions can also be optained by quantizing the weights to high precisions and then performing dynamic programming with a pseudo-polynomial complexity (Bertsimas & Demir, 2002)."" That version then went on to show that these simple heuristics were already optimal, just like their own method.----------------In a revision between December 11 and 14, however, that paragraph, along with the optimal results of ExpKnap and MinKnap seems to have been dropped, and the authors instead introduced two new poor baseline methods (random search and greedy). This was likely in an effort to find some methods that are not optimal on these very easy instances. I personally find it pointless to present results for random search here, as nobody would use that for TSP. It's like comparing results on MNIST against a decision stump (yes, you'll do better than that, but that is not surprising). The results for greedy are interesting to see. However, dropping the strong results of the simple heuristics ExpKnap and MinKnap (and their entire discussion) appears unresponsible, since the resulting table in the new version of the paper now suggests that the authors' method is better than all baselines. Of course, if all that one is after is a column of bold numbers for ones own approach that's what one can do, but I don't find it responsible to hide the better baselines. Also, why don't the authors try at least the same OR-tools solver from Google that they tried for TSP? It seems to support Knapsack directly: https://developers.google.com/optimization/bin/knapsack----------------Really, all I'm after is a responsible (and if you wish, humble) presentation of what I believe to be great results that are really promising for the field. The authors just need to make it crystal clear that, as of now, their method is still very far away from the state of the art. And that's OK; you don't typically beat an entire field with one paper. If the authors clearly stated that throughout, I would clearly argue for acceptance ... (Maybe that's not the norm in machine learning, but I don't think you have to beat everything quite yet if your approach is very different and promising -- see DL and ImageNet.)--------Concretely, I would recommend that the authors do the following:----------------- Put the original Figure 1 back in, but dropping the previous poor local search and labelling the line at 1.0 ""local search (LK-H) = exact (Concorde)"". If the authors would like to, they could also in addition leave the previous poor local search in and label it ""Google OR tools (generic local search)"" --------- Put the other baselines back into the Knapsack section--------- Make sure the wording clearly states throughout that the method is still quite far from the state of the art (it's perfectly fine to strongly state that the direction is very promising).------------------------Overall, this paper has to watch out for not becoming an example of promising too much (some would call it hype-generation) before having state-of-the-art results. (I believe the previous comparison against local search fell into that category, and now the current section on Knapsack does.) I believe that would do a great disservice to our community since it builds up an expectation that the field cannot live up to (and that's a recipe for building up a bubble that has no other chance but burst).----------------Having said all that, I think the paper can be saved if the authors embrace that they are still far away from the state of the art. I'll be optimistic and trust that they will come around and make the changes I suggested above. Hoping for those changes, since I think the method is a solid step forward, I'm updating my score to a weak accept.","This was one of the more controversial submissions to this area, and there was extensive discussion over the merits and contributions of the work. The paper also benefitted from ICLRs open review system as additional researchers chimed in on the paper and the authors resubmitted a draft. The authors did a great job responding and updating the work and responding to criticisms. In the end though, even after these consideration, none of the reviewers strongly supported the work and all of them expressed some reservations.     Pros:  - All agree that the work is extremely clear, going as far as saying the work is ""very well written"" and ""easy to understand"".   - Generally there was a predisposition to support the work for its originality particularly due to its ""methodological contributions"", and even going so far as a saying it would generally be a natural accept.    Cons:  - There was a very uncommonly strong backlash to the claims made by the paper, particularly the first draft, but even upon revisions. One reviewer even saying this was an ""excellent example of hype-generation far before having state-of-the-art results"" and that it was ""doing a disservice to our community since it builds up an expectation that the field cannot live up to"" . This does not seem to be an isolated reviewer, but a general feeling across the reviews. Another faulting ""the toy-ness of the evaluation metric"" and the way the comparisons were carried out.  - A related concern was a feeling that the body of work in operations research was not fully taken account in this work, noting ""operations research literature is replete with a large number of benchmark problems that have become standard to compare solver quality"". The authors did fix some of these issues, but not to the point that any reviewer stood up for the work."
https://openreview.net/forum?id=S1QefL5ge,"# Summary--------This paper proposes an algorithm to learn the structure of continuous SPNs in a single pass through the data,--------basically by ""growing"" the SPN when two variables are correlated.----------------## NOTE--------I am not an expert on SPNs, and can not really judge how impressive the presented results are due to lack of familiarity with the datsets.----------------# Pro--------- This looks like possibly impactful work, proposing a simple and elegant algorithm for learning SPN structure single-pass, rather than just using random structure which has been done in other work in the online settings.----------------# Con--------- The paper is heavily updated between submission deadline and submission of reviews.--------- The paper reads like a rush job, sloppily written - at least the first version.--------- Comparison to literature is severely lacking; eg ""several automated structure learning techniques have been proposed"" followed by 6 citations but no discussion of any of them, which one is most related, which ideas carry over from the offline setting to this online setting, etc. Also since this work presents both joint structure & *parameter* learning, comparison to the online parameter learning papers (3 cited) would be appreciated, specifically since these prior approaches seem to be more principled with Bayesian Moment Matching in Jaini 2016 for example.--------- I do not know enough about SPNs and the datasets to properly judge how strong the results are, but they seem to be a bit underwhelming on the large datasets wrt Random----------------# Remaining questions after the paper updates--------- Table 3: Random structure as baseline ok, but how were the parameters here learned? Your simple running average or with more advanced methods?--------- Table 1: you are presenting *positive* average log-likelihood values? This should be an average of log(p<=1) < 0 values? What am I missing here?----------------I recommend reject mostly because this paper should have been finished and polished at submission time, not at review deadline time.","The authors present a framework for online structure learning of sum-product network. They overcome challenges such as being able to learn a valid sum product network and to have an online learning mechanism. Based on the extensive discussions presented by the reviewers, our recommendation is to accept this paper for a workshop."
https://openreview.net/forum?id=HJpfMIFll,"On the plus side, the paper proposes a mathematically interesting model for a context of a word (i.e., a Grassmanian manifold).----------------On the minus side, the paper mostly ignores the long history of Word Sense Induction (WSI) and Word Sense Disambiguation (WSD), citing and comparing only some relatively recent papers. The experiments in this paper done on SemEval-2010 are not very persuasive. (It's difficult to evaluate the experiments done on the 2016 data, since they are not directly comparable to published results).----------------For example, going back to the SemEval-2010 WSI task in [1], the best system seems to be UoY [2]. The F-measure seems to be a poor metric: always assigning one sense to every word (""MFS"") yields the highest F-measure of 63.5%. The paper's result with ""2 clusters"" (with an average of about 1.9) seems to be close to MFS. So I don't think we can use F-measure to compare.----------------The V-measure seems to be tilted towards systems that have high number of senses per word. UoY has V=15.7%, while the paper (with ""5 clusters"") has 14.4%. That isn't very convincing that the proposed method has captured the geometry of polysemy.----------------In general, I have often wondered why people work on pure unsupervised WSI and WSD. The assessment is very difficult (as described above). More importantly, some very weakly supervised systems (with minimal labels) can work pretty well to bootstrap. See, e.g., the classic paper [3].----------------If the authors used the Grassmannian idea to solve higher-level NLP problems directly (such as analogies), that would be very persuasive. However, that's a very different paper than what was submitted. For an example of application of Grassmannian manifolds to analogies, see [4].----------------References:--------1. Manandhar, Suresh, et al. ""SemEval-2010 task 14: Word sense induction & disambiguation."" Proceedings of the 5th international workshop on semantic evaluation. Association for Computational Linguistics, 2010.--------2. Korkontzelos, Ioannis, and Suresh Manandhar. ""Uoy: Graphs of unambiguous vertices for word sense induction and disambiguation."" Proceedings of the 5th international workshop on semantic evaluation. Association for Computational Linguistics, 2010.--------3. Yarowsky, David. ""Unsupervised word sense disambiguation rivaling supervised methods."" Proceedings of the 33rd annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1995.--------4. Mahadevan, Sridhar,  and Sarath Chandar Reasoning about Linguistic Regularities in Word Embeddings using Matrix Manifolds https://arxiv.org/abs/1507.07636 ",The paper considers an important problem largely ignored by continuous word representation learning: polysemy. The approach is mathematically grounded and interesting and well explored.
https://openreview.net/forum?id=BkJsCIcgl,"This work proposes a computational structure of function approximator with a strong prior: it is optimized to act as an abstract MRP, capable of learning its own internal state, model, and notion of time-step. Thanks to the incorporation of a \lambda-return style return estimation, it can effectively adapt its own ""thinking-depth"" on the current input, thus performing some sort of soft iterative inference.----------------Such a prior, maintained by strong regularization, helps perform better than similar baselines or some prediction tasks that require some form of sequential reasoning.----------------The proposed idea is novel, and a very interesting take on forcing internal models upon function approximators which begs for future work. The experimental methodology is complete, showcases the potential of the approach, and nicely analyses the iterative/adaptative thinking depth learned by the model.----------------As pointed out by my previous comments, the paper reads well but utilizes language that may confuse a reader unfamiliar with the subject. I think some rewording could be done without having much impact on the depth of the paper. In particular, introducing the method as a regularized model pushed to act like an MRP, rather than an actual MRP performing some abstract reasoning, may help confused readers such as myself.","There is potential here for a great paper, unfortunately in its current form there is too deep of a disconnect between the framing and promise of the presentation, and the empirical validation actually delivered by the experiments.  The choice of the experimental setting (iid input from fixed distributions in a pattern recognition setting where targets are between 0 and 1) is too narrow to allow for convincing validation of the central hypothesis of the paper, namely, that the architecture (a recurrent convnet with sigmoid gating) can be useful for problems involving planning. Instead, it simply shows that the architecture is better at outputting the targets than other deep architectures without sigmoid gating.  I strongly encourage the authors to add more ambitious experiments to keep the empirical arm more in step with the stated promises of the set-up."
https://openreview.net/forum?id=rJEgeXFex,"This is a well written, organized, and presented paper that I enjoyed reading.  I commend the authors on their attention to the narrative and the explanations.  While it did not present any new methodology or architecture, it instead addressed an important application of predicting the medications a patient is using, given the record of billing codes.  The dataset they use is impressive and useful and, frankly, more interesting than the typical toy datasets in machine learning.  That said, the investigation of those results was not as deep as I thought it should have been in an empirical/applications paper.  Despite their focus on the application, I was encouraged to see the authors use cutting edge choices (eg Keras, adadelta, etc) in their architecture.  A few points of criticism:-----------------The numerical results are in my view too brief.  Fig 4 is anecdotal, Fig 5 is essentially a negative result (tSNE is only in some places interpretable), so that leaves Table 1.  I recognize there is only one dataset, but this does not offer a vast amount of empirical evidence and analysis that one might expect out of a paper with no major algorithmic/theoretical advances.  To be clear I don't think this is disqualifying or deeply concerning; I simply found it a bit underwhelming.----------------- To be constructive, re the results I would recommend removing Fig 5 and replacing that with some more meaningful analysis of performance.  I found Fig 5 to be mostly uninformative, other than as a negative result, which I think can be stated in a sentence rather than in a large figure.----------------- There is a bit of jargon used and expertise required that may not be familiar to the typical ICLR reader.  I saw that another reviewer suggested perhaps ICLR is not the right venue for this work.  While I certainly see the reviewer's point that a medical or healthcare venue may be more suitable, I do want to cast my vote of keeping this paper here... our community benefits from more thoughtful and in depth applications. Instead I think this can be addressed by tightening up those points of jargon and making the results more easy to evaluate by an ICLR reader (that is, as it stands now researchers without medical experience have to take your results after Table 1 on faith, rather than getting to apply their well-trained quantitative eye). ----------------Overall, a nice paper.","This paper applies RNNs to predict medications from billing costs. While this paper does not have technical novelty, it is well done and well organized. It demonstrates a creative use of recent models in a very important domain, and I think many people in our community are interested and inspired by well-done applications that branch to socially important domains. Moreover, I think an advantage to accepting it at ICLR is that it gives our ""expert"" stamp of approval -- I see a lot of questionable / badly applied / antiquated machine learning methods in domain conferences, so I think it would be helpful for those domains to have examples of application papers that are considered sound."
https://openreview.net/forum?id=rkaRFYcgl,"The paper proposes a low-rank version of pass-through networks to better control capacity, which can be useful in some cases, as shown in the experiments.--------That said, I found the results not very convincing overall. Results are overall not as good as state-of-the-art on sequential MNIST or the memory task, but add one more hyper-parameter to tune. As I said, it would help to show in Tables and/or Figures competing approaches like uRNNs.","The reviewers seem to agree that the framework presented is not very novel, something I agree with.  The experiments show that the low rank + diagonal parameterization can be useful, however. The paper could be improved by making a more tightened message, and clearer arguments. As it currently stands, however it does not seem ready for publication in ICLR."
https://openreview.net/forum?id=ryuxYmvel,"The authors describe a dataset of proof steps in higher order logic derived from a set of proven theorems. The success of methods like AlphaGo suggests that for hard combinatorial style problems, having a curated set of expert data (in this case the sequence of subproofs) is a good launching point for possibly super-human performance. Super-human ATPs are clearly extremely valuable. Although relatively smaller than the original Go datasets, this dataset seems to be a great first step. Unfortunately, the ATP and HOL aspect of this work is not my area of expertise. I can't comment on the quality of this aspect.----------------It would be great to see future work scale up the baselines and integrate the networks into state of the art ATPs. The capacity of deep learning methods to scale and take advantage of larger datasets means there's a possibility of an iterative approach to improving ATPs: as the ATPs get stronger they may generate more data in the form of new theorems. This may be a long way off, but the possibility is exciting.","The paper presents a new dataset and initial machine-learning results for an interesting problem, namely, higher-order logic theorem proving. This dataset is of great potential value in the development of deep-learning approaches for (mathematical) reasoning.    As a personal side note: It would be great if the camera-ready version of the paper would provide somewhat more context on how the state-of-the-art approaches in automatic theorem proving perform on the conjectures in HolStep. Also, it would be good to clarify how the dataset makes sure there is no ""overlap"" between the training and test set: for instance, a typical proof of the Cauchy-Schwarz inequality employs the Pythagorean theorem: how can we be sure that we don't have Cauchy-Schwarz in the training set and Pythagoras in the test set?"
https://openreview.net/forum?id=Hyq4yhile,"This paper presents an approach for skills transfer from one task to another in a control setting (trained by RL) by forcing the embeddings learned on two different tasks to be close (L2 penalty). The experiments are conducted in MuJoCo, with a set of experiments being from the state of the joints/links (5.2/5.3) and a set of experiments on the pixels (5.4). They exhibit transfer from arms with different number of links, and from a torque-driven arm to a tendon-driven arm.----------------One limitation of the paper is that the authors suppose that time alignment is trivial, because the tasks are all episodic and in the same domain. Time alignment is one form of domain adaptation / transfer that is not dealt with in the paper, that could be dealt with through subsampling, dynamic time warping, or learning a matching function (e.g. neural network).----------------General remarks: The approach is compared to CCA, which is a relevant baseline. However, as the paper is purely experimental, another baseline (worse than CCA) would be to just have the random projections for ""f"" and ""g"" (the embedding functions on the two domains), to check that the bad performance of the ""no transfer"" version of the model is due to over-specialisation of these embeddings. I would also add (for information) that the problem of learning invariant feature spaces is also linked to metric learning (e.g. [Xing et al. 2002]). More generally, no parallel is drawn with multi-task learning in ML. In the case of knowledge transfer (4.1.1), it may make sense to anneal \alpha.----------------The experiments feel a bit rushed. In particular, the performance of the baseline being always 0 (no transfer at all) is uninformative, at least a much bigger sample budget should be tested. Also, why does Figure 7.b contain no ""CCA"" nor ""direct mapping"" results? Another concern that I have with the experiments: (if/how) did the author control for the fact that the embeddings were trained with more iterations in the case of doing transfer?----------------Overall, the study of transfer is most welcomed in RL. The experiments in this paper are interesting enough for publication, but the paper could have been more thorough.","pros:  - tackles a fundamental problem of interest to many  - novel approach    cons:  - originally not evaluated against some reasonable benchmarks. Note: now added or addressed  - little theoretical development cf MDP theory  - some remaining questions about the necessity (and ability) to find good time alignments    I personally found the ideas to be quite compelling, and believe that this is likely to inspire future work.  The experiments represent interesting scenarios for transfer, with the caveat that they are just in simulation."
https://openreview.net/forum?id=BJlxmAKlg,"This paper proposes a new architecture for document comprehension. The main addition to the model, as claimed by the authors, is that the model is able to adaptively determine how many inference ‘hops’ is required in order to solve a particular problem. This is in contrast to previous work, where the number of hops is fixed.----------------Overall, the change of adding a termination gate is rather modest, and it leads to rather small gains on the tested CNN/ Daily Mail dataset (is it possible to include significance here?). Actually, I’m not sure why having an adaptive number of hops would be better performance-wise than using the maximum number of hops – unless I missed it the authors don’t argue this point well (other than saying it mimics humans). Does the model forget some things if it performs too many hops? The authors do say:--------“The results suggest that the termination gate variable in the ReasoNet is helpful when training with sophisticated examples, and makes models converge faster”--------but don’t elaborate much beyond this. I could see for example an argument being made that it reduces the amount of computation required per question. The authors do show results comparing the model without a termination gate on the Graph Reachability dataset, and the full model does seem to perform quite a bit better, but I would like this to also be done on the CNN/ Daily Mail datasets, and for there to be more insights into why the performance is improved vs. the ReasoNet-Last model.----------------One of the contributions I like most from this paper is not the actual model, but the Graph Reachability dataset. It is designed to test the reasoning abilities of the ReasoNet model in more detail. One of the benefits is that the inference procedure necessary to solve the task is very clear, as opposed to the CNN/ Daily Mail dataset, thus it is easier to see what the model is actually doing. I would like to see future models also tested on this dataset.----------------Overall, I think this is a borderline paper.----------------Other remarks:----------------Note that learning a baseline for REINFORCE has previously been studied, see: https://arxiv.org/pdf/1606.01541v4.pdf (although the authors mention only it briefly in the paper)----------------“ReasoNets are devised to mimic the inference process of human readers.”--------I think this is too strong a claim (that humans use the same kind of ‘iterative hop’ method for answering questions), unless it is supported by actual analysis of humans – I think the similarities to humans are more at a surface level. Also, I think it’s unnecessary to actually understanding the model. I would change ‘mimic’ to ‘inspired by’, or something along those lines.------------------------EDIT: I thank the authors for taking the time to reply, and clearing some things up with regards to the idea of multiple hops. I am keeping my score the same for the following reason: in my opinion, accepted papers involving new model architectures should either consist of (1) a small adjustment that leads to a large improvement, or (2) a very innovative idea that leads to comparable performance (or better), but provides many new insights about the problem or can be transferred to many different domains. This paper seems to be a rather small adjustment (allowing multiple hops) which results in small improvements on CNN/Daily Mail (which as Reviewer 3 points out has little headroom). I think this paper would be suitable for a conference such as EMNLP.","This paper introduces a method for estimating the number of iterations of an attention mechanism in a neural machine reading module using REINFORCE and a custom baseline, which is estimated on the data. Estimating a baseline from data, multi-hop attention, and modelling latent variable models with REINFORCE are not new, so their composition, while sensible, is slightly incremental. There were some concerns from several of the reviewers with the impact the experiments have in terms of validating the model changes. Improvements on ""real"" tasks such as CNN/DailyMail did not impress reviewers, some of whom believe the benchmarks compared to were not representative of the best comparable architectures tried on these datasets (e.g. pointer networks, which definitely can be used). Overall, I do not find this paper strong enough to recommend acceptance to the main conference in its current state. With better evaluation, it could be a decently strong paper if the results come through."
https://openreview.net/forum?id=BJVEEF9lx,"The paper presents a framework to formulate data-structures in a learnable way. It is an interesting and novel approach that could generalize well to interesting datastructures and algorithms. In its current state (Revision of Dec. 9th), there are two strong weaknesses remaining: analysis of related work, and experimental evidence.----------------Reviewer 2 detailed some of the related work already, and especially DeepMind (which I am not affiliated with) presented some interesting and highly related results with its neural touring machine and following work. While it may be of course very hard to make direct comparisons in the experimental section due to complexity of the re-implementation, it would at least be very important to mention and compare to these works conceptually.----------------The experimental section shows mostly qualitative results, that do not (fully) conclusively treat the topic. Some suggestions for improvements:--------* It would be highly interesting to learn about the accuracy of the stack and queue structures, for increasing numbers of elements to store.--------* Can a queue / stack be used in arbitrary situations of push-pop operations occuring, even though it was only trained solely with consecutive pushes / consecutive pops? Does it in this enhanced setting `diverge' at some point?--------* The encoded elements from MNIST, even though in a 28x28 (binary?) space, are elements of a ten-element set, and can hence be encoded a lot more efficiently just by `parsing' them, which CNNs can do quite well. Is the NN `just' learning to do that? If so, its performance can be expected to strongly degrade when having to learn to stack more than 28*28/4=196 numbers (in case of an optimal parser and loss-less encoding). To argue more in this direction, experiments would be needed with an increasing number of stack / queue elements. Experimenting with an MNIST parsing NN in front of the actual stack/queue network could help strengthening or falsifying the claim.--------* The claims about `mental representations' have very little support throughout the paper. If indication for correspondence to mental models, etc., could be found, it would allow to hold the claim. Otherwise, I would remove it from the paper and focus on the NN aspects and maybe mention mental models as motivation.","The consensus of the reviewers, although their reviews where somewhat succinct, was that the paper proposes an interesting research direction by training neural networks to approximate datastructures by constraining them to (attempt to) respect the axioms of the structure, but is thin on the ground in terms of evaluation and comparison to existing work in the domain (both in terms of models and ""standard"" experiments""). The authors have not sought to defend their paper against the reviewers' critique, and thus I am happy to accept the consensus and reject the paper."
https://openreview.net/forum?id=SypU81Ole,"This paper proposed a set of different things under the name of ""sampling generative models"", focusing on analyzing the learned latent space and synthesizing desirable output images with certain properties for GANs.  This paper does not have one single clear message or idea, but rather proposed a set of techniques that seem to produce visually good looking results.  While this paper has some interesting ideas, it also has a number of problems.----------------The spherical interpolation idea is interesting, but after a second thought this does not make much sense.  The proposed slerp interpolation equation (page 2) implicitly assumes that the two points q1 and q2 lie on the same sphere, in which case the parameter theta is the angle corresponding to the great arc connecting the two points on the sphere.  However, the latent space of a GAN, no matter trained with a uniform distribution or a Gaussian distribution, is not a distribution on a sphere, and many points have different distances to the origin.  The author's justification for this comes from the well known fact that in high dimensional space, even with a uniform distribution most points lie on a thin shell in the unit cube.  This is true because in high-dimensional space, the outer shell takes up most of the volume in space, and the inner part takes only a very small fraction of the space, in terms of volume.  This does not mean the density of data in the outer shell is greater than the inner part, though.  In a uniform distribution, the data density should be equal everywhere, a point on the outer shell is not more likely than a point in the inner part.  Under a Gaussian model, the data density is on the other hand higher in the center and much lower on the out side.  If we have a good model of data, then sampling the most likely points from the model should give us plausible looking samples.  In this sense, spherical interpolation should do no better than the normally used linear interpolation.  From the questions and answers it seems that the author does not recognize this distinction.  The results shown in this paper seem to indicate that spherical interpolation is better visually, but it is rather hard to make any concrete conclusions from three pairs of examples.  If this is really the case then there must be something else wrong about our understanding of the learned model.----------------Aside from these, the J-diagram and the nearest neighbor latent space traversal both seems to be good ways to explore the latent space of a learned model.  The attribute vector section on transforming images to new ones with desired attributes is also interesting, and it provides a few new ways to make the GAN latent space more interpretable.----------------Overall I feel most of the techniques proposed in this paper are nice visualization tools.  The contributions however, are mostly on the design of the visualizations, and not much on the technical and model side.  The spherical interpolation provides the only mathematical equation in the paper, yet the correctness of the technique is arguable.  For the visualization tools, there are also no quantitative evaluation, maybe these results are more art than science.","This paper proposes some interesting ideas about visualizing latent-variable models. The paper is nicely written and presented, but the originality and importance of the work isn't enough. Also, neither the reviewers nor I were convinced that spherical interpolation makes more sense than linear interpolation."
https://openreview.net/forum?id=r1LXit5ee,"This work introduces some StarCraft micro-management tasks (controlling individual units during a battle). These tasks are difficult for recent DeepRL methods due to high-dimensional, variable action spaces (the action space is the task of each unit, the number of units may vary). In such large action spaces, simple exploration strategies (such as epsilon-greedy) perform poorly.----------------They introduce a novel algorithm ZO to tackle this problem. This algorithm combines ideas from policy gradient, deep networks trained with backpropagation for state embedding and gradient free optimization. The algorithm is well explained and is compared to some existing baselines. Due to the gradient free optimization providing for much better structured exploration, it performs far better.----------------This is a well-written paper and a novel algorithm which is applied to a very relevant problem. After the success of DeepRL approaches at learning in large state spaces such as visual environment, there is significant interest in applying RL to more structured state and action spaces. The tasks introduced here are interesting environments for these sorts of problems.----------------It would be helpful if the authors were able to share the source code / specifications for their tasks, to allow other groups to compare against this work.----------------I found section 5 (the details of the raw inputs and feature encodings) somewhat difficult to understand. In addition to clarifying, the authors might wish to consider whether they could provide the source code to their algorithm or at least the encoder to allow careful comparisons by other work.----------------Although discussed, there is no baseline comparison with valued based approaches with attempt to do better exploration by modeling uncertainty (such as Bootstrapped DQN). It would useful to understand how such approaches, which also promise better exploration, compare.----------------It would also be interesting to discuss whether action embedding models such as energy-based approaches (e.g. https://arxiv.org/pdf/1512.07679v2.pdf, http://www.jmlr.org/papers/volume5/sallans04a/sallans04a.pdf ) or continuous action embeddings (https://arxiv.org/pdf/1512.07679v2.pdf ) would provide an alternative approach for structured exploration in these action spaces.","The paper presents an approach to structured exploration in StarCraft micromanagement policies (essentially small tasks in the game). From an application standpoint, this is a notable advance, since the authors are tackling a challenging domain and in doing so develop a novel exploration algorithm that seems to work quite well here.    The main downside of the paper is that the proposed algorithm does seem fairly ad-hoc: certain approximations in the algorithm, like ignoring the argmax term in computing the backprop, are not really justified except in that it ""seems to work well in practice"" (a quote from the paper), so it's really unclear whether these represent general techniques or just a method that happens to work well on the target domain for poorly understood reasons.    Despite this, however, I think the strength of the application is sufficient here. Deep learning has been alternatively pushed forward by more algorithmic/mathematical advances and more applied advances, with many of the major breakthroughs coming from seemingly ad-hoc strategies applied to challenging problems. This paper falls in that later category: the ZO algorithm may or may not lead to something slightly more disciplined in the future, but for now the compelling results on StarCraft are I believe enough to warrant accepting the paper.    Pros:  + Substantial performance improvement (over basic techniques like Q-learning) on a challenging task  + Nice intuitive justification of a new exploration approach    Cons:  - Proposed algorithm seems rather ad-hoc, making some (admittedly not theoretically justified) approximations simply because they seem to work well in practice"
https://openreview.net/forum?id=Sk2Im59ex,"Update: thank you for running more experiments, and add more explanations in the manuscript. They addressed most of my concerns, so I updated the score accordingly. ------------------------The work aims at learning a generative function G that can maps input from source domain to the target domain, such that a given representation function f remain unchanged accepting inputs from either domain. The criteria is termed f-constancy. The proposed method is evaluated on two visual domain adaptation tasks. ----------------The paper is relatively easy to follow, and the authors provided quite extensive experimental results on the two datasets. f-constancy is the main novelty of the work. ----------------It seems counter-intuitive to force the function G to be of g o f, i.e., starting from a restricted function f which might have already lost information. As in the face dataset, f is learned to optimize the performance of certain task on some external dataset. It is not clear if an input from the source or target domain can be recovered from applying G as in equation (5) and (6). Also, the f function is learned with a particular task in mind. As in the two experiments, the representation function f is learned to identify the digits in the source SVHN dataset or the identity of some face dataset.  As a result, the procedure has to be repeated if we were to perform domain adaptation for the same domains but for different tasks, such as recognizing expressions instead of identity. ----------------Do the authors have insight on why the baseline method proposed in equation (1) and (2) perform so poorly?----------------Figure 5 shows some visual comparison between style transfer and the proposed method. It is not clear though which method is better. Will it be possible to apply style transfer to generate emojis from photos and repeat the experiments shown in table 4?","In section 5.1, you said that you employ the extra training split of SVHN for two purposes: learning the function f and as a unsupervised training set s. So can I say that, in Table 2, your results are based on SVHN 'extra' training split as s domain traning set? But it is different from other methods since SVHN dataset includes train set, test set and extra training split. As far as I know, other methods are using train set as the s domain unsupervised tranining data. "
https://openreview.net/forum?id=HJ0UKP9ge,"This is a solid paper with good results. However, there aren't many very interesting takeaways (most of the architecture seems a concatenation of standard elements to do well in the leaderboard) and some issues in the writing.----------------The second paragraph of the introduction is very confusing. It's clear the authors got really deep into their world and made no attempts to actually clearly explain their model, not even to an expert in the field, let a lone somebody who isn't familiar with similar approaches.--------The authors keep referring to ""previously popular attention paradigms"" without any citation and then, I believe, incorrectly describe whatever those are supposed to be by writing that these unknown but popular approaches ""summarize each modality into a single vector."" That's one of the most incorrect descriptions I've yet seen for attention mechanisms. First, I don't know what model works over several modalities in a single attention pass. Maybe the authors don't know what a ""modality"" is? More importantly, the whole point of most attention mechanisms is that one does not simply summarize the whole input but instead can access all elements of it. So, this paper's supposedly new way of using attention is pretty much exactly the standard way.--------Both modeling and modelling spellings are in the text.--------I understand the need to sometimes invent new terminology to describe a model but in this paragraph, the authors 3 times talk about a ""modeling layer (RNN)""... It's just an RNN, you don't need to give an RNN another name, especially one that's as nondescript as ""modeling layer"" all layers are part of a model? --------Typo: ""let's the modeling (RNN) layer to learn""--------This paragraph is supposed to give an overview of the model but just confuses readers.--------I would delete it.----------------""Phrase embedding layer"" -- terrible word choice as you are not embedding phrases here. It's a standard bidirectional LSTM over words, not phrases. In all subsequent parts of the paper you just give examples of words embedded in context. No phrases. Please change this to ""contextual word embedding layer"" or something less incorrect.--------Your phrase layer embeddings only show single words, as expected in Table 2.--------Section 2: point 4. Second sentence needs citations for ""popular"" --------Typo: ""from both *of* the context and query word""--------Typo: ""aveaged""----------------It seems like your output layer changes quite substantially so your claim in the abstract/intro of using the same model isn't quite accurate. I'd say you're changing one module or part of your model.--------Section 4: attention isn't countable (no ""a"" in front of ""huge attention""). Also, academic writing usually doesn't include such adjectives in the first place. ","The authors are doing some operations but I cannot understand the intuitions (or precisely causes) behind them. Are those experiments-backed  or have solid theoretical reasoning ?  To begin with, I think it could have been made more clear behind the choice of similarity functions used - alpha and beta (using paper notations), mostly the choice of operands inside concatenations. For eg. while computing alpha, what might be the need of concatenating h and u when hadamard product of h and u alone might do the job. Moreover, while computing G, what is the reason behind using contextual embedding H and why U~ and H~ being attended embedding are not enough ?  Also, in the output layer, to predict the start index the probability distributions are computed using the concatenation of G and M. But why M is passed through a bi-LSTM to get M^2 and then used G and M^2 to predict the end index. I mean what is the intuition behind the bi-LSTM here.   However, I found the Modeling Layer quite intuitive -- capturing interaction between context words which are already query-aware from the previous Attention Flow Layer!"
https://openreview.net/forum?id=H12GRgcxg,"This paper looks at how to train if there are significant label noise present.--------This is a good paper where two main methods are proposed, the first one is a latent variable model and training would require the EM algorithm, alternating between estimating the true label and maximizing the parameters given a true label.----------------The second directly integrates out the true label and simply optimizes the p(z|x).----------------Pros: the paper examines a training scenario which is a real concern for big dataset which are not carefully annotated.--------Cons: the results on mnist is all synthetic and it's hard to tell if this would translate to a win on real datasets.----------------- comments:--------Equation 11 should be expensive, what happens if you are training on imagenet with 1000 classes?--------It would be nice to see how well you can recover the corrupting distribution parameter using either the EM or the integration method. ----------------Overall, this is an OK paper. However, the ideas are not novel as previous cited papers have tried to handle noise in the labels. I think the authors can make the paper better by either demonstrating state-of-the-art results on a dataset known to have label noise, or demonstrate that a method can reliably estimate the true label corrupting probabilities.","Reviewers agreed that the problem was important and the method was interesting and novel. The main (shared) concerns were preliminary nature of the experiments and questions around scalability to more classes.     During the discussion phase, the authors provided additional CIFAR-100 results and introduced a new approximate but scalable method for performing inference. I engaged the reviewers in discussion, who were originally borderline, to see what they thought about the changes. R2 championed the paper, stating that the additional experiments and response re: scalability were an improvement. On the balance, I think the paper is a poster accept."
https://openreview.net/forum?id=HJGwcKclx,"The authors propose a method to compress neural networks by retraining them while putting a mixture of Gaussians prior on the weights with learned means and variances which then can be used to compress the neural network by first setting all weights to the mean of their infered mixture component (resulting in a possible loss of precision) and storing the network in a format which saves only the fixture index and exploits the sparseness of the weights that was enforced in training.----------------Quality:--------Of course it is a serious drawback that the method doesn't seem to work on VGG which would render the method unusable for production (as it is right now, maybe this can be improved). I guess AlexNet takes too long to process, too, otherwise this might be a very valuable addition.--------In Figure 2 I am noticing two things: On the left, there is a large number of points with improved accuracy which is not the case for LeNet5-Caffe. Is there any intuition for why that's the case? Additionally regarding the spearmint optimization: Do they authors have found any clues about which hyperparameter settings worked well? This might be helpful for other people trying to apply this method.--------I really like Figure 7 in it's latest version.----------------Clarity:--------Especially section 2 on MDL is written very well and gives a nice theoretic introduction. Sections 4, 5 and 6 are very short but seem to contain most relevant information. It might be helpful to have at least some more details about the used models in the paper (maybe the number of layers and the number of parameters).--------In 6.1 the authors claim ""Even though most variances seem to be reasonable small there are some that are large"". From figure 1 this is very hard to assess, especially as the vertical histogram essentially shows only the zero component. It might be helpful to have either a log histogram or separate histograms for each componenent. What are the large points in Figure 2 as opposed to the smaller ones? They seem to have a very good compression/accuracy loss ratio, is that it?--------Some other points are listed below----------------originality: While there has been some work on compressing neural networks by using a reduced number of bits to store the parameters and exploiting sparsity structure, I like the idea to directly learn the quantization by means of a gaussian mixture prior in retraining which seems to be more principled than other approaches----------------significance: The method achievs state-of-the-art performance on the two shown examples on MNIST, however these networks are far from the deep networks used in state-of-the-art models. This obviously is a drawback for the practical usability of the methods and therefor it's significance. If the method could be made to work on more state-of-the-art networks like VGG or ResNet, I would consider this a contribution of high significance.----------------Minor issues:----------------page 1: There seems to be a space in front of the first author's name--------page 3: ""in this scenario, pi_0 may be fixed..."". Missing backslash in TeX?--------page 6: 6.2: two wrong blanks in ""the number of components_, \tau_.""--------page 6, 6.3: ""in experiences with VGG"": In experiments?--------page 12: ""Figure C"": Figure 7?","Along the paper we publish a little tutorial. It contains the basic functionalities.   https://github.com/KarenUllrich/Tutorial-SoftWeightSharingForNNCompression/blob/master/tutorial.ipynb  Kind regards, Karen"
