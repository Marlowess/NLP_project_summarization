,Unnamed: 0.4,Unnamed: 0.3,Unnamed: 0.2,Unnamed: 0.1,Unnamed: 0,id,summary,text,SHMetric/Comprehensible/proba_1,SHMetric/Comprehensible/proba_0,SHMetric/Comprehensible/guess,SHMetric/Repetition/proba_1,SHMetric/Repetition/proba_0,SHMetric/Repetition/guess,SHMetric/Grammar/proba_1,SHMetric/Grammar/proba_0,SHMetric/Grammar/guess,SHMetric/Attribution/proba_1,SHMetric/Attribution/proba_0,SHMetric/Attribution/guess,SHMetric/Main ideas/proba_1,SHMetric/Main ideas/proba_0,SHMetric/Main ideas/guess,SHMetric/Conciseness/proba_1,SHMetric/Conciseness/proba_0,SHMetric/Conciseness/guess
0,0,0,0,0,0,https://openreview.net/forum?id=B16dGcqlx,"---- The presented experimental evaluation somewhat hides the cost of TRPO training with the obtained reward function..---3)  Reinforcement learning using agent A data, and apply the policy on agent A.  I expect this might do better than 3rd person imitation learning but it might depend on the scenario (e.g..---2)  Standard 1st person imitation learning using agent A data, then apply the policy on agent B.

This paper proposed a novel adversarial framework to train a model from demonstrations in a third-person perspective, to perform the task in the first-person view..Here the adversarial training is used to extract a novice-expert (or third-person/first-person) independent feature so that the agent can use to perform the same policy in a different view point..-While the idea is quite elegant and novel (I enjoy reading it), more experiments are needed to justify the approach.","['This paper proposed a novel adversarial framework to train a model from demonstrations in a third-person perspective, to perform the task in the first-person view. Here the adversarial training is used to extract a novice-expert (or third-person/first-person) independent feature so that the agent can use to perform the same policy in a different view point.----------------While the idea is quite elegant and novel (I enjoy reading it), more experiments are needed to justify the approach. Probably the most important issue is that there is no baseline, e.g., what if we train the model with the image from the same viewpoint? It should be better than the proposed approach but how close are they? How the performance changes when we gradually change the viewpoint from third-person to first-person? Another important question is that maybe the network just blindly remembers the policy, in this case, the extracted feature could be artifacts of the input image that implicitly counts the time tick in some way (and thus domain-agonistic), but can still perform reasonable policy. Since the experiments are conduct in a synthetic environment, this might happen. An easy check is to run the algorithm on multiple viewpoint and/or with blurred/differently rendered images, and/or with random initial conditions.----------------Other ablation analysis is also needed. For example, I am not fully convinced by the gradient flipping trick used in Eqn. 5, and in the experiments there is no ablation analysis for that (GAN/EM style training versus gradient flipping trick). For the experiments, Fig. 4,5,6 does not have error bars and is not very convincing.', 'The paper presents an interesting new problem setup for imitation learning: an agent tries to imitate a trajectory demonstrated by an expert but said trajectory is demonstrated in a different state or observation space than the one accessible by the agent (although the dynamics of the underlying MDP are shared). The paper proposes a solution strategy that combines recent work on domain confusion losses with a recent IRL method based on generative adversarial networks.----------------I believe the general problem to be relevant and agree with the authors that it results in a more natural formulation for imitation learning that might be more widely applicable.--------There are however a few issues with the paper in its current state that make the paper fall short of being a great exploration of a novel idea. I will list these concerns in the following (in arbitrary order)--------- The paper feels at times to be a bit hurriedly written (this also mainly manifests itself in the experiments, see comment below) and makes a few fairly strong claims in the introduction that in my opinion are not backed up by their approach. For example: ""Advancements in this class of algorithms would significantly improve the state of robotics, because it will enable anyone to easily teach robots new skills""; given that the current method to my understanding has the same issues that come with standard GAN training (e.g. instability etc.) and requires a very accurate simulator to work well (since TRPO will require a large number of simulated trajectories in each step) this seems like an overstatement.--------  There are some sentences that are ungrammatical or switch tense in the middle of the sentence making the paper harder to read than necessary, e.g. Page 2: ""we find that this simple approach has been able to solve the problems""--------- The general idea of third person imitation learning is nice, clear and (at least to my understanding) also novel. However, instead of exploring how to generally adapt current IRL algorithms to this setting the authors pick a specific approach that they find promising (using GANs for IRL) and extend it. A significant amount of time is then spent on explaining why current IRL algorithms will fail in the third-person setting. I fail to see why the situation for the GAN based approach is any different than that of any other existing IRL algorithm. To be more clear: I see no reason why e.g. behavioral cloning could not be extended with a domain confusion loss in exactly the same way as the approach presented. To this end it would have been nice to rather discuss which algorithms can be adapted in the same way (and also test them) and which ones cannot. One straightforward approach to apply any IRL algorithm would for example be to train two autoencoders for both domains that share higher layers + a domain confusion loss on the highest layer, should that not result in features that are directly usable? If not, why?--------- While the general argument that existing IRL algorithms will fail in the proposed setting seems reasonable it is still unfortunate that no attempts have been made to validate this empirically. No comparison is made regarding what happens when one e.g. performs supervised learning (behavioral cloning) using the expert observations and then transfers to the changed domain. How well would this work in practice ? Also, how fast can different IRL algorithms solve the target task in general (assuming a first person perspective) ?--------- Although I like the idea of presenting the experiments as being directed towards answering a specific set of questions I feel like the posed questions somewhat distract from the main theme of the paper. Question 2 suddenly makes the use of additional velocity information to be a main point of importance and the experiments regarding Question 3 in the end only contain evaluations regarding two hyperparameters (ignoring all other parameters such as the parameters for TRPO, the number of rollouts per iteration, the number of presented expert episodes and  the design choices for the GAN). I understand that not all of these can be evaluated thoroughly in a conference paper but I feel like some more experiments or at least some discussion would have helped here.--------- The presented experimental evaluation somewhat hides the cost of TRPO training with the obtained reward function. How many roll-outs are necessary in each step?--------- The experiments lack some details: How are the expert trajectories obtained? The domains for the pendulum experiment seem identical except for coloring of the pole, is that correct (I am surprised this small change seems to have such a detrimental effect)? Figure 3 shows average performance over 5 trials, what about Figure 5 (if this is also average performance, what is the variance here)? Given that GANs are not easy to train, how often does the training fail/were you able to re-use the hyperparameters across all experiments?----------------UPDATE:--------I updated the score. Please see my response to the rebuttal below.', 'The paper extends the imitation learning paradigm to the case where the demonstrator and learner have different points of view. This is an important contribution, with several good applications.  The main insight is to use adversarial training to learn a policy that is robust to this difference in perspective.  This problem formulation is quite novel compared to the standard imitation learning literature (usually first-order perspective), though has close links to the literature on transfer learning (as explained in Sec.2).----------------The basic approach is clearly explained, and follows quite readily from recent literature on imitation learning and adversarial training.----------------I would have expected to see comparison to the following methods added to Figure 3:--------1)  Standard 1st person imitation learning using agent A data, and apply the policy on agent A.  This is an upper-bound on how well you can expect to do, since you have the correct perspective.--------2)  Standard 1st person imitation learning using agent A data, then apply the policy on agent B.  Here, I expect it might do less well than 3rd person learning, but worth checking to be sure, and showing what is the gap in performance.--------3)  Reinforcement learning using agent A data, and apply the policy on agent A.  I expect this might do better than 3rd person imitation learning but it might depend on the scenario (e.g. difficulty of imitation vs exploration; how different are the points of view between the agents). I understand this is how the expert data is collected for the demonstrator, but I don’t see the performance results from just using this procedure on the learner (to compare to Fig.3 results).----------------Including these results would in my view significantly enhance the impact of the paper.']",0.85400390625,0.1461181640625,1,0.94873046875,0.05145263671875,1,0.2091064453125,0.791015625,0,0.88037109375,0.1195068359375,1,0.08416748046875,0.916015625,0,0.457763671875,0.5419921875,0
1,1,1,1,1,1,https://openreview.net/forum?id=B184E5qee,"-The main contribution of this paper is the observation that simply using the hidden states h_i as keys for words x_i, and h_t as the query vector, naturally gives a lookup mechanism that works fine without tuning by backprop..---Experiments on PTB, Wikitext, and LAMBADA datasets show that the cache model improves over standard LSTM language model..---While it is obvious from the results that adding a cache component improves over language models without memory, it is still unclear that this is the best way to do it (instead of, e.g., using pointer networks).

The authors present a simple method to affix a cache to neural language models, which provides in effect a copying mechanism from recently used words..Unlike much related work in neural networks with copying mechanisms, this mechanism need not be trained with long-term backpropagation, which makes it efficient and scalable to much larger cache sizes..This is a simple observation and might already exist as folk knowledge among some people, but it has nice implications for scalability and the experiments are convincing.","['The authors present a simple method to affix a cache to neural language models, which provides in effect a copying mechanism from recently used words. Unlike much related work in neural networks with copying mechanisms, this mechanism need not be trained with long-term backpropagation, which makes it efficient and scalable to much larger cache sizes. They demonstrate good improvements on language modeling by adding this cache to RNN baselines.----------------The main contribution of this paper is the observation that simply using the hidden states h_i as keys for words x_i, and h_t as the query vector, naturally gives a lookup mechanism that works fine without tuning by backprop. This is a simple observation and might already exist as folk knowledge among some people, but it has nice implications for scalability and the experiments are convincing.----------------The basic idea of repurposing locally-learned representations for large-scale attention where backprop would normally be prohibitively expensive is an interesting one, and could probably be used to improve other types of memory networks.----------------My main criticism of this work is its simplicity and incrementality when compared to previously existing literature. As a simple modification of existing NLP models, but with good empirical success, simplicity and practicality, it is probably more suitable for an NLP-specific conference. However, I think that approaches that distill recent work into a simple, efficient, applicable form should be rewarded and that this tool will be useful to a large enough portion of the ICLR community to recommend its publication.', 'This paper proposes a simple extension to a neural network language model by adding a cache component. --------The model stores <previous hidden state, word> pairs in memory cells and uses the current hidden state to control the lookup. --------The final probability of a word is a linear interpolation between a standard language model and the cache language model. --------Additionally, an alternative that uses global normalization instead of linear interpolation is also presented. --------Experiments on PTB, Wikitext, and LAMBADA datasets show that the cache model improves over standard LSTM language model.----------------There is a lot of similar work on memory-augmented/pointer neural language models, and the main difference is that the proposed method is simple and scales to a large cache size.--------However, since the technical contribution is rather limited, the experiments need to be more thorough and conclusive. --------While it is obvious from the results that adding a cache component improves over language models without memory, it is still unclear that this is the best way to do it (instead of, e.g., using pointer networks). --------A side-by-side comparison of models with pointer networks vs. models with cache with roughly the same number of parameters is needed to convincingly argue that the proposed method is a better alternative (either because it achieves lower perplexity, faster to train but similar test perplexity, faster at test time, etc.)----------------Some questions:--------- In the experiment results, for your neural cache model, are those results with linear interpolation or global normalization, or the best model? Can you show results for both? --------- Why is the neural cache model worse than LSTM on Ctrl (Lambada dataset)? Please also show accuracy on this dataset. --------- It is also interesting that the authors mentioned that training the cache component instead of only using it at test time gives little improvements. Are the results about the same or worse?', 'This paper not only shows that a cache model on top of a pre-trained RNN can improve language modeling, but also illustrates a shortcoming of standard RNN models in that they are unable to capture this information themselves. Regardless of whether this is due to the small BPTT window (35 is standard) or an issue with the capability of the RNN itself, this is a useful insight. This technique is an interesting variation of memory augmented neural networks with a number of advantages to many of the standard memory augmented architectures.----------------They illustrate the neural cache model on not just the Penn Treebank but also WikiText-2 and WikiText-103, two datasets specifically tailored to illustrating long term dependencies with a more realistic vocabulary size. I have not seen the ability to refer up to 2000 words back previously.--------I recommend this paper be accepted. There is additionally extensive analysis of the hyperparameters on these datasets, providing further insight.----------------I recommend this interesting and well analyzed paper be accepted.']",0.8564453125,0.1435546875,1,0.7744140625,0.2257080078125,1,0.2091064453125,0.791015625,0,0.92578125,0.07440185546875,1,0.10882568359375,0.89111328125,0,0.36669921875,0.63330078125,0
2,2,2,2,2,2,https://openreview.net/forum?id=B1E7Pwqgl,"-PROS:
---+ Interesting and novel idea
---CONS:
---- Improper experimental protocols
---- Missing baselines
---- Missing diagnostic experiments


-[R1] Heess, N., Williams, C. K. I., and Hinton, G. E. (2009)..-The above idea is interesting and novel, but unfortunately is not sufficiently validated by the experimental results..For example, the authors fail to specify the exact form of the energy function: this seems like a glaring omission.

“the generator”..There are comparisons provided for face completion experiments - but even there comparisons with descriptor or generator network trained separately or with other deep auto-encoders are missing..---- For most of the experiments presented in the paper it is hard to assess the specific value brought by the proposed cooperative training approach because baseline results are missing.","[""The authors proposes an interesting idea of connecting the energy-based model (descriptor) and --------the generator network to help each other. The samples from the generator are used as the initialization --------of the descriptor inference. And the revised samples from the descriptor is in turn used to update--------the generator as the target image. ----------------The proposed idea is interesting. However, I think the main flaw is that the advantages of having that --------architecture are not convincingly demonstrated in the experiments. For example, readers will expect --------quantative analysis on how initializing with the samples from the generator helps? Also, the only --------quantative experiment on the reconstruction is also compared to quite old models. Considering that --------the model is quite close to the model of Kim & Bengio 2016, readers would also expect a comparison --------to that model. ----------------** Minor--------- I'm wondering if the analysis on the convergence is sound when considering the fact that samples --------from SGLD are biased samples (with fixed step size). --------- Can you explain a bit more on how you get Eqn 8? when p(x|y) is also dependent on W_G?"", 'This paper introduces CoopNets, an algorithm which trains a Deep-Energy Model (DEM, the “descriptor”) with the help of an auxiliary directed bayes net, e.g. “the generator”. The descriptor is trained via standard maximum likelihood, with Langevin MCMC for sampling. The generator is trained to generate likely samples under the DEM in a single, feed-forward ancestral sampling step. It can thus be used to shortcut expensive MCMC sampling, hence the reference to “cooperative training”.----------------The above idea is interesting and novel, but unfortunately is not sufficiently validated by the experimental results. First and foremost, two out of the three experiments do not feature a train /test split, and ignore standard training and evaluation protocols for texture generation (see [R1]). Datasets are also much too small. As such these experiments only seem to confirm the ability of the model to overfit. On the third in-painting tasks, baselines are almost non-existent: no VAEs, RBMs, DEM, etc which makes it difficult to evaluate the benefits of the proposed approach.----------------In a future revision, I would also encourage the authors to answer the following questions experimentally. What is the impact of the missing rejection step in Langevin MCMC (train with, without ?). What is the impact of the generator on the burn-in process of the Markov chain (show sample auto-correlation). How bad is approximation of training the generator from ({\\tilde{Y}, \\hat{X}) instead of ({\\tilde{Y}, \\tilde{X}) ? Run comparative experiments.----------------The paper would also greatly benefit from a rewrite focusing on clarity, instead of hyperbole (“pioneering work” in reference to closely related, but non-peer reviewed work) and prose (“tale of two nets”). For example, the authors fail to specify the exact form of the energy function: this seems like a glaring omission.----------------PROS:--------+ Interesting and novel idea--------CONS:--------- Improper experimental protocols--------- Missing baselines--------- Missing diagnostic experiments----------------[R1] Heess, N., Williams, C. K. I., and Hinton, G. E. (2009). Learning generative texture models with extended fields of-experts.', 'This paper proposed a new joint training scheme for two probabilistic models of signals (e.g. images) which are both deep neural network based and are termed generator and descriptor networks.  In the new scheme, termed cooperative training, the two networks train together and assist each other: the generator network provides samples that work as initial samples for the descriptor network, and the descriptor network updates those samples to help guide training of the generator network.----------------This is an interesting approach for coupling the training of these two models.  The paper however is quite weak on the empirical studies.  In particular:--------- The training datasets are tiny, from sets of 1 image to 5-6.  What is the reason for not using larger sets?  I think the small datasets are leading to over training and are really masking the true value of the proposed cooperative training approach.--------- For most of the experiments presented in the paper it is hard to assess the specific value brought by the proposed cooperative training approach because baseline results are missing.  There are comparisons provided for face completion experiments - but even there comparisons with descriptor or generator network trained separately or with other deep auto-encoders are missing.  Thus it is hard to conclude if and how much gain is obtained by cooperative training over say individually training the descriptor and generator networks.----------------Another comment is that in the “related work” section, I think relation with variational auto encoders (Kingma and Welling 2013) should be included.----------------Despite limitations mentioned above, I think the ideas presented in the paper are intuitively appealing and worth discussing at ICLR.  Paper would be considerably strengthened by adding more relevant baselines and addressing the training data size issues.']",0.7392578125,0.260986328125,1,0.90185546875,0.09814453125,1,0.1497802734375,0.85009765625,0,0.88623046875,0.1138916015625,1,0.1392822265625,0.86083984375,0,0.424560546875,0.5751953125,0
3,3,3,3,3,3,https://openreview.net/forum?id=B1TTpYKgx,"-“Comparing architectures in such a fashion limits the generality of the conclusions”


-To my knowledge much of the previous work has focused on mathematical proof, and has led to very general conclusions on the representative power of deep networks (one example being Leroux et al again)..seems T_{d+1} for you is a random variable and t_{d} is fixed..-- On page 2 one finds the statement ``We discover and prove the underlying reason for this – all three measures are directly proportional to a fourth quantity, trajectory length.''

both cases should be studied, to see the tradeoff between contraction and selectivity to the class label..or different label?.-- In these plots the 2 mnist points had same label ?","['This paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks.----------------Random networks (deep networks with random Gaussian weights, hard tanh or ReLU activation) are studied according to several criterions: number of neutron transitions, activation patterns, dichotomies and trajectory length.----------------There doesn\'t seem to be a solid justification for why the newly introduced measures of expressivity really measure expressivity.--------For instance the trajectory length seems a very discutable measure of expressivity. The only justification given for why it should be a good measure of expressivity is proportionality with other measures of expressivity in the specific case of random networks.----------------The paper is too obscure and too long. The work may have some interesting ideas but it does not seem to be properly replaced in context.----------------Some findings seem trivial.----------------detailed comments----------------p2 ----------------""Much of the work examining achievable functions relies on unrealistic architectural assumptions such as layers being exponentially wide""----------------I don’t think so. In ""Deep Belief Networks are Compact Universal Approximators"" by Leroux et al., proof is given that deep but narrow feed-forward neural networks with sigmoidal units can represent any Boolean expression i.e. A neural network with 2n−1 + 1 layers of n units (with n the number of input neutron).----------------“Comparing architectures in such a fashion limits the generality of the conclusions”----------------To my knowledge much of the previous work has focused on mathematical proof, and has led to very general conclusions on the representative power of deep networks (one example being Leroux et al again).----------------It is much harder to generalise the approach you propose, based on random networks which are not used in practice.----------------“[we study] a family of networks arising in practice: the behaviour of networks after random initialisation”----------------These networks arise in practice as an intermediate step that is not used to perform computations; this means that the representative power of such intermediate networks is a priori irrelevant. You would need to justify why it is not.----------------“results on random networks provide natural baselines to compare trained networks with”----------------random networks are not “natural” for the study of expressivity of deep networks. It is not clear how the representative power of random networks (what kind of random networks seems an important question here) is linked to the representative power of (i) of the whole class of networks or (ii) the class of networks after training. Those two classes of networks are the ones we would a priori care about and you would need to justify why the study of random networks helps in understanding either (i) or (ii).----------------p5----------------“As FW is a random neural network […] it would suggest that points far enough away from each other would have independent signs, i.e. a direct proportionality between the length of z(n)(t) and the number of times it crosses the decision boundary.”----------------As you say, it seems that proportionality of the two measures depends on the network being random. This seems to invalidate generalisation to other networks, i.e. if the networks are not random, one would assume that path lengths are not proportional.----------------p6----------------the expressivity w.r.t. remaining depth seems a trivial concerns, completely equivalent to the expressivity w.r.t. depth. This makes the remark in figure 5 that the number of achievable dichotomies only depends *only* on the number of layers above the layer swept seem trivial----------------p7----------------in figure 6 a network width of 100 for MNIST seems much too small. Accordingly performance is very poor and it is difficult to generalise the results to relevant situations.', ""SUMMARY --------This paper studies the expressive power of deep neural networks under various related measures of expressivity. --------It discusses how these measures relate to the `trajectory length', which is shown to depend exponentially on the depth of the network, in expectation (at least experimentally, at an intuitive level, or theoretically under certain assumptions). --------The paper also emphasises the importance of the weights in the earlier layers of the network, as these have a larger influence on the represented classes of functions, and demonstrates this in an experimental setting. ----------------PROS --------The paper further advances on topics related to the expressive power of feedforward neural networks with piecewise linear activation functions, in particular elaborating on the relations between various points of view. ----------------CONS --------The paper further advances and elaborates on interesting topics, but to my appraisal it does not contribute significantly new aspects to the discussion. ----------------COMMENTS--------- The paper is a bit long (especially the appendix) and seems to have been written a bit in a rush. --------Overall the main points are presented clearly, but the results and conclusions could be clearer about the assumptions / experimental vs theoretical nature. --------The connection to previous works could also be clearer. ----------------- On page 2 one finds the statement ``Furthermore, architectures are often compared via ‘hardcoded’ weight values -- a specific function that can be represented efficiently by one architecture is shown to only be inefficiently approximated by another.'' ----------------This is partially true, but it neglects important parts of the discussion conducted in the cited papers. --------In particular, the paper [Montufar, Pascanu, Cho, Bengio 2014] discusses not one hard coded function, but classes of functions with a given number of linear regions. --------That paper shows that deep networks generically* produce functions with at least a given number of linear regions, while shallow networks never do. --------* Generically meaning that, after fixing the number of parameters, any function represented by the network, for parameter values form an open, positive -measure, neighbourhood, belongs to the class of functions which have at least a certain number of linear regions. --------In particular, such statements can be directly interpreted in terms of networks with random weights. ----------------- One of the measures for expressivity discussed in the present paper is the number of Dichotomies. In statistical learning theory, this notion is used to define the VC-dimension. In that context, a high value is associated with a high statistical complexity, meaning that picking a good hypothesis requires more data. ----------------- On page 2 one finds the statement ``We discover and prove the underlying reason for this – all three measures are directly proportional to a fourth quantity, trajectory length.'' --------The expected trajectory length increasing exponentially with depth can be interpreted as the increase (or decrease) in the scale by a composition of the form a*...*a x, which scales the inputs by a^d. Such a scaling by itself certainly is not an underlying cause for an increase in the number of dichotomies or activation patterns or transitions. Here it seems that at least the assumptions on the considered types of trajectories also play an important role. --------This is probably related to another observation from page 4: ``if the variance of the bias is comparatively too large... then we no longer see exponential growth.''----------------OTHER SPECIFIC COMMENTS --------In Theorem 1 --------- Here it would be good to be more specific about ``random neural network'', i.e., fixed connectivity structure with random weights, and also about the kind of one-dimensional trajectory, i.e., finite in length, closed, differentiable almost everywhere, etc. ----------------- The notation ``g \\geq O(f)'' used in the theorem reads literally as |g| \\geq \\leq k |f| for some k>0, for large enough arguments. It could also be read as g being not smaller than some function that is bounded above by f, which holds for instance whenever g\\geq 0. --------For expressing asymptotic lower bounds one can use the notation \\Omega (see https://en.wikipedia.org/wiki/Big_O_notation). ----------------- It would be helpful to mention that the expectation is being taken with respect to the network weights and that these are normally distributed with variance \\sigma. ----------------- Theorem 2. Here it would be good to be more specific about the kind of sign transitions. Is this about transitions at any units of the network, or about sign transitions at the scalar output of the entire network. ----------------- Theorem 3 is quite trivial. --------The bijection between transitions and activation patterns is not clear. --------Take a regular n-gon in the plane and a circle that crosses each edge twice. --------This makes 2n transitions but only n+1 activation patterns. ----------------- Theorem 4. --------Where is the proof of this statement? --------How does this relate to the simple fact that each activation pattern corresponds to the vector indicating the units that are `active'? ------------------------MINOR COMMENTS--------- The names of the theorems (e.g. ``Bound on ...'' in Theorem 1) could be separated more clearly from the statements, for instance using bold font, a dot, or parentheses. --------- On page 4, in Latex one can use \\gg for the `much larger' symbol. --------- On page 4, explain the notation \\delta z_\\orth.  --------- On page 4, explain that ``latent image'' refers to the image in the last layer. --------- Why are there no error bars in Figure 2?  --------- On page 5 explain that the hyperplane is in the last hidden layer. --------- On page 5, ``is transitioning for any input''. This is not clearly stated, since a transition takes place at a point in a trajectory of inputs, not for a single input. --------- The y-axis labels in Figure 1 (c) and (d) are too small. --------- Why are there no error bars in Figure 1 (a) and (b)? The caption could at least mention that shown are the averages over experiments. --------- In Figure 4 (b) the curves are occluded by the labels. --------- The numbering of results is confusing. In the Appendix some numbers are repeated with the main part and some are missing. --------- On page 19. Theorem 6. As far as I remember Stanley also provides an elementary proof of case with hyperplanes in general position. Many other works also provide elementary proofs using the same induction arguments in what is known as the sweep hyperplane method. "", ""Summary of the paper:----------------Authors study in this paper quantities related to the expressivity of neural networks.The analysis is done for a random network. authors define the ‘trajectory length’ of a one dimensional trajectory as the length of the trajectory as the points (in a m- dimensional space) are embedded by layers of the network. They provide growth factors as function of hidden units k, and number of layers d.  the growth factor is exponential in the number of layers. Authors relates this trajectory length to authors quantities : ‘transitions’,’activation patterns ’ and ‘Dichotomies’. --------As a consequence of this study authors suggest that training only  earlier layers in the network  leads higher accuracy then just training later layers. Experiments are presented on MNIST and CIFAR10.----------------Clarity:----------------The  paper is a little hard to follow, since  the motivations are not clear in the introduction and the definitions across the paper are not clear. ----------------Novelty:----------------Studying the trajectory length as function of transforming the data by a multilayer network is   new and interesting idea. The relation to transition numbers is in term of the growth factor, and not as a quantity to quantity relationship. Hence it is hard to understand what are the implications.----------------Significance:----------------The geometry of the input set (of dimension m)  shows up only weakly in the activation patterns analysis.  The trajectory study should tell us how the network organizes the input set. As observed in the experiments the network becomes contractive/selective as we train the network. It would be interesting to study those phenomenas using this trajectory length , as a measure for disentangling nuisance factors ( such as invariances etc.). In the supervised setting the network need not to be contractive every where , so it needs to be selective to the class label, a  theoretical study of the selectivity and contraction using the trajectory length would be more appealing.----------------Detailed comments:----------------Theorem 1:----------------- As raised by reviewer one the definition of a one dimensional input trajectory is missing. --------- What does theorem 1 tells us about the design and the architecture to use in neural networks as promised in the introduction is not clear. The connection to transitions in Theorem 2 is rather weak. ----------------Theorem 2:----------------- in the proof of theorem 2 it not clear what is meant by T and t. Notations are confusing, the expectation is taken with respect to which weight: is it W_{d+1} or (W_{d+1} and W_{d})? I understand you don't want to overload notation but maybe E_{d+1} can help keeping track. I don't see how the recursion is applied if T and t in it, have different definitions. seems T_{d+1} for you is a random variable and t_{d} is fixed. Are you fixing W_d and then looking at W_{d+1} as  random?----------------- In the same proof:  the recursion  is for d>1  ? your analysis is for W \\in R^{k\\times k}, you don't not study the W \\in \\mathbb{R}^{k\\times m}. In this case you can not assume assume that |z^(0)|=1.----------------- should d=1, be analyzed alone to know how it scales with m?----------------Theorem 4 in main text:----------------- Is the proof missing? or Theorem 4 in the main text is Theorem 6 in the appendix?----------------Figures 8 and 9:----------------- the trajectory length reduction in the training isn't that just the network becoming contractive to enable mapping the training points to the labels? See for instance  on contraction in deep networks https://arxiv.org/pdf/1601.04920.pdf----------------- How much the plot depends on the shape of the trajectory? have you tried other then circular trajectory?----------------- In these plots the 2 mnist points had same label ? or different label?  both cases should be studied, to see the tradeoff between contraction and selectivity to the class label.""]",0.414794921875,0.5849609375,0,0.8525390625,0.1475830078125,1,0.11614990234375,0.8837890625,0,0.80810546875,0.1920166015625,1,0.14404296875,0.85595703125,0,0.2027587890625,0.79736328125,0
4,4,4,4,4,4,https://openreview.net/forum?id=B1ckMDqlg,"This paper describes a method for greatly expanding network model size (in terms of number of stored parameters) in the context of a recurrent net, by applying a Mixture of Experts between recurrent net layers that is shared between all time steps..Paper Strengths: 

 Elegant use of MoE for expanding model capacity and enabling training large models necessary for exploiting  very large datasets in a computationally feasible manner


--- The effective batch size for training the MoE drastically increased also


--- Interesting experimental results on the effects of increasing the number of MoEs, which is expected..----Paper Weaknesses:


---- there are many different ways of increasing model capacity to enable the exploitation of very large datasets; it would be very nice to discuss  the use of MoE and other alternatives in terms of computational efficiency and other factors.

The idea is based on using a large mixture of experts (MoE) (i.e..---Additionally, the paper introduces two techniques for increasing the batch-size passed to each expert, and hence maximizing parallelization in GPUs..small networks), where only a few of them are adaptively activated via a gating network.","['This paper proposes a method for significantly increasing the number of parameters in a single layer while keeping computation in par with (or even less than) current SOTA models. The idea is based on using a large mixture of experts (MoE) (i.e. small networks), where only a few of them are adaptively activated via a gating network. While the idea seems intuitive, the main novelty in the paper is in designing the gating network which is encouraged to achieve two objectives: utilizing all available experts (aka importance), and distributing computation fairly across them (aka load). --------Additionally, the paper introduces two techniques for increasing the batch-size passed to each expert, and hence maximizing parallelization in GPUs.--------Experiments applying the proposed approach on RNNs in language modelling task show that it can beat SOTA results with significantly less computation, which is a result of selectively using much more parameters. Results on machine translation show that a model with more than 30x number of parameters can beat SOTA while incurring half of the effective computation.----------------I have the several comments on the paper:--------- I believe that the authors can do a better job in their presentation. The paper currently is at 11 pages (which is too long in my opinion), but I find that Section 3.2 (the crux of the paper) needs better motivation and intuitive explanation. For example, equation 8 deserves more description than currently devoted to it. Additional space can be easily regained by moving details in the experiments section (e.g. architecture and training details) to the appendix for the curious readers. Experiment section can be better organized by finishing on experiment completely before moving to the other one. There are also some glitches in the writing, e.g. the end of Section 3.1. --------- The paper is missing some important references in conditional computation (e.g. https://arxiv.org/pdf/1308.3432.pdf) which deal with very similar issues in deep learning.--------- One very important lesson from the conditional computation literature is that while we can in theory incur much less computation, in practice (especially with the current GPU architectures) the actual time does not match the theory. This can be due to inefficient branching in GPUs. It would be nice if the paper includes a discussion of how their model (and perhaps implementation) deal with this problem, and why it scales well in practice.--------- Table 1 and Table 3 contain repetitive information, and I think they should be combined in one (maybe moving Table 3 to appendix). One thing I do not understand is how does the number of ops/timestep relate to the training time. This also related to the pervious comment.', 'Paper Strengths: ---------- Elegant use of MoE for expanding model capacity and enabling training large models necessary for exploiting  very large datasets in a computationally feasible manner------------------ The effective batch size for training the MoE drastically increased also------------------ Interesting experimental results on the effects of increasing the number of MoEs, which is expected.------------------------Paper Weaknesses:------------------- there are many different ways of increasing model capacity to enable the exploitation of very large datasets; it would be very nice to discuss  the use of MoE and other alternatives in terms of computational efficiency and other factors.', ""This paper describes a method for greatly expanding network model size (in terms of number of stored parameters) in the context of a recurrent net, by applying a Mixture of Experts between recurrent net layers that is shared between all time steps.  By process features from all timesteps at the same time, the effective batch size to the MoE is increased by a factor of the number of steps in the model; thus even for sparsely assigned experts, each expert can be used on a large enough sub-batch of inputs to remain computationally efficient.  Another second technique that redistributes elements within a distributed model is also described, further increasing per-expert batch sizes.----------------Experiments are performed on language modeling and machine translation tasks, showing significant gains by increasing the number of experts, compared to both SoA as well as explicitly computationally-matched baseline systems.----------------An area that falls a bit short is in presenting plots or statistics on the real computational load and system behavior.  While two loss terms were employed to balance the use of experts, these are not explored in the experiments section.  It would have been nice to see the effects of these more, along with the effects of increasing effective batch sizes, e.g. measurements of the losses over the course of training, compared to the counts/histogram distributions of per-expert batch sizes.----------------Overall I think this is a well-described system that achieves good results, using a nifty placement for the MoE that can overcome what otherwise might be a disadvantage for sparse computation.--------------------------------Small comment:--------I like Fig 3, but it's not entirely clear whether datapoints coincide between left and right plots.  The H-H line has 3 points on left but 5 on the right?  Also would be nice if the colors matched between corresponding lines.""]",0.39697265625,0.60302734375,0,0.89404296875,0.10601806640625,1,0.1510009765625,0.84912109375,0,0.89013671875,0.10968017578125,1,0.11724853515625,0.8828125,0,0.34326171875,0.65673828125,0
5,5,5,5,5,5,https://openreview.net/forum?id=BJ3filKll,"-(3) The error bound presented in Section 4 appears vacuous for any practical setting, as the upper bound on the error is exponential in the total curvature (a quantity that will be quite large in most practical settings)..-(2) It is difficult to see how the analysis generalizes to more complex data in which local linearity assumptions on the data manifold are vacuous given the sparsity of data in high-dimensional space, or how it generalizes to deep network architectures that are not pure ReLU networks..-Experiments involve looking at embedding synthetic data from a monotonic chain using a distance preservation loss.

---- On page 6 ``divided into segments'' here `segments' is maybe not the best word..---- ``Color coded'' where the color codes what?.Is the difference squared?","['The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. Specifically, the paper focuses on what the authors call ""monotonic chains of linear segments"", which are essentially sets of intersecting tangent planes. The paper presents a construction that efficiently models such manifolds in a deep net, and presents a basic error analysis of the resulting construction.----------------While the presented results are novel to the best of my knowledge, they are hardly surprising (1) given what we already know about the representational power of deep networks and (2) given that the study selects a deep network architecture and a data structure that are very ""compatible"". In particular, I have three main concerns with respect to the results presented in this paper:----------------(1) In the last decade, there has been quite a bit of work on learning data representations from sets of local tangent planes. Examples that spring to mind are local tangent space analysis of Zhang & Zha (2002), manifold charting by Brand (2002) and alignment of local models by Verbeek, Roweis, and Vlassis (2003). None of this work is referred to in related work, even though it seems highly relevant to the analysis presented here. For instance, it would be interesting to see how these old techniques compare to the deep network trained to produce the embedding of Figure 6. This may provide some insight into the inductive biases the deep net introduces: does it learn better representations that non-parametric techniques because it has better inductive biases, or does it learn worse representations because the loss being optimized is non-convex?----------------(2) It is difficult to see how the analysis generalizes to more complex data in which local linearity assumptions on the data manifold are vacuous given the sparsity of data in high-dimensional space, or how it generalizes to deep network architectures that are not pure ReLU networks. For instance, most modern networks use a variant of batch normalization; this already appears to break the presented analyses.----------------(3) The error bound presented in Section 4 appears vacuous for any practical setting, as the upper bound on the error is exponential in the total curvature (a quantity that will be quite large in most practical settings). This is underlined by the analysis of the Swiss roll dataset, of which the authors state that the ""bound for this case is very loose"". The fact that the bound is already so loose for this arguably very simple manifold makes that the error analysis may tell us very little about the representational power of deep nets.----------------I would encourage the authors to address issue (1) in the revision of the paper. Issue (2) and (3) may be harder to address, but is essential that they are addressed for the line of work pioneered by this paper to have an impact on our understanding of deep learning.------------------------Minor comments: ----------------- In prior work, the authors only refer to fully supervised siamese network approaches. These approaches differ from that taken by the authors, as their approach is unsupervised. It should be noted that the authors are not the first to study unsupervised representation learners parametrized by deep networks: other important examples are deep autoencoders (Hinton & Salakhutdinov, 2006 and work on denoising autoencoders from Bengio\'s group) and parametric t-SNE (van der Maaten, 2009).--------- What loss do the authors use in their experiments? Using ""the difference between the ground truth distance ... and the distance computed by the network"" seems odd, because it encourages the network to produce infinitely large distances (to get a loss of minus infinity). Is the difference squared?', 'Summary:--------In this paper, the authors look at the ability of neural networks to represent low dimensional manifolds efficiently e.g. embed them into a lower dimensional Euclidian space. --------They define a class of manifolds, monotonic chains (affine spaces that intersect, with hyperplanes separating monotonic intervals of spaces) and give a construction to embed such a chain with a neural network with one hidden layer.----------------They also give a bound on the number of parameters required to do so, and examine what happens when the manifold is noisy. ----------------Experiments involve looking at embedding synthetic data from a monotonic chain using a distance preservation loss. This experiment supports the theoretical bound on number of parameters needed to embed the monotonic chain. Another experiment varies the elevation and azimuth of of faces, which are known to lie on a monotonic chain, on a regression loss.----------------Comments:----------------The direction of investigation in the paper (looking at what happens to manifolds in a neural network), is very compelling, and I strongly encourage the authors to continue exploring this direction.----------------However, the current version of the paper could use some more work:----------------The experiments are all with a regression loss and a shallow network, and as part of the reason for interest in this question is the very large, high dimensional datasets we use now, which require a deeper network, it seems important to address this case.----------------It also seems important to confirm that embedding works well when *classification* loss is used, instead of regression----------------The theory sections could do with being more clearly written -- I’m not as familiar with the literature in this area, and while the proof method used is relatively elementary, it was difficult to understand what exactly was being proved -- e.g. formally stating what could be expected of an embedding that “accurately and efficiently” preserves a monotonic chain, etc.', ""SUMMARY --------This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in terms of neural networks with two hidden layers. ----------------PROS --------Interesting, easy to follow view on some of the capabilities of neural networks, highlighting the dimensionality reduction aspect, and pointing at possible directions for further investigation. ----------------CONS --------The paper presents a construction illustrating certain structures that can be captured by a network, but it does not address the learning problem (although it presents experiments where such structures do emerge, more or less). ----------------COMMENTS --------It would be interesting to study the ramifications of the presented observations for the case of deep(er) networks. --------Also, to study to what extent the proposed picture describes the totality of functions that are representable by the networks. ----------------MINOR COMMENTS --------- Figure 1 could be referenced first in the text.  --------- ``Color coded'' where the color codes what? --------- Thank you for thinking about revising the points from my first questions. Note: Isometry on the manifold. --------- On page 5, mention how the orthogonal projection on S_k is realized in the network. --------- On page 6 ``divided into segments'' here `segments' is maybe not the best word. --------- On page 6 ``The mean relative error is 0.98'' what is the baseline here, or what does this number mean? ""]",0.75390625,0.2462158203125,1,0.93212890625,0.06768798828125,1,0.1634521484375,0.83642578125,0,0.82666015625,0.1734619140625,1,0.096435546875,0.90380859375,0,0.1234130859375,0.87646484375,0
6,6,6,6,6,6,https://openreview.net/forum?id=BJ8fyHceg,"-Even though some choices made along the way seem rather ad-hoc and simplistic from a music theoretical perspective, the results sound like an improvement upon the note RNN baseline, but we also don't know how cherry picked these results are..Instead for the comparator techniques it reverts to treating the p(a|s) as a “prior” term rather than a reward term, leaving a bit of a question as to whether DQN is particularly appropriate..-The reinforcement learning methodology is appropriate but straightforward and closely resembles previous work for text modeling and dialogue generation.

Hence the optimal control target under unification is p(b=1|\tau)E_p(A,S) \prod_t \pi(a_t|s_t): i.e..The SOC off policy objective still does still contain the KL term so the approach would still differ from the approach of this paper..I think it would be great if such papers would be written with the help or feedback of people that have real musical training and are more critical towards these details.","[""This paper suggests combining LSTMs, trained on a large midi corpus, with a handcrafted reward function that helps to fine-tune the model in a musically meaningful way. The idea to use hand-crafted rewards in such a way is great and seems promising for practical scenarios, where a musician would like to design a set of rules, rather than a set of melodies.----------------Even though some choices made along the way seem rather ad-hoc and simplistic from a music theoretical perspective, the results sound like an improvement upon the note RNN baseline, but we also don't know how cherry picked these results are. --------I am not convinced that this approach will scale to much more complicated reward functions necessary to compose real music. Maybe LSTMs are the wrong approach altogether if they have so much trouble learning to produce pleasant melodies from such a relatively big corpus of data. Aren't there any alternative differentiable models that are more suitable? What about dilated convolution based approaches?----------------What I don't like about the paper is that the short melodies are referenced as compositions while being very far from meaningful music, they are not even polyphonic after all. I think it would be great if such papers would be written with the help or feedback of people that have real musical training and are more critical towards these details. ----------------What I like about the paper is that the authors make an effort to understand what is going on, table 1 is interesting for instance. However, Figure 3 should have included real melody excerpts with the same sound synthesis/sample setup. Besides that, more discussion on the shortcomings of the presented method should be added.----------------In summary, I do like the paper and idea and I can imagine that such RL based fine-tuning approaches will indeed be useful for musicians. Even though the novelty might be limited, the paper serves as a documentation on how to achieve solid results in practice."", 'The authors propose a solution for the task of synthesizing melodies. The authors claim that the ""language-model""-type approaches with LSTMs generate melodies with certain shortcomings. They tend to lack long-range structure, to repeat notes etc. To solve this problem the authors suggest that the model could be first trained as a pure LM-style LSTM and then trained with reinforcement learning to optimize an objective which includes some non-differentiable music-theory related constraints. ----------------The reinforcement learning methodology is appropriate but straightforward and closely resembles previous work for text modeling and dialogue generation. By itself the methodology doesn\'t offer a new technique. ----------------To me, the paper\'s contribution then comes down to the novelty / utility / impact of the application. The authors clearly put substantial of effort into crafting the rules and user study and that is commendable. On the other hand, music itself is dealt with somewhat naively. While the user study reflects hard work, it seems premature. The semi-plausible piano melodies here are only music in the way that LSTM Shakespeare passes as poetry. So it\'s analogous to conducting a user study comparing LSTM Shakespeare to n-gram Shakespeare. ----------------I\'d caution the author\'s against the uncritical motivation that a problem has previously been studied. Research contains abundant dead ends (not to say this is necessarily one) and the burden to motivate research shouldn\'t be forgotten. This is especially true when the application is the primary thrust of a paper.----------------Generally the authors should be careful about describing this model as ""composing"". By analogy to a Shakespeare-LSTM, the language model is not really composing English prose. The relationship between constructing a statistical sequence model and creating art - an activity that involves communication grounded in real-world semantics should not be overstated. ----------------I appreciate the authors\' efforts to respond to some criticisms of the problem setup and encourage them to anticipate these arguments in the paper and to better motivate the work in the future. If the main contribution is the application (the methods have been used elsewhere), then the motivation is of central importance. I also appreciate their contention that the field benefits from multiple datasets and not simply relying on language modeling. Further, they are correct in asserting that MIDI can capture all the information in a score (not merely ""Gameboy music"", and that for some musics (e.g. European classical) the score is of central importance. However, the authors may overstate the role of a score in jazz music.----------------Overall, for me, the application, while fun, doesn\'t add enough to the impact of the paper. And the methodology, while appropriate, doesn\'t stand on its own. ------------------Update-- Thanks for your modifications and arguments. I\'ve revised my scores to add a point. ', 'This paper uses a combination of likelihood and reward based learning to learn sequence models for music. The ability to combine likelihood and reward based learning has been long known, as a result of the unification of inference and learning first appearing in the ML literature with the EM formalism of Attias (2003) for fixed horizons, extended by Toussaint and Storkey (2006), to general horizon settings, Toussaint et al. (2011) to POMDPs and generalised further by Kappen et Al. (2012) and Rawlik et Al. (2012). These papers introduced the basic unification, and so any additional probabilistic or data driven objective can be combined with the reinforcement learning signal: it is all part of a unified reward/likelihood. Hence the optimal control target under unification is p(b=1|\\tau)E_p(A,S) \\prod_t \\pi(a_t|s_t): i.e. the probability of getting reward, and probability of the policy actions under the known data-derived distribution, thereby introducing the log p(a_t|s_t) into (9) too.----------------The interpretation of the secondary objective as the prior is an alternative approach under a stochastic optimal control setting, but not the most natural one given the whole principle of SOC of matching control objectives to inference objectives. The SOC off policy objective still does still contain the KL term so the approach would still differ from the approach of this paper.----------------Though the discussion of optimal control is good, I think some further elaboration of the history and how reward augmentation can work in SOC would be valuable. This would allow SOC off-policy methods to be compared with the DQN directly, like for like.----------------The motivation of the objective (3) is sensible but could be made clearer via the unification argument above. Then the paper uses DCN to take a different approach from the variational SOC for achieving that objective.----------------Another interesting point of discussion is the choice of E_pi \\log p(a_t|s_t) – this means the policy must “cover” the model. But one problem in generation is that a well-trained model is often underfit, resulting in actions that, over the course of a number of iterations, move the state into data-unsupported parts of the space. As a result the model is no longer confident and quickly tends to be fairly random. This approach (as opposed to a KL(p||pi) – which is not obvious how to implement) cannot mitigate against that, without a very strong signal (to overcome the tails of a distribution). In music, with a smaller discrete alphabet, this is likely to be less of a problem than for real valued policy densities, with exponentially decaying tails. Some further discussion of what you see in light of this issue would be valuable: the use of c to balance things seems critical, and it seems clear from Figure 2 that the reward signal needed to be very high to push the log p signal into the right range.----------------Altogether, in the music setting this paper provides a reasonable demonstration that augmentation of a sequence model with an additional reward constraint is valuable. It demonstrates that DQN is one way of learning that signal, but AFAICS it does not compare learning the same signal via other techniques. Instead for the comparator techniques it reverts to treating the p(a|s) as a “prior” term rather than a reward term, leaving a bit of a question as to whether DQN is particularly appropriate. ----------------Another interesting question for the discussion is whether the music theory reward could be approximated by a differentiable model, mitigating the need for an RL approach at all.']",0.8701171875,0.1297607421875,1,0.9296875,0.07049560546875,1,0.399658203125,0.60009765625,0,0.91796875,0.0819091796875,1,0.11944580078125,0.88037109375,0,0.41552734375,0.58447265625,0
7,7,7,7,7,7,https://openreview.net/forum?id=BJVEEF9lx,"-Reviewer 2 detailed some of the related work already, and especially DeepMind (which I am not affiliated with) presented some interesting and highly related results with its neural touring machine and following work..---* Can a queue / stack be used in arbitrary situations of push-pop operations occuring, even though it was only trained solely with consecutive pushes / consecutive pops?.-There are several flaws:
--- - In the case of the stack: I do not see a difference between this and a seq-to-seq RNN trained with e.g.

-The claims about mental representations are not well supported..It is an interesting and novel approach that could generalize well to interesting datastructures and algorithms..The paper presents a framework to formulate data-structures in a learnable way.","['A method for training neural networks to mimic abstract data structures is presented. The idea of training a network to satisfy an abstract interface is very interesting and promising, but empirical support is currently too weak. The paper would be significantly strengthened if the method could be shown to be useful in a realistic application, or be shown to work better than standard RNN approaches on algorithmic learning tasks.----------------The claims about mental representations are not well supported. I would remove the references to mind and brain, as well as the more philosophical points, or write a paper that really emphasizes one of these aspects and supports the claims.', ""The paper presents a framework to formulate data-structures in a learnable way. It is an interesting and novel approach that could generalize well to interesting datastructures and algorithms. In its current state (Revision of Dec. 9th), there are two strong weaknesses remaining: analysis of related work, and experimental evidence.----------------Reviewer 2 detailed some of the related work already, and especially DeepMind (which I am not affiliated with) presented some interesting and highly related results with its neural touring machine and following work. While it may be of course very hard to make direct comparisons in the experimental section due to complexity of the re-implementation, it would at least be very important to mention and compare to these works conceptually.----------------The experimental section shows mostly qualitative results, that do not (fully) conclusively treat the topic. Some suggestions for improvements:--------* It would be highly interesting to learn about the accuracy of the stack and queue structures, for increasing numbers of elements to store.--------* Can a queue / stack be used in arbitrary situations of push-pop operations occuring, even though it was only trained solely with consecutive pushes / consecutive pops? Does it in this enhanced setting `diverge' at some point?--------* The encoded elements from MNIST, even though in a 28x28 (binary?) space, are elements of a ten-element set, and can hence be encoded a lot more efficiently just by `parsing' them, which CNNs can do quite well. Is the NN `just' learning to do that? If so, its performance can be expected to strongly degrade when having to learn to stack more than 28*28/4=196 numbers (in case of an optimal parser and loss-less encoding). To argue more in this direction, experiments would be needed with an increasing number of stack / queue elements. Experimenting with an MNIST parsing NN in front of the actual stack/queue network could help strengthening or falsifying the claim.--------* The claims about `mental representations' have very little support throughout the paper. If indication for correspondence to mental models, etc., could be found, it would allow to hold the claim. Otherwise, I would remove it from the paper and focus on the NN aspects and maybe mention mental models as motivation."", 'The paper presents a way to ""learn"" approximate data structures. They train neural networks (ConvNets here) to perform as an approximate abstract data structure by having an L2 loss (for the unrolled NN) on respecting the axioms of the data structure they want the NN to learn. E.g. you NN.push(8), NN.push(6), NN.push(4), the loss is proportional to the distance with what is NN.pop()ed three times and 4, 6, 8 (this example is the one of Figure 1).----------------There are several flaws:-------- - In the case of the stack: I do not see a difference between this and a seq-to-seq RNN trained with e.g. 8, 6, 4 as input sequence, to predict 4, 6, 8.-------- - While some of the previous work is adequately cited, there is an important body of previous work (some from the 90s) on learning Peano\'s axioms, stacks, queues, etc. that is not cited nor compared to. For instance [Das et al. 1992], [Wiles & Elman 1995], and more recently [Graves et al. 2014], [Joulin & Mikolov 2015], [Kaiser & Sutskever 2016]...-------- - Using MNIST digits, and not e.g. a categorical distribution on numbers, is adding complexity for no reason.-------- - (Probably the biggest flaw) The experimental section is too weak to support the claims. The figures are adequate, but there is no comparison to anything. There is also no description nor attempt to quantify a form of ""success rate"" of learning such data structures, for instance w.r.t the number of examples, or w.r.t to the size of the input sequences. The current version of the paper (December 9th 2016) provides, at best, anecdotal experimental evidence to support the claims of the rest of the paper.----------------While an interesting direction of research, I think that this paper is not experimentally sound enough for ICLR.']",0.67138671875,0.32861328125,1,0.46923828125,0.53076171875,0,0.1134033203125,0.88671875,0,0.9013671875,0.09844970703125,1,0.11151123046875,0.888671875,0,0.42333984375,0.57666015625,0
8,8,8,8,8,8,https://openreview.net/forum?id=BJm4T4Kgx,"-In this part the authors raise many interesting problems or guess, but lack theoretical explanations..---Using the ratio of 'clean accuracy' over ‘adversarial accuracy’ as the measure of robust is more reasonable compared to the existing works in the literature..---The paper is well written and easy to follow.

This paper has two main contributions: 
---(1) Applying adversarial training to imagenet, a larger dataset than previously considered 
---(2) Comparing different adversarial training approaches, focusing importantly on the transferability of different methods..Although I still have some concerns about the paper (see the comments below), this paper has good contributions and worth to publish..For example, one dataset of imageNet can not infer the conclusions for all large-scale datasets.","['This paper has two main contributions: --------(1) Applying adversarial training to imagenet, a larger dataset than previously considered --------(2) Comparing different adversarial training approaches, focusing importantly on the transferability of different methods. The authors also uncover and explain the label leaking effect which is an important contribution.----------------This paper is clear, well written and does a good job of assessing and comparing adversarial training methods and understanding their relation to one another. A wide range of empirical results are shown which helps elucidate the adversarial training procedure. This paper makes an important contribution towards understand adversarial training and believe ICLR is an appropriate venue for this work.', ""This paper investigate the phenomenon of the adversarial examples and the adversarial training on the dataset of ImageNet. While the final conclusions are still vague, this paper raises several noteworthy finding from its experiments.--------The paper is well written and easy to follow. Although I still have some concerns about the paper (see the comments below), this paper has good contributions and worth to publish.----------------Pros:--------For the first time in the literature, this paper proposed the concept of ‘label leaking’. Although its effect only becomes significant when the dataset is large, it should be carefully handled in the future research works along this line.--------Using the ratio of 'clean accuracy' over ‘adversarial accuracy’ as the measure of robust is more reasonable compared to the existing works in the literature. ----------------Cons:--------Although the conclusions of the paper are based on the experiments on ImageNet, the title of the paper seems a little misleading. I consider Section 4 as the main contribution of the paper. Note that Section 4.3 and Section 4.4 are not specific to large-scale dataset, thus emphasizing the ‘large-scale’ in the title and in the introduction seems improper. --------Basically all the conclusions of the paper are made based on observing the experimental results. Further tests should have been performed to verify these hypotheses. Without that, the conclusions of the paper seems rushy. For example, one dataset of imageNet can not infer the conclusions for all large-scale datasets. "", 'This paper is a well written paper. This paper can be divided into 2 parts:--------1.Adversary training on ImageNet --------2.Empirical study of label leak, single/multiple step attack, transferability and importance of model capacity----------------For part [1], I don’t think training without clean example will not make reasonable ImageNet level model. Ian’s experiment in “Explaining and Harnessing Adversarial Examples” didn\'t use BatchNorm, which may be important for training large scale model. This part looks like an extension to Ian’s work with Inception-V3 model. I suggest to add an experiment of training without clean samples.----------------For part [2], The experiments cover most variables in adversary training, yet lack technical depth.  The depth, model capacity experiments can be explained by regularizer effect of adv training;  Label leaking is novel; In transferability experiment with FGSM, if we do careful observe on some special MNIST FGSM example, we can find augmentation effect on numbers, which makes grey part on image to make the number look more like the other numbers. Although this effect is hard to be observed with complex data such as CIFAR-10 or ImageNet, they may be related to the authors\' observation ""FGSM examples are most transferable"".  ----------------In this part the authors raise many interesting problems or guess, but lack theoretical explanations. ----------------Overall I think these empirical observations are useful for future work. ']",0.9248046875,0.074951171875,1,0.94775390625,0.05242919921875,1,0.319580078125,0.6806640625,0,0.89697265625,0.10308837890625,1,0.1043701171875,0.8955078125,0,0.375,0.625,0
9,9,9,9,9,9,https://openreview.net/forum?id=Bk0MRI5lg,"-The idea put forth that SOI map networks without additional nonlinearities are comparable to linear functions is rather misleading as they are, in expectation, nonlinear functions of their input..(TIMIT results on frame classification also aren't that interesting without evaluating word error rate within a speech pipeline, but this is a minor point.).-The plots are very difficult to read in grayscale,

Though the implied connection between nonlinearities and stochastic regularizers is intriguing, in my opinion the empirical performance does not exceed the performance achieved by similar methods by a large enough margin to arrive at a meaningful conclusion..Experiments are performed with both..Approaches like adaptive dropout also have the binary mask as a function of input to a neuron very similar to the proposed approach.","['The proposed regularizer seems to be a particular combination of existing methods. Though the implied connection between nonlinearities and stochastic regularizers is intriguing, in my opinion the empirical performance does not exceed the performance achieved by similar methods by a large enough margin to arrive at a meaningful conclusion. ', ""The method proposed essential trains neural networks without a traditional nonlinearity, using multiplicative gating by the CDF of a Gaussian evaluated at the preactivation; this is motivated as a relaxation of a probit-Bernoulli stochastic gate. Experiments are performed with both.----------------The work is somewhat novel and interesting. Little is said about why this is preferable to other similar parameterizations of the same (sigmoidal? softsign? etc.) It would be stronger with more empirical interrogation of why this works and exploration of the nearby conceptual space. The CIFAR results look okay by today's standards but the MNIST results are quite bad, neural nets were doing better than 1.5% a decade ago and the SOI map results (and the ReLU baseline) are above 2%. (TIMIT results on frame classification also aren't that interesting without evaluating word error rate within a speech pipeline, but this is a minor point.)----------------The idea put forth that SOI map networks without additional nonlinearities are comparable to linear functions is rather misleading as they are, in expectation, nonlinear functions of their input. Varying an input example by multiplying or adding a constant will not be linearly reflected in the expected output of the network. In this sense they are more nonlinear than ReLU networks which are at least locally linear.----------------The plots are very difficult to read in grayscale,"", 'Approaches like adaptive dropout also have the binary mask as a function of input to a neuron very similar to the proposed approach. It is not clear, even from the new draft, how the proposed approach differs to Adaptive dropout in terms of functionality. The experimental validation is also not extensive since comparison to SOTA is not included. ']",0.84814453125,0.151611328125,1,0.94140625,0.05853271484375,1,0.1341552734375,0.86572265625,0,0.91064453125,0.089599609375,1,0.142822265625,0.857421875,0,0.317626953125,0.68212890625,0
10,10,10,10,10,10,https://openreview.net/forum?id=Bk67W4Yxl,"This should have been put front and center in the abstract and introduction..It would’ve been more convincing to train both AlphaGo and this paper's architectures on an equal number of RL self-play iterations, then have them play each other..-Fully 3 pages are spent on giant but simple architecture diagrams.

The paper tests various feedforward network architectures for supervised training to predict a human’s next move, given a board position..It trains on human play data taken from KGX, augmenting the data by considering all 8 rotations/reflections of each board position..However, the paper only presents supervised learning results, not RL.","[""The paper tests various feedforward network architectures for supervised training to predict a human’s next move, given a board position. It trains on human play data taken from KGX, augmenting the data by considering all 8 rotations/reflections of each board position. ----------------The paper’s presentation is inefficient and muddled, and the results seem incremental.----------------Presentation:----------------The abstract and introduction point out that AlphaGo requires many RL iterations to train, and propose to improve this by swapping out the policy network with one that is more amenable to training. However, the paper only presents supervised learning results, not RL.----------------While it’s not unreasonable to assume that a higher-capacity network that shows improvements in supervised learning will also yield dividends in RL, it’s still unsatisfying to be presented with SL improvements and be asked to assume that the RL improvements will be of a similar magnitude, whatever that may mean. It would’ve been more convincing to train both AlphaGo and this paper's architectures on an equal number of RL self-play iterations, then have them play each other. (Both would be pre-trained using supervised training, as per the AlphaGo paper).----------------It is not until section 3.3 that it is clearly stated this is strictly a supervised-learning paper. This should have been put front and center in the abstract and introduction.----------------Fully 3 pages are spent on giant but simple architecture diagrams. This is both extravagant and muddles the exposition. It seems better to show just the architectures used in the experiments, and spend at most half a page doing so, so that they may be seen alongside one another.----------------The results (Table 1, Figures 7 and 8) are hard to skim, as there is little information in the captions, and the graph axes are poorly labeled. For example, I assume “Examples” in figures 7 and 8 should be “Training examples”, and the number of training examples isn’t 0-50, but some large multiple thereof.----------------Results: ----------------The take-home seems to be that that deeper networks do better, and residual architectures and spatial batch normalization each improve the results in this domain, as they are known to do in others. Furthermore, we are asked to assume that the improvements in an RL setting will be similar to the improvements in SL shown here. These results seem too incremental to justify an ICLR publication. "", 'This paper reports new CNN architectures for playing Go. The results are better than previously reported, but there is no mention of computational time and efficiency, and relative metric of performance/flop or performance/flop/energy.--------Overall a good paper.', 'The paper trains slightly different network architectures on Computer Go, and provides an analysis of the accuracy as the number of training samples increases and the network architecture differs.----------------The paper looks like a follow up paper of the author’s previous paper Cazenave (2016a), however, the contribution over the previous paper is not clear. A section should be added to state what the differences are. ----------------The paper states that the improvements that are obtained in this study are because of the changes in the training set, the input features and the architecture of the network. It is not clear what are the changes that were done to the training set and input features. How are they different than the previous work?', ""The paper investigates several different architectures for move prediction in computer Go. The main innovation seems to be the use of residual networks. The best proposed architecture outperforms previous results on KGS move prediction dataset. The network also reached amateur 3 dan level on KGS.----------------I found the paper to be somewhat poorly written and lacking important details. Here are my main concerns:--------1) This paper references a previous paper by the author(Cazenave 2016a) as having introduced residual network architectures to computer go. The overlap with this paper seems quite significant but I could not find it anywhere. What exactly is new?--------2) The author claims the addition of batch norm to a residual architecture as the main architectural innovation, but the original ResNet paper was already using batch norm between the convolution and activation layers. Have you compared your architecture (ResNet with batch norm after ReLU) with the original ResNet architecture (batch norm before ReLU)?--------3) It is not at all surprising that ResNets do slightly better than vanilla CNNs on move prediction. I don't think this alone is enough for an ICLR paper. It would be good to see at least a comparison of several different variants of the network evaluated at actually playing Go, even if it's against other bots like GnuGo, Pachi, and Fuego.--------4) Are the differences between net_dark and your proposed networks after 20 iterations (Table 1) significant? ----------------Please also see my original questions.""]",0.90576171875,0.0941162109375,1,0.96435546875,0.035400390625,1,0.2314453125,0.7685546875,0,0.91064453125,0.089111328125,1,0.1729736328125,0.8271484375,0,0.50830078125,0.49169921875,1
11,11,11,11,11,11,https://openreview.net/forum?id=Bk8BvDqex,"Content of this type this could form a useful additional appendix..3 is generally clear, but the lack of x-axis tick marks on any subplots makes it more challenging than necessary to compare among the experts..-I found the formalism a bit hard to follow without specific examples- that is, it wasn't clear to me at first what the specific components in figure 1A were.

What constitutes the controller, a control, the optimizer, what was being optimized, etc., in specific cases..A description of existing models that fall under your conceptual framework might help as well..The metacontroller is a model-free reinforcement learning agent that selects how many optimization iterations and what function or “expert” to consult from a fixed set (such as an action-value or state transition function).","[""A well written paper and an interesting construction - I thoroughly enjoyed reading it. ----------------I found the formalism a bit hard to follow without specific examples- that is, it wasn't clear to me at first what the specific components in figure 1A were. What constitutes the controller, a control, the optimizer, what was being optimized, etc., in specific cases. Algorithm boxes may have been helpful, especially in the case of your experiments. A description of existing models that fall under your conceptual framework might help as well.----------------In Practical Bayesian Optimization of Machine Learning Algorithms, Snoek, Larochelle and Adams propose optimizing with respect to expected improvement per second to balance computation cost and performance loss. It might be interesting to see how this falls into your framework.----------------Experimental results were presented clearly and well illustrated the usefulness of the metacontroller. I'm curious to see the results of using more metaexperts."", ""Pros (quality, clarity, originality, significance:):----------------This paper presents a novel metacontroller optimization system that learns the best action for a one-shot learning task, but as a framework has the potential for wider application. The metacontroller is a model-free reinforcement learning agent that selects how many optimization iterations and what function or “expert” to consult from a fixed set (such as an action-value or state transition function). Experimental results are presented from simulation experiments where a spacecraft must fire its thruster once to reach a target location, in the presence of between 1 and 5 heavy bodies.----------------The metacontroller system has a similar performance loss on the one-shot learning task as an iterative (standard) optimization procedure. However, by taking into account the computational complexity of running a classical, iterative optimization procedure as a second “resource loss” term, the metacontroller is shown to be more efficient. Moreover, the metacontroller agent successfully selects the optimal expert to consult, rather than relying on an informed choice by a domain-expert model designer. The experimental performance is a contribution that merits publication, and it also exhibits the use of an interaction network for learning the dynamics of a simulated physical system. The dataset that has been developed for this task also has the potential to act as a new baseline for future work on one-shot physical control systems. The dataset constitutes an ancillary contribution which could positively impact future research in this area.----------------Cons:--------It's not clear how this approach could be applied more broadly to other types of optimization. Moreover, the REINFORCE gradient estimation method is known to suffer from very high variance, yielding poor estimates. I'm curious what methods were used to ameliorate these problems and if any other performance tricks were necessary to train well. Content of this type this could form a useful additional appendix.----------------A few critiques on the communication of results:----------------- The formal explication of the paper’s content is clear, but Fig.’s 1A and 3 could be improved. Fig. 1A is missing a clear visual demarcation of what exactly the metacontroller agent is. Have you considered a plate or bounding box around the corresponding components? This would likely speed the uptake of the formal description.----------------- Fig. 3 is generally clear, but the lack of x-axis tick marks on any subplots makes it more challenging than necessary to compare among the experts. Also, the overlap among the points and confidence intervals in the upper-left subplot interferes with the quantitative meaning of those symbols. Perhaps thinner bars of different colors would help here. Moreover, this figure lacks a legend and so the different lines are impossible to compare with each other.----------------- Lastly, the second sentence in Appendix B. 2 is a typo and terminates without completion."", 'This paper introduces an approach to reinforcement learning and control wherein, rather than training a single controller to perform a task, a metacontroller with access to a base-level controller and a number of accessory « experts » is utilized. The job of the metacontroller is to decide how many times to call the controller and the experts, and which expert to invoke at which iteration. (The controller is a bit special in that in addition to being provided the current state, it is given a summary of the history of previous calls to itself and previous experts.) The sequence of controls and expert advice is embedded into a fixed-size vector through an LSTM. The method is tested on an N-body  control task, where it is shown that there are benefits to multiple iterations (« pondering ») even for simple experts, and that the metacontroller can deliver accuracy and computational cost benefits over fixed-iteration controls.----------------The paper is in general well written, and reasonably easy to follow. As the authors note, the topic of metareasoning has been studied to some extent in AI, but its use as a differentiable and fully trainable component within an RL system appears new. At this stage, it is difficult to evaluate the impact of this kind of approach: the overall model architecture is intriguing and probably merits publication, but whether and how this will scale to other domains remains the subject of future work. The experimental validation is interesting and well carried out, but remains of limited scope. Moreover, given such a complex architecture, there should be a discussion of the training difficulties and convergence issues, if any.----------------Here are a few specific comments, questions and suggestions:----------------1) in Figure 1A, the meaning of the graphical language should be explained. For instance, there are arrows of different thickness and line style — do these mean different things? ----------------2) in Figure 3, the caption should better explain the contents of the figure. For example, what do the colours of the different lines refer to? Also, in the top row, there are dots and error bars that are given, but this is explained only in the « bottom row » part. This makes understanding this figure difficult.----------------3) in Figure 4, the shaded area represents a 95% confidence interval on the regression line; in addition, it would be helpful to give a standard error on the regression slope (to verify that it excludes zero, i.e. the slope is significant), as well as a fraction of explained variance (R^2). ----------------4) in Figure 5, the fraction of samples using the MLP expert does not appear to decrease monotonically with the increasing cost of the MLP expert (i.e. the bottom left part of the right plot, with a few red-shaded boxes). Why is that? Is there lots of variance in these fractions from experiment to experiment?----------------5) the supplementary materials are very helpful. Thank you for all these details.', 'Thank you for an interesting read on an approach to choose computational models based on kind of examples given.----------------Pros--------- As an idea, using a meta controller to decide the computational model and the number of steps to reach the conclusion is keeping in line with solving an important practical issue of increased computational times of a simple example. ----------------- The approach seems similar to an ensemble learning construct. But instead of random experts and a fixed computational complexity during testing time the architecture is designed to estimate hyper-parameters like number of ponder steps which gives this approach a distinct advantage.------------------------Cons--------- Even though the metacontroller is designed to choose the best amongst the given experts, its complete capability has not been explored yet. It would be interesting to see the architecture handle more than 2 experts.']",0.89794921875,0.1019287109375,1,0.95849609375,0.04150390625,1,0.36669921875,0.63330078125,0,0.92041015625,0.079345703125,1,0.09832763671875,0.90185546875,0,0.490478515625,0.50927734375,0
12,12,12,12,12,12,https://openreview.net/forum?id=BkLhzHtlg,"What I see presented is a nice system that has been demonstrated to handle spatiotemporal trajectories..-In response to comment about scaling to human behavior: I agree that in principle, adding conv layers directly above the sensory input would be the right thing to try, but seriously: there is usually a pretty big gap between what ""should"" work and what actually works, as I am sure the authors are aware..(Indeed, I am sure the authors have a much more experiential and detailed understanding of the limitations of their work than I do).

While my above review title is too verbose, it would be a more accurate title for the paper than the current one (an overall better title would probably be somewhere in between)..-The overall approach is interesting: all three of the key techniques (aux..tasks, skip/diagonal connections, and the use of internal labels for the kind of data available) make a lot of sense.","['While my above review title is too verbose, it would be a more accurate title for the paper than the current one (an overall better title would probably be somewhere in between). ----------------The overall approach is interesting: all three of the key techniques (aux. tasks, skip/diagonal connections, and the use of internal labels for the kind of data available) make a lot of sense.----------------I found some of the results hard to understand/interpret. Some of the explanation in the discussion below has been helpful (e.g. see my earlier questions about Fig 4 and 5); the paper would benefit from including more such explanations. ----------------It may be worthwhile very briefly mentioning the relationship of ""diagonal"" connections to other emerging terms for similar ideas (e.g. skip connections, etc). ""Skip"" seems to me to be accurate regardless of how you draw the network, whereas ""diagonal"" only makes sense for certain visual layouts.----------------In response to comment in the discussion below: ""leading to less over-segmentation of action bouts"" (and corresponding discussion in section 5.1 of the paper): I would be like to have a bit more about this in the paper. I have assumed that ""per-bout"" refers to ""per-action event"", but now I am not certain that I have understood this correctly (i.e. can a ""bout"" last for a few minutes?): given the readership, I think it would not be inappropriate to define some of these things explicitly.----------------In response to comment about fly behaviours that last minutes vs milliseconds: This is interesting, and I would be curious to know how classification accuracy relates to the time-scale of the behaviour (e.g. are most of the mistakes on long-term behaviours? i realize that this would only tell part of the story, e.g. if you have a behaviour that has both a long-term duration, but that also has very different short-term characteristics than many other behaviours, it should be easy to classify accuractely despite being ""long-term""). If easy to investigate this, I would add a comment about it; if this is hard to investigate, it\'s probably not worth it at this point, although it\'s something you might want to look at in future.----------------In response to comment about scaling to human behavior: I agree that in principle, adding conv layers directly above the sensory input would be the right thing to try, but seriously: there is usually a pretty big gap between what ""should"" work and what actually works, as I am sure the authors are aware. (Indeed, I am sure the authors have a much more experiential and detailed understanding of the limitations of their work than I do). What I see presented is a nice system that has been demonstrated to handle spatiotemporal trajectories. The claims made should correspond to this. ----------------I would consider adjusting my rating to a 7 depending on future revisions.', 'The paper presents a method for joint motion prediction and activity classification from sequences with two different applications: motion of fruit flies and online handwriting recognition.----------------The method uses a classical encoder-decoder pipeline, with skip connections allowing direct communication between the encoder and the decoder on respective levels of abstraction.--------Motion is discretized and predicted using classification. The model is trained on classification loss combined with a loss on motion prediction. The goal is to leverage latter loss in a semi-supervised setting from parts of the data which do not contain action labels.----------------The idea of leveraging predictions to train feature representations for discrimination is not new. However, the paper presents a couple of interesting ideas, partially inspired from other work in other areas.----------------My biggest concern is with the experimental evaluation. The experimental section contains a large amount of figures, which visualize what the model has learned in a qualitative way. However, quantitative evaluation is rarer.----------------- On the fly application, the authors compare the classification performance with another method previously published by the first author.--------- Again on the fly application, the performance gain on motion prediction in figure 5c looks small compared to the baseline. I am not sure it is significant.--------- I did not see any recognition results on the handwriting application. Has this part not been evaluated?----------------Figure 5a is difficult to understand and to interpret. The term ""BesNet"" is used here without any introduction.----------------Figure 4 seems to tell multiple and different stories. I\'d suggest splitting it into at least two different figures.', 'This paper proposes a recurrent architecture for simultaneously predicting motion and action states of agents.--------The paper is well written, clear in its presentation and backed up by good experiments.--------They demonstrate that by forcing the network to predict motion has beneficial consequences on the classification of actions states,--------allowing more accurate classification with less training data.--------They also show how the information learned by the network is interpretable and organised in a hierarchy.----------------Weaknesses:--------- a critical discussion on the interplay between motion an behaviour that is needed to experience the benefits of their proposed model is missing from the paper.--------- moreover, a discussion on how this approach could scale to more challenging scenarios ""involving animals"" and visual input for instance and more general ""behaviours"" is also missing;--------The criticism here is pointed at the fact that the title/abstract claim general behaviour modelling, whilst the experiments are focused on two very specific and relatively simple scenarios,--------making the original claim a little bit far fetched unless its backed up by additional evidence.--------Using ""Insects"", or ""fruit flies"" would be more appropriate than ""animals"".']",0.83642578125,0.1634521484375,1,0.66064453125,0.33935546875,1,0.4423828125,0.5576171875,0,0.91357421875,0.08642578125,1,0.14208984375,0.85791015625,0,0.34228515625,0.65771484375,0
13,13,13,13,13,13,https://openreview.net/forum?id=By14kuqxx,"The central idea of the proposed architecture (PRA) revolves around the fact that the regular (parallel) MACC operations waste a considerable amount of area/power to perform multiplications with zero bits..What struck me is that your paper contains very few implementation details except for the technology (PRA 65nm vs DaDianNao 28nm)..Since in the DNN scenario, one of the multiplicands (the weight) is known in advance, the multiplications by the zero digits can be eliminated without affecting the calculation and lowest non-zero bits can be further dropped at the expense of precision.

-To summarize, I believe that immediately useful hardware DNN accelerators still need to operate in floating point (a good example are Movidius chips -- nowadays Intel)..1-bit DNNs involve no multiplication at all; moreover, the proposed multiplier-dependent representation of multiplication discussed in the present paper can be implemented as a 1-bit DNN..One of the extreme cases showed quanitzation to 1bit weights with negligible loss in performance (arXiv:1602.02830).","['The paper presents a hardware accelerator architecture for deep neural network inference, and a simulated performance evaluation thereof. The central idea of the proposed architecture (PRA) revolves around the fact that the regular (parallel) MACC operations waste a considerable amount of area/power to perform multiplications with zero bits. Since in the DNN scenario, one of the multiplicands (the weight) is known in advance, the multiplications by the zero digits can be eliminated without affecting the calculation and lowest non-zero bits can be further dropped at the expense of precision. The paper proposes an architecture exploiting this simple idea implementing bit-serial evaluation of multiplications with throughput depending on the number of non-zero bits in each weight. ----------------While the idea is in general healthy, it is limited to fixed point arithmetics. Nowadays, DNNs trained on regular graphics hardware have been shown to work well in floating point down to single (32bit) and even half-precision (16bit) in many cases with little or no additional adjustments. However, this is generally not true for 16bit (not mentioning 8bit) fixed point. Since it is not trivial to quantize a network to 16 or 8 bits using standard learning, recent efforts have shown successful incorporation of quantization into the training process. One of the extreme cases showed quanitzation to 1bit weights with negligible loss in performance (arXiv:1602.02830). 1-bit DNNs involve no multiplication at all; moreover, the proposed multiplier-dependent representation of multiplication discussed in the present paper can be implemented as a 1-bit DNN. I think it would be very helpful if the authors could address the advantages their architecture brings to the evaluation of 1-bit DNNs. ----------------To summarize, I believe that immediately useful hardware DNN accelerators still need to operate in floating point (a good example are Movidius chips -- nowadays Intel). Fixed point architectures promise additional efficiency and are important in low-power applications, but they depend very much on what has been done at training. In view of this -- and this is my own extreme opinion -- it makes sense to build an architecture for 1-bit DNNs. I have the impression that the proposed architecture could be very suitable for this, but the devil is in the details and currently evidence is missing to make such claims.', ""Interesting and timely paper. Lots of new neural network accelerators popping up.----------------I'm not an expert in this domain and to familiarize myself with the topic, I browsed through related work and skimmed the DaDianNao paper.--------My main question is about the choice of technology. What struck me is that your paper contains very few implementation details except for the technology (PRA 65nm vs DaDianNao 28nm). --------Combined with the fact that the main improvement of your work appears to be performance rather than energy efficiency, I was wondering about the maximum clock estimated frequency of the PRA implementation due to the added complexity? Based on the explanation in the methodology section, I assume that the performance comparison is based on number of clock cycles. Do you have any numbers/estimates about the performance in practices (taking into account clock frequency)?"", 'An interesting idea, and seems reasonably justified and well-explored in the paper, though this reviewer is no expert in this area, and not familiar with the prior work. --------Paper is fairly clear. Performance evaluation (in simulation) is on a reasonable range of recent image conv-nets, and seems thorough enough.----------------Rather specialized application area may have limited appeal to ICLR audience. (hence the ""below threshold rating"", I don\'t have any fundamental structural / methodological criticism for this paper.)------------------------Improve your bibliography citation style - differentiate between parenthetical citations and inline citations where only the date is in parentheses. ']",0.94091796875,0.0589599609375,1,0.91650390625,0.08355712890625,1,0.1669921875,0.8330078125,0,0.92919921875,0.07098388671875,1,0.1463623046875,0.853515625,0,0.34912109375,0.65087890625,0
14,14,14,14,14,14,https://openreview.net/forum?id=ByEPMj5el,"I'm worried about the diversity in the subjects because there may be too few subjects who are shown too many samples and/or are experts in this field..Just like when measuring generalization performance, one should be careful not to reuse the same held-out classes for model selection and for evaluation..The proposed metric is also problematic.

--- - The ordering of the exposition is also frustrating..The grammar is fine, but:
--- - The first four pages are completely theoretical and difficult to follow without any concrete examples..I found myself constantly having to refer ahead to figures and back to details that were important but seemingly presented out of order.","[""This paper examines computational creativity from a machine learning perspective. Creativity is defined as a model's ability to generate new types of objects unseen during training. The authors argue that likelihood training and evaluation are by construction ill-suited for out-of-class generation and propose a new evaluation framework which relies on the use of held-out classes of objects to measure a model's ability to generate new and interesting object types.----------------I am not very familiar with the literature on computational creativity research, so I can't judge on how well this work has been put into the context of existing work. From a machine learning perspective, I find the ideas presented in this paper new, interesting and thought-provoking.----------------As I understand, the hypothesis is that the ability of a model to generate new and interesting types we *do not* know about correlates with its ability to generate new and interesting types we *do* know about, and the latter is a good proxy for the former. The extent to which this is true depends on the bias introduced by model selection. Just like when measuring generalization performance, one should be careful not to reuse the same held-out classes for model selection and for evaluation.----------------Nevertheless, I appreciate the effort that has been made to formalize the notion of computational creativity within the machine learning framework. I view it as an important first step in that direction, and I think it deserves its place at ICLR, especially given that the paper is well-written and approachable for machine learning researchers."", 'First, the bad:----------------This paper is frustratingly written. The grammar is fine, but:-------- - The first four pages are completely theoretical and difficult to follow without any concrete examples. These sections would benefit greatly from a common example woven through the different aspects of the theoretical discussion.-------- - The ordering of the exposition is also frustrating. I found myself constantly having to refer ahead to figures and back to details that were important but seemingly presented out of order. Perhaps a reordering of some details could fix this. Recommendation: give the most naturally ordered oral presentation of the work and then order the paper similarly.----------------Finally, the description of the experiments is cursory, and I found myself wondering whether the details omitted were important or not. Including experimental details in a supplementary section could help assuage these fears.----------------The good:----------------What the paper does well is to gather together past work on novelty generation and propose a unified framework in which to evaluate past and future models. This is done by repurposing existing generative model evaluation metrics for the task of evaluating novelty. The experiments are basic, but even the basic experiments go beyond previous work in this area (to this reviewer’s knowledge).----------------Overall I recommend the paper be accepted, but I strongly recommend rewriting some components to make it more digestible. As with other novelty papers, it would be read thoroughly by the interested few, but it is likely to fight an uphill battle against the majority of readers outside the sub-sub-field of novelty generation; for this reason the theory should be made even more intuitive and clear and the experiments and results even more accessible.', 'The authors proposed an way to measure the generation of out-of-distribution novelty. Their methods implied, if a model trained on MNIST digits could generate some samples are more like letters judged by anther model trained both on MNIST and letters,  the model trained on MNIST could be seen as having  the ability to generate novel samples. Some empirical experiments were reported. --------The novelty is hard to define. The proposed metric is also problematic. A naive combination of MNIST and letters dataset do not represent the natural distribution of handwritten digits and letters. IT means that the model trained on the combination could not properly distinguished digits and letters. The proposed out-of-class count and out-of-class max are thus pointless. For the ""novel"" samples in Fig. 3,  they are clearly digits. I guess they quantize the samples to binary. If they would quantize the samples to 8 bit, the resulting images would look even more like digits. ', 'This paper proposed a quantitative metric for evaluating out-of-class novelty of samples from generative models. The authors evaluated the proposed metric on over 1000 models with different hyperparameters and performed human subject study on a subset of them.----------------The authors mentioned difficulties in human subject studies, but did not provide details of their own setting. An ""in-house"" annotation tool was used but it\'s unclear how many subjects were involved, who they are, and how many samples were presented to each subject. I\'m worried about the diversity in the subjects because there may be too few subjects who are shown too many samples and/or are experts in this field.----------------This paper aims at proposing a general metric for novelty but the experiments only used one setting, namely generating Arabic digits and English letters. There is insufficient evidence to prove the generality of the proposed metric.----------------Moreover, defining English letters as ""novel"" compared to Arabic digits is questionable. What if the model generates Arabic or Indian letters? Can a human who has never seen Arabic handwriting tell it from random doodle? What makes English letters more ""novel"" than random doodle? In my opinion these questions are best answered through large scale human subject study on tasks that has clear real world meanings. For example, do you prefer painting A (generated) or B (painted by artist).']",0.8154296875,0.184814453125,1,0.93701171875,0.06304931640625,1,0.21337890625,0.78662109375,0,0.89794921875,0.10198974609375,1,0.050262451171875,0.94970703125,0,0.330810546875,0.6689453125,0
15,15,15,15,15,15,https://openreview.net/forum?id=ByG8A7cee,"---- What's the advantage of using an ""Entity state update"" rule, compared to a pointer network or copy network, which you used in the dialogue and recipe tasks?.----Other comments:
---- For the ""Attention based decoder"", is the attention computed using the word embeddings themselves or the hidden states of the sentence encoder?.Please elaborate on this.

This paper presents a new type of language model that treats entity references as latent variables..But it seems the paper was rushed into the deadline, as there are a few major weaknesses..Because mentions were assumed to be given for the reference-aware language models, and because of the fact that mention generators are designed similar to a pointer network, the probability scores over mentions will naturally be higher, compared to the regular language model that needs to consider a much bigger vocabulary set.","['This paper presents a new type of language model that treats entity references as latent variables. The paper is structured as three specialized models for three applications: dialog generation with references to database entries, recipe generation with references to ingredients, and text generation with coreference mentions.----------------Despite some opaqueness in details that I will discuss later, the paper does a great job making the main idea coming through, which I think is quite interesting and definitely worth pursuing further. But it seems the paper was rushed into the deadline, as there are a few major weaknesses.----------------The first major weakness is that the claimed latent variables are hardly latent in the actual empirical evaluation. As clarified by the authors via pre-review QAs, all mentions were assumed to be given to all model variants, and so, it would seem like an over-claim to call these variables as latent when they are in fact treated as observed variables. Is it because the models with latent variables were too difficult to train right?----------------A related problem is the use of perplexity as an evaluation measure when comparing reference-aware language models to vanilla language models. Essentially the authors are comparing two language models defined over different event space, which is not a fair comparison. Because mentions were assumed to be given for the reference-aware language models, and because of the fact that mention generators are designed similar to a pointer network, the probability scores over mentions will naturally be higher, compared to the regular language model that needs to consider a much bigger vocabulary set. The effect is analogous to comparing language models with aggressive UNK (and a small vocabulary set) to a language models with no UNK (and a much larger vocabulary set).----------------To mitigate this problem, the authors need to perform one of the following additional evaluations: either assuming no mention boundaries and marginalizing over all possibilities (treating latent variables as truly latent), or showing other types of evaluation beyond perplexity, for example, BLEU, METEOR, human evaluation etc on the corresponding generation task.----------------The other major weakness is writing in terms of technical accuracy and completeness. I found many details opaque and confusing even after QAs. I wonder if the main challenge that hinders the quality of writing has something to do with having three very specialized models in one paper, each having a lot of details to be worked out, which may have not been extremely important for the main story of the paper, but nonetheless not negligible in order to understand what is going on with the paper.    Perhaps the authors can restructure the paper so that the most important details are clearly worked out in the main body of the paper, especially in terms of latent variable handling — how to make mention detection and conference resolution truly latent, and if and when entity update helps, which in the current version is not elaborated at all, as it is mentioned only very briefly for the third application (coreference resolution) without any empirical comparisons to motivate the update operation.', 'This paper explores 3 language modeling applications with an explicit modeling of reference expressions: dialog, receipt generation and coreferences. While these are important tasks for NLP and the authors have done a number of experiments, the paper is limited for a few reasons:----------------1. This paper is not clearly written and is pretty hard to follow some details. In particular,  there are many obvious math errors, such as missing the marginalization sum in Eq (1), and P(z_{i,v}...) = 1 (should be 0 here) on page 5, pointer switch section.----------------2. The major novelty seems to be the 2-dimensional attention from the table and the pointer to the 2-D table. These are more of a customization of existing work to a particular task with 2-D tables as a part of the input to seq2seq model with both attentions and pointer networks.----------------3. The empirical results are not very conclusive yet, limited by either the relatively small data size, or the lack of well-established baseline for some new applications (e.g., the recipe generation task).----------------Overall, this paper, as it is for now, is more suitable for a workshop rather than for the main conference.', 'This paper introduces pointer-network neural networks, which are applied to referring expressions in three small-scale language modeling tasks: dialogue modeling, recipe modeling and news article modeling. When conditioned on the co-reference chain, the proposed models outperform standard sequence-to-sequence models with attention.----------------The proposed models are essentially variants of pointer networks with copy mechanisms (Gulcehre et al., 2016; Gu et al., 2016; Ling et al., 2016), which have been modified to take into account reference chains. As such, the main architectural novelty lies in 1) restricting the pointer mechanism to focus on co-referenced entities, 2) applying pointer mechanism to 2D arrays (tables), and 3) training with supervised alignments. Although useful in practice, these are minor contributions from an architectural perspective.----------------The empirical contributions are centred around measuring perplexity on the three language modeling tasks. Measuring perplexity is typical for standard language modeling tasks, but is really an unreliable proxy for dialogue modeling and recipe generation performance. In addition to this, both the dialogue and recipe tasks are tiny compared to standard language modeling tasks. This makes it difficult to evaluate the impact of the dialogue and recipe modeling results. For example, if one was to bootstrap from a larger corpus, it seems likely that a standard sequence-to-sequence model with attention would yield performance comparable to the proposed models (with enough data, the attention mechanism could learn to align referring entities by itself). The language modeling task on news article (Gigaword) seems to yield the most conclusive results. However, the dataset for this task is non-standard and results are provided for only a single baseline. Overall, this limits the conclusions we can draw from the empirical experiments.------------------------Finally, the paper itself contains many errors, including mathematical errors, grammatical errors and typos:--------- Eq. (1) is missing a sum over .--------- ""into the a decoder LSTM"" -> ""into the decoder LSTM""--------- ""denoted as his"" -> ""denoted as""--------- ""Surprising,"" -> ""Surprisingly,""--------- ""torkens"" -> ""tokens""--------- ""if follows that the next token"" -> ""the next token""--------- In the ""COREFERENCE BASED LANGUAGE MODEL"" sub-section, what does  denote?--------- In the sentence: ""The attribute of each column is denoted as cs_cp(z_{i,v} |s_{i,v}) = 1p(z_{i,v} |s_{i,v}) = 0$.--------- In the ""Table Pointer"" paragraph, I assume you mean outer-product instead of cross-product? Otherwise, I don\'t see how the equations add up.------------------------Other comments:--------- For the ""Attention based decoder"", is the attention computed using the word embeddings themselves or the hidden states of the sentence encoder? Also, it applied only to the previous turn of the dialogue or to the entire dialogue history? Please clarify this.--------- What\'s the advantage of using an ""Entity state update"" rule, compared to a pointer network or copy network, which you used in the dialogue and recipe tasks? Please elaborate on this.--------- In the Related Work section, the following sentence is not quite accurate: ""For the task oriented dialogues, most of them embed the seq2seq model in traditional dialogue systems while our model queries the database directly."". There are task-oriented dialogue models which do query databases during natural language generation. See, for example, ""A Network-based End-to-End Trainable Task-oriented Dialogue System"" by Wen et al.']",0.81884765625,0.1812744140625,1,0.912109375,0.08782958984375,1,0.40478515625,0.59521484375,0,0.90673828125,0.09326171875,1,0.048736572265625,0.951171875,0,0.3251953125,0.6748046875,0
16,16,16,16,16,16,https://openreview.net/forum?id=BycCx8qex,"Can I implement the
---dynamic programming components as transition units and (importantly) would it be
---efficient?.The main idea is straightforward: use a
---transition system to unroll a computation graph..-The most interesting contribution of this work is the ease by which it can be used to incorporate dynamic recurrent connections through the definition of the transition system.

However, it is difficult to say whether the DRAGNN is novel..Transition-based framework is already well defined and there's a huge trend in NLP using neural networks to implement transition-based systems..The authors present a general framework for defining a wide variety of recurrent neural network architectures, including seq2seq models, tree-structured models, attention, and a new family of dynamically connected architectures.","[""The paper proposes a new neural architecture, called DRAGNN, for the transition-based framework. A DRAGNN uses TBRUs which are neural units to compute hidden activations for the current state of a transition-based system. The paper proves that DRAGNNs can cover a wide range of transition-based methods in the literature. In addition, one can easily implement multitask learning systems with DRAGNNs. The experimental results shows that using DRAGNNs the authors built (near) state-of-the-art systems for 2 tasks: parsing and summarization. ----------------The paper contains two major parts: DRAGNN and demonstrations of its usages. ----------------Regarding to the first part, the proposed DRAGNN is a neat tool for building any transition-based systems. However, it is difficult to say whether the DRAGNN is novel. Transition-based framework is already well defined and there's a huge trend in NLP using neural networks to implement transition-based systems. In my opinion, the difference between the Stack-LSTM (Dyer et al., 2015) and DRAGNN is slight. Of course, the DRAGNN is a powerful architecture but the contribution here should be considered mainly in terms of software engineering.----------------In the second part, the authors used DRAGNN to implement new transition-based systems for different (multi-)tasks. The implementations are neat, confirming that DRAGNN is a powerful architecture, especially for multitask learning. However, we should bear in mind that the solutions employed are already there in the literature, thus making difficult to judge the novelty of this part w.r.t. the theme of the conference.  "", 'The authors present a general framework for defining a wide variety of recurrent neural network architectures, including seq2seq models, tree-structured models, attention, and a new family of dynamically connected architectures. The framework defines a new, general-purpose recurrent unit called the TBRU, which takes a transition system, defining and constraining its inputs and outputs, and input function which defines the mapping between raw inputs and fixed-width vector representations, and recurrence function that defines the inputs to each recurrent step as a function of the current state, and an RNN cell that computes the output from the input (fixed and recurrent). Many example instantiations of this framework are provided, including sequential tagging RNNs, Google’s Parsey McParseface parser, encoder/decoder networks, tree LSTMs and less familiar examples that demonstrate the power this framework. ----------------The most interesting contribution of this work is the ease by which it can be used to incorporate dynamic recurrent connections through the definition of the transition system. In particular, this paper explores the application of these dynamic connections to syntactic dependency parsing, both as a standalone task, and by multitasking parsing with extractive summarization, using the same compositional phrase representations as features for the parser and summarization (previous work used discrete parse features), which is particularly simple/elegant in this framework. In experimental results, the authors demonstrate that such multitasking leads to more accurate summarization models, and using the framework to incorporate more structure into existing parsing models also leads to increased accuracy with no big-oh efficiency loss (compared with e.g. attention). ----------------The “raison d’etre,” in particular the example, perhaps described even more thoroughly/explicitly, should be made as clear as possible as soon as possible. This is the most important contribution, but it gets lost in the description and presentation as a framework — emphasizing that attention, seq2seq, etc can be represented in the framework is distracting and makes it seem less novel than it is. AnonReviewer6 clearly missed this point, as did I in my first pass over the paper. To get this idea across and to emphasize the benefits of this representation, I’d love to see more detailed analysis of these representations and their importance to achieving your experimental results. I think it would also be helpful to emphasize the difference between a stack LSTM and Example 6. ----------------Overall I think this paper presents a valuable contribution, though the exposition could be improved and analysis of experimental results expanded. ', 'Overall, this is a nice paper. Developing a unifying framework for these newer--------neural models is a worthwhile endeavor.----------------However, it\'s unclear if the DRAGNN framework (in its current form) is a--------significant standalone contribution. The main idea is straightforward: use a--------transition system to unroll a computation graph. When you implement models in--------this way you can reuse code because modules can be mixed and matched. This is--------nice, but (in my opinion) is just good software engineering, not machine --------learning research.----------------Moreover, there appears to be little incentive to use DRAGNN, as there are no--------\'free things\' (benefits) that you get by using the framework. For example:----------------- If you write your neuralnet in an automatic differentiation library (e.g.,--------  tensorflow or dynet) you get gradients for \'free\'.----------------- In the VW framework, there are efficiency tricks that \'the credit assignment--------  compiler\' provides for you, which would be tedious to implement on your--------  own. There is also a variety of algorithms for training the model in a--------  principled way (i.e., without exposure bias).----------------I don\'t feel that my question about the limitations of the framework has been--------satisfactorily addressed. Let me ask it in a different way: Can you give me--------examples of a few models that I can\'t (nicely) express in the DRAGNN framework?--------What if I wanted to implement https://openreview.net/pdf?id=HkE0Nvqlg or--------http://www.cs.jhu.edu/~jason/papers/rastogi+al.naacl16.pdf? Can I implement the--------dynamic programming components as transition units and (importantly) would it be--------efficient?---------------- disagree that the VW framework is orthogonal, it is a *competing* way to--------implement recurrent models. The main different to me appears to be that VW\'s--------imperative framework is more general, but less modular.----------------The experimental contribution seems useful as does the emphasis on how easy it--------is to incorporate multi-task learning.----------------Minor:----------------- It would be useful to see actual code snippets (possibly in an--------  appendix). Otherwise, its unclear how modular DRAGNN really are.----------------- The introduction states that (unlike seq2seq+attention) inference remains--------  linear. Is this *necessarily* the case? Users define a transition system that--------  is quadratic, just let attention be over all previous states. I recommend that--------  authors rephrase statement more carefully.----------------- It seems strange to use A() as in ""actions"", then use d as ""decision"" for its--------  elements.----------------- I recommend adding i as an argument to the definition of the recurrence--------  function r(s) to make it clear that it\'s the subset of previous states at time--------  i, otherwise it looks like an undefined variable. A nice terse option is to--------  write r(s_i).----------------- Real numbers should be \\mathbb{R} not \\mathcal{R}.----------------- It\'s more conventional to use t for a time-step instead of i.----------------- Example 2: ""52 feature embeddings"" -> did you mean ""52-DIMENSIONAL feature--------  embeddings""?']",0.73583984375,0.264404296875,1,0.92529296875,0.07470703125,1,0.3037109375,0.6962890625,0,0.91357421875,0.0865478515625,1,0.3173828125,0.6826171875,0,0.58154296875,0.418212890625,1
17,17,17,17,17,17,https://openreview.net/forum?id=BymIbLKgl,"Seeing the MPEG-7 dataset and references to curvature scale space brought to mind the old saying that ""if it's not worth doing, it's not worth doing well."".I wouldn't mind seeing this paper accepted, since it's different from the mainstream, but I worry about there being too narrow an audience at ICLR that still cares about this type of shape representation..-Furthermore the paper misses a more thorough quantitative evaluation and concentrates more on showing particular examples, instead of measuring more robust statistics over multiple curves (invariance to noise and sampling artifacts).

Fair enough..I brought up the question of ""why use this representation"" with the authors and they said their ""main purpose was to connect the theory of differential geometry of curves with the computational engine of a convolutional neural network."".Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves.","['I\'m torn on this one. Seeing the MPEG-7 dataset and references to curvature scale space brought to mind the old saying that ""if it\'s not worth doing, it\'s not worth doing well."" There is no question that the MPEG-7 dataset/benchmark got saturated long ago, and it\'s quite surprising to see it in a submission to a modern ML conference. I brought up the question of ""why use this representation"" with the authors and they said their ""main purpose was to connect the theory of differential geometry of curves with the computational engine of a convolutional neural network."" Fair enough. I agree these are seemingly different fields, and the authors deserve some credit for connecting them. If we give them the benefit of the doubt that this was worth doing, then the approach they pursue using a Siamese configuration makes sense, and their adaptation of deep convnet frameworks to 1D signals is reasonable. To the extent that the old invariant based methods made use of smoothed/filtered representations coupled with nonlinearities, it\'s sensible to revisit this problem using convnets. I wouldn\'t mind seeing this paper accepted, since it\'s different from the mainstream, but I worry about there being too narrow an audience at ICLR that still cares about this type of shape representation.', 'Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves. With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants, as evaluated on few toy examples.----------------The paper is generally well written and shows an interesting application of the Siamese architecture. However, the experimental evaluation and the results show that these are rather preliminary results as not many of the choices are validated. My biggest concern is in the choice of the negative samples, as the network basically learns only to distinguish between shapes at different scales, instead of recognizing different shapes. It is well known fact that in order to achieve a good performance with the contrastive loss, one has to be careful about the hard negative sampling, as using too easy negatives may lead to inferior results. Thus, this may be the underlying reason for such choice of the negatives? Unfortunately, this is not discussed in the paper.----------------Furthermore the paper misses a more thorough quantitative evaluation and concentrates more on showing particular examples, instead of measuring more robust statistics over multiple curves (invariance to noise and sampling artifacts).----------------In general, the paper shows interesting first steps in this direction, however it is not clear whether the experimental section is strong and thorough enough for the ICLR conference. Also the novelty of the proposed idea is limited as Siamese networks are used for many years and this work only shows that they can be applied to a different task.', 'Pros : --------- New representation with nice properties that are derived and compared with a mathematical baseline and background--------- A simple algorithm to obtain the representation----------------Cons :--------- The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first.']",0.83203125,0.1678466796875,1,0.6044921875,0.3955078125,1,0.359375,0.640625,0,0.92919921875,0.07073974609375,1,0.3046875,0.6953125,0,0.491943359375,0.5078125,0
18,18,18,18,18,18,https://openreview.net/forum?id=H12GRgcxg,"---Cons: the results on mnist is all synthetic and it's hard to tell if this would translate to a win on real datasets..---This is a good paper where two main methods are proposed, the first one is a latent variable model and training would require the EM algorithm, alternating between estimating the true label and maximizing the parameters given a true label..-- comments:
---Equation 11 should be expensive, what happens if you are training on imagenet with 1000 classes?

This paper looks at how to train if there are significant label noise present..The effectiveness of the proposed method is hard to gauge, especially how to scale the proposed method to large number of classification targets and whether it is still effective..I think the authors can make the paper better by either demonstrating state-of-the-art results on a dataset known to have label noise, or demonstrate that a method can reliably estimate the true label corrupting probabilities.","[""This paper looks at how to train if there are significant label noise present.--------This is a good paper where two main methods are proposed, the first one is a latent variable model and training would require the EM algorithm, alternating between estimating the true label and maximizing the parameters given a true label.----------------The second directly integrates out the true label and simply optimizes the p(z|x).----------------Pros: the paper examines a training scenario which is a real concern for big dataset which are not carefully annotated.--------Cons: the results on mnist is all synthetic and it's hard to tell if this would translate to a win on real datasets.----------------- comments:--------Equation 11 should be expensive, what happens if you are training on imagenet with 1000 classes?--------It would be nice to see how well you can recover the corrupting distribution parameter using either the EM or the integration method. ----------------Overall, this is an OK paper. However, the ideas are not novel as previous cited papers have tried to handle noise in the labels. I think the authors can make the paper better by either demonstrating state-of-the-art results on a dataset known to have label noise, or demonstrate that a method can reliably estimate the true label corrupting probabilities."", 'The paper addressed the erroneous label problem for supervised training. The problem is well formulated and the presented solution is novel. ----------------The experimental justification is limited. The effectiveness of the proposed method is hard to gauge, especially how to scale the proposed method to large number of classification targets and whether it is still effective.----------------For example, it would be interesting to see whether the proposed method is better than training with only less but high quality data. ----------------From Figure 2, it seems with more data, the proposed method tends to behave very well when the noise fraction is below a threshold and dramatically degrades once passing that threshold. Analysis and justification of this behavior whether it is just by chance or an expected one of the method would be very useful. ---------------- ', ""This work address the problem of supervised learning from strongly labeled data with label noise. This is a very practical and relevant problem in applied machine learning.  The authors note that using sampling approaches such as EM isn't effective, too slow and cannot be integrated into end-to-end training. Thus, they propose to simulate the effects of EM by a noisy adaptation layer, effectively a softmax, that is added to the architecture during training, and is omitted at inference time. The proposed algorithm is evaluated on MNIST and shows improvements over existing approaches that deal with noisy labeled data.----------------A few comments.--------1. There is no discussion in the work about the increased complexity of training for the model with two softmaxes. ----------------2. What is the rationale for having consecutive (serialized) softmaxes, instead of having a compound objective with two losses, or a network with parallel losses and two sets of gradients?----------------3. The proposed architecture with only two hidden layers isn't not representative of larger and deeper models that are practically used, and it is not clear that shown results will scale to bigger networks. ----------------4. Why is the approach only evaluated on MNIST, a dataset that is unrealistically simple.""]",0.85009765625,0.1500244140625,1,0.81201171875,0.18798828125,1,0.2147216796875,0.78515625,0,0.9150390625,0.084716796875,1,0.186767578125,0.81298828125,0,0.34326171875,0.65673828125,0
19,19,19,19,19,19,https://openreview.net/forum?id=H1Go7Koex,"Character information has been applied in many previously published work, as cited by the authors..-To be specific:


-- I was confused by the contribution of this paper: character-aware word embedding or residual network or both?.---- This paper combines several components in the classification framework, including character-aware model for word embedding, residual network and attention weight in Type 1 feature.

Is there any explanation about what is captured in the residual component from the perspective of sentence modeling?.Even though the results show that adding residual network could help, I was still not be convinced..Although this paper shows that this new model could give the best results on several datasets, it lacks a strong evidence/intuition/motivation to support the network architecture.","['This paper proposes a new model for sentence classification. ----------------Pros:--------- Some interesting architecture choices in the network.----------------Cons:--------- No evaluation of the architecture choices. An ablation study is critical here to understand what is important and what is not.--------- No evaluation on standard datasets. On the only pre-existing dataset evaluated on a simple TFIDF-SVM method is state-of-the-art, so results are unconvincing.', 'This paper proposes a new neural network model for sentence representation. This new model is inspired by the success of residual network in Computer Vision and some observation of word morphology in Natural Language Processing. Although this paper shows that this new model could give the best results on several datasets, it lacks a strong evidence/intuition/motivation to support the network architecture.----------------To be specific:----------------- I was confused by the contribution of this paper: character-aware word embedding or residual network or both?--------- The claim of using residual network in section 3.3 seems pretty thin, since it ignores some fundamental difference between image representation and sentence representation. Even though the results show that adding residual network could help, I was still not be convinced. Is there any explanation about what is captured in the residual component from the perspective of sentence modeling?--------- This paper combines several components in the classification framework, including character-aware model for word embedding, residual network and attention weight in Type 1 feature. I would like to see the contribution from each of them to the final performance, while in Table 3 I only saw one of them. Is it possible to add more results on the ablation test?--------- In equation (5), what is the meaning of  in ?--------- The citation format is impropriate', 'This paper proposes a character-aware attention residual network for sentence embedding. Several text classification tasks are used to evaluate the effectiveness of the proposed model. On two of the three tasks, the residual network outforms a few baselines, but couldn\'t beat the simple TFIDF-SVM on the last one.----------------This work is not novel enough. Character information has been applied in many previously published work, as cited by the authors. Residual network is also not new.----------------Why not testing the model on a few more widely used datasets for short text classification, such as TREC? More competitive baselines can be compared to. Also, it\'s not clear how the ""Question"" dataset was created and which domain it is.----------------Last, it is surprising that the format of citations throughout the paper is all wrong. ----------------For example:--------like Word2Vec Mikolov et al. (2013)--------->--------like Word2Vec (Mikolov et al., 2013)----------------The citations can\'t just mix with the normal text. Please refer to other published papers.']",0.85009765625,0.1497802734375,1,0.83935546875,0.16064453125,1,0.1748046875,0.8251953125,0,0.92041015625,0.079833984375,1,0.34130859375,0.65869140625,0,0.39599609375,0.60400390625,0
20,20,20,20,20,20,https://openreview.net/forum?id=H1_EDpogx,"-For experiments, the choice of MNIST is somewhat bizarre: this task is small and performance is notoriously terrible when using linear approaches (the authors do not even report it).-Even though the researchers took an interesting approach to evaluate the performance of the system, it's difficult for me to grasp the expected practical improvements of this approach..-I don't consider myself an expert on this topic even though I have some experience with SystemC.

This paper is clear and well written in the present form and would probably mostly need a proper benchmark on a large scale linear task..I did not understand from the paper how it was thought that this could scale to contemporary scaled networks, in terms of numbers of parameters for both storage and bandwidth..Combining storage and processing capabilities is an interesting research topic because data transfer is a major issue for many machine learning tasks.","['For more than a decade, near data processing has been a key requirement for large scale linear learning platforms, as the time to load the data exceeds the learning time, and this has justified the introduction of approaches such as Spark----------------Deep learning usually deals with the data that can be contained in a single machine and the bottleneck is often the CPU-GPU bus or the GPU-GPU-bus, so a method that overcomes this bottleneck could be relevant.----------------Unfortunately, this work is still very preliminary and limited to linear training algorithms, so of little interest yet to ICLR readership. I would recommend publication to a conference where it can reach the large-scale linear ML audience first, such as ICML. This paper is clear and well written in the present form and would probably mostly need a proper benchmark on a large scale linear task. Obviously, when the authors have convincing DNN learning simulations, they are welcome to target ICLR, but can the flash memory FPGA handle it?----------------For experiments, the choice of MNIST is somewhat bizarre: this task is small and performance is notoriously terrible when using linear approaches (the authors do not even report it)', ""Combining storage and processing capabilities is an interesting research topic because data transfer is a major issue for many machine learning tasks.--------The paper itself is well-written, but unfortunately addresses a lot of things only to medium depth (probably due length constraints).--------My opinion is that a journal with an in-depth discussion of the technical details would be a better target for this paper.----------------Even though the researchers took an interesting approach to evaluate the performance of the system, it's difficult for me to grasp the expected practical improvements of this approach.--------With such a big focus on GPU (and more specialized hardware such as TPUs), the one question that comes to mind: By how much does this - or do you expect it to - beat the latest and greatest GPU on a real task?----------------I don't consider myself an expert on this topic even though I have some experience with SystemC."", ""While the idea of moving the processing for machine learning into silicon contained within the (SSD) data storage devices is intriguing and offers the potential for low-power efficient computation, it is a rather specialized topic, so I don't feel it will be of especially wide interest to the ICLR audience. The paper describes simulation results, rather than actual hardware implementation, and describes implementations of existing algorithms. --------The comparisons of algorithms' train/test performance does not seem relevant (since there is no novelty in the algorithms) and the use of a single layer perceptron on MNIST calls into question the practicality of the system, since this is a tiny neural network by today's standards. I did not understand from the paper how it was thought that this could scale to contemporary scaled networks, in terms of numbers of parameters for both storage and bandwidth. ----------------I am not an expert in this area, so have not evaluated in depth. ""]",0.94384765625,0.0560302734375,1,0.333251953125,0.66650390625,0,0.3232421875,0.6767578125,0,0.91845703125,0.08160400390625,1,0.1240234375,0.8759765625,0,0.345947265625,0.65380859375,0
21,21,21,21,21,21,https://openreview.net/forum?id=H1kjdOYlx,"It would be great to get a more detailed comparison to MAXQ based multiagent RL approaches, where the value function is explicitly decomposed..Eventually the sketches get mapped into real policies and enable policy transfer and temporal abstraction..It would have been useful to see this spelled out in the context of abstract SMDP models to see what they bring to the table.

The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL, based on actor-critic methods..The approach is illustrated in two tasks: gridworld with objects and a simplified Minecraft problem)..The idea of providing symbolic descriptions of tasks and learning corresponding ""implementations"" is potentially interesting and the empirical results are promising.","['The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL, based on actor-critic methods. The approach is illustrated in two tasks: gridworld with objects and a simplified Minecraft problem).  The idea of providing symbolic descriptions of tasks and learning corresponding ""implementations"" is potentially interesting and the empirical results are promising.  However, there are two main drawbacks of the current incarnation of this work.  First, the ideas presented in the paper have all been explored in other work (symbolic specifications, actor-critic, shared representations).  While related work is discussed, it is not really clear what is new here, and what is the main contribution of this work besides providing a new implementation of existing ideas in the context of deep learning. The main contribution if the work needs to be clearly spelled out.  Secondly, the approach presented relies crucially on curriculum learning (this is quite clear from the experiments).  While the authors argue that specifying tasks in simplified language is easy, designing a curriculum may in fact be pretty complicated, depending on the task at hand.  The examples provided are fairly small, and there is no hint of how curriculum can be designed for larger problems. Because the approach is sensitive to the curriculum, this limits the potential utility of the work. It is also unclear if there is a way to provide supervision automatically, instead of doing it based on prior domain knowledge.--------More minor comments:--------- The experiments are not described in enough detail in the paper. It\'s great to provide github code, but one needs to explain in the paper why certain choices were made in the task setup (were these optimized? What\'s this the first thing that worked?) Even with the code, the experiments as described are not reproducible--------- The description of the approach is pretty tangled with the specific algorithmic choices. Can the authors step back and think more generally of how this approach can be formalized?  I think this would help relate it to the prior work more clearly as well.', 'This paper studies the problem of abstract hierarchical multiagent RL with policy sketches, high level descriptions of abstract actions. The work is related to much previous work in hierarchical RL, and adds some new elements by using neural implementations of prior work on hierarchical learning and skill representations. ----------------Sketches are sequences of high level symbolic labels drawn from some fixed vocabulary, which initially are devoid of any meaning. Eventually the sketches get mapped into real policies and enable policy transfer and temporal abstraction. Learning occurs through a variant of the standard actor critic architecture. ----------------Experiments are provided through a standard game like domain (maze, minecraft etc.). ----------------The paper as written suffers from two problems. One, the idea of policy sketches is nice, but not sufficiently fleshed out to have any real impact. It would have been useful to see this spelled out in the context of abstract SMDP models to see what they bring to the table. What one gets here is some specialized invocation of this idea in the context of the specific approach proposed here. Second, the experiments are not thorough enough in terms of comparing with all the related work. For example, Ghavamzadeh et al. explored the use of MAXQ like abstractions in the context of mulitagent RL. It would be great to get a more detailed comparison to MAXQ based multiagent RL approaches, where the value function is explicitly decomposed. ', ""The paper proposes a new RL architecture that aims at learning policies from sketches i.e sequence of high-level operations to execute for solving a particular task. The model relies on a hierarchical structure where the sub-policy is chosen depending on the current operation to execute in the sketch . The learning algorithm is based on an extension of the actor-critic model for that particular case, and also involves curriculum learning techniques when the task to solve is hard. Experimental results are provided on different learning problems and compared to baseline methods. ----------------The paper is well-written and very easy to follow. I am not really convinced by the impact of such a paper since the problem solved here can be seen as an option-learning problem with a richer supervision (i.e the sequence of option is given). It thus corresponds to an easier problem with a limited impact. Moreover, I do not really understand to which concrete application this setting corresponds. For example, learning from natural langage instructions is clearly more relevant. So since the model proposed in this article is not a major contribution and shares many common ideas with existing hierarchical reinforcement learning methods,  the paper lacks a strong motivation and/or concrete application. So, the paper only has a marginal interest for the RL community----------------@pros: --------* Original problem with well design experiments--------* Simple adaptation of the actor-critic method to the problem of learning sub policies------------------------@cons:--------* Very simple task that can be seen as a simplification of more complex problems like options discovery, hierarchical RL or learning from instructions--------* No strong underlying applications that could help to 'reinforce' the interest of the approach""]",0.88037109375,0.11962890625,1,0.865234375,0.1346435546875,1,0.349365234375,0.65087890625,0,0.9091796875,0.09088134765625,1,0.125,0.875,0,0.455078125,0.544921875,0
22,22,22,22,22,22,https://openreview.net/forum?id=H1oyRlYgg,"-Pros and Cons:
---Although there is little novelty in the paper, I think the work is of great value in shedding light into some interesting questions around generalization of deep networks..-Significance:
---I think such results may have impact on both theory and practice, respectively by suggesting what assumptions are legitimate for real scenarios for building new theories, or be used heuristically to develop new algorithms with generalization by smart manipulation of mini-batch sizes..-Comments:
---Earlier I had some concern about the correctness of a claim made by the authors, which is resolved now.

The paper is an empirical study to justify that: 1..SGD with smaller batch sizes converges to flatter minima, 2. flatter minima have better generalization ability..Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well","['Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well', 'The paper is an empirical study to justify that: 1. SGD with smaller batch sizes converges to flatter minima, 2. flatter minima have better generalization ability. ----------------Pros and Cons:--------Although there is little novelty in the paper, I think the work is of great value in shedding light into some interesting questions around generalization of deep networks. ----------------Significance:--------I think such results may have impact on both theory and practice, respectively by suggesting what assumptions are legitimate for real scenarios for building new theories, or be used heuristically to develop new algorithms with generalization by smart manipulation of mini-batch sizes.----------------Comments:--------Earlier I had some concern about the correctness of a claim made by the authors, which is resolved now. They had claimed their proposed sharpness criterion is scale invariance. They took care of it by removing this claim in the revised version.', 'I think that the paper is quite interesting and useful. --------It might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime one can get advantages of the SB regime.']",0.65087890625,0.34912109375,1,0.79736328125,0.2027587890625,1,0.050994873046875,0.94921875,0,0.88623046875,0.11383056640625,1,0.282958984375,0.71728515625,0,0.316650390625,0.68359375,0
23,23,23,23,23,23,https://openreview.net/forum?id=HJ0NvFzxl,"A description of the actual implementation would help  (no pointer to open source code is provide)..-The preliminary results do not tell us yet if the highly complex graph-based differentiable memory has more learning or generalization capacity than other approaches..This is still clearly promising.

The main contribution of this paper seems to be an introduction of a set of differential graph transformations which will allow you to learn graph->graph classification tasks using gradient descent..This maps naturally to a task of learning a cellular automaton represented as sequence of graphs..In that task, the graph of nodes grows at each iteration, with nodes pointing to neighbors and special nodes 0/1 representing the values.","['The main contribution of this paper seems to be an introduction of a set of differential graph transformations which will allow you to learn graph->graph classification tasks using gradient descent. This maps naturally to a task of learning a cellular automaton represented as sequence of graphs. In that task, the graph of nodes grows at each iteration, with nodes pointing to neighbors and special nodes 0/1 representing the values. Proposed architecture allows one to learn this sequence of graphs, although in the experiment, this task (Rule 30) was far from solved.----------------This idea is combined with ideas from previous papers (GGS-NN) to allow the model to produce textual output rather than graph output, and use graphs as intermediate representation, which allows it to beat state of the art on BaBi tasks. ', 'This paper proposes learning on the fly to represent a dialog as a graph (which acts as the memory), and is first demonstrated on the bAbI tasks. Graph learning is part of the inference process, though there is long term representation learning to learn graph transformation parameters and the encoding of sentences as input to the graph. This seems to be the first implementation of a differentiable memory as graph: it is much more complex than previous approaches like memory networks without significant gain in performance in bAbI tasks, but it is still very preliminary work, and the representation of memory as a graph seems much more powerful than a stack. Clarity is a major issue, but from an initial version that was constructive and better read by a computer than a human, the author proposed a hugely improved later version. This original, technically accurate (within what I understood) and thought provoking paper is worth publishing.----------------The preliminary results do not tell us yet if the highly complex graph-based differentiable memory has more learning or generalization capacity than other approaches. The performance on the bAbI task is comparable to the best memory networks, but still worse than more traditional rule induction (see http://www.public.asu.edu/~cbaral/papers/aaai2016-sub.pdf). This is still clearly promising.---------------- The sequence of transformation in algorithm 1 looks sensible, though the authors do not discuss any other operation ordering. In particular, it is not clear to me that you need the node state update step T_h if you have the direct reference update step T_h,direct. ----------------It is striking that the only trick that is essential for proper performance is the ‘direct reference’ , which actually has nothing to do with the graph building process, but is rather an attention mechanism for the graph input: attention is focused on words that are relevant to the node type rather than the whole sentence. So the question “how useful are all these graph operations” remain. A much simpler version of a similar trick may have been proposed in the context of memory networks, also for ICLR\'17 (see match type in ""LEARNING END-TO-END GOAL-ORIENTED DIALOG"" by Bordes et al)------------------------The authors also mention the time and size needed to train the model: is the issue arising for learning, inference or both? A description of the actual implementation would help  (no pointer to open source code is provide). The author mentions Theano in one of my questions: how are the transformations compiled in advance as units? How is the gradient back-propagated through the graph is this one is only described at runtime?------------------------Typo: in the appendices B.2 and B.2.1, the right side of the equation that applies the update gate has h’_nu while it should be h_nu.----------------In the references, the author could mention the pioneering work  of Lee Giles on representing graphs with  RNNs.----------------Revision: I have improved my rating for the following reasons:--------- Pointers to an highly readable and well structured Theano source is provided.--------- The delta improvement of the paper has been impressive over the review process, and I am confident this will be an impactful paper.--------- Much simpler alternatives approaches such as Memory Networks seem to be plateauing for problems such as dialog modeling, we need alternatives.--------- The architecture is this work is still too complex, but this is often as we start with DNNs, and then find simplifications that actually improve performance', ""The paper proposes an extension of the Gated Graph Sequence Neural Network by including in this model the ability to produce complex graph transformations. The underlying idea is to propose a method that will be able build/modify a graph-structure as an internal representation for solving a problem, and particularly for solving question-answering problems in this paper. The author proposes 5 different possible differentiable transformations that will be learned on a training set, typically in a supervised fashion where the state of the graph is given at each timestep. A particular occurence of the model is presented that takes a sequence as an input a iteratively update an internal graph state to a final prediction, and which can be applied for solving QA tasks (e.g BaBi) with interesting results.----------------The approach  in this paper is really interesting since the proposed model is able to maintain a representation of its current state as a complex graph, but still keeping the property of being differentiable and thus easily learnable through gradient-descent techniques. It can be seen as a succesfull attempt to mix continuous and symbolic representations. It moreover seems more general that the recent attempts made to add some 'symbolic' stuffs in differentiable models (Memory networks, NTM, etc...) since the shape of the state is not fixed here and can evolve. My main concerns is about the way the model is trained i.e by providing the state of the graph at each timestep which can be done for particular tasks (e.g Babi) only, and cannot be the solution for more complex problems. My other concern is about the whole content of the paper that would perhaps best fit a journal format and not a conference format, making the article still difficult to read due to its density. ""]",0.90234375,0.09771728515625,1,0.92529296875,0.0748291015625,1,0.406982421875,0.59326171875,0,0.90673828125,0.093505859375,1,0.06939697265625,0.9306640625,0,0.336669921875,0.66357421875,0
24,24,24,24,24,24,https://openreview.net/forum?id=HJ1kmv9xx,"-Overall, this is a reasonable work that approaches an important problem from a new angle..-I would like to see some discussion about the choice of foreground+mask rather than just predicting foreground directly..-- What is the 3rd & 6th column in Fig 9?

Yet, I think sizable efforts remain needed to make it a generic methodology..How would the proposed method deal with such situations?.-Regarding the eval experiment using AMT it is not clear why it is better to provide the users with L2 minimized NN matches rather than random pairs.","['The paper presents an interesting framework for image generation, which stitches the foreground and background to form an image. This is obviously a reasonable approach there is clearly a foreground object. However, real world images are often quite complicated, which may contain multiple layers of composition, instead of a simple foreground-background layer. How would the proposed method deal with such situations?----------------Overall, this is a reasonable work that approaches an important problem from a new angle. Yet, I think sizable efforts remain needed to make it a generic methodology. ', 'The paper proposes a model for image generation where the back-ground is generated first and then the foreground is pasted in by generating first a foregound mask and corresponding appearance, curving the appearance image using the mask and transforming the mask using predicted affine transform to paste it on top of the image. Using AMTurkers the authors verify their generated images are selected 68% of the time as being more naturally looking than corresponding images from a DC-GAN model that does not use a figure-ground aware image generator.----------------The segmentations masks learn to depict objects in very constrained datasets (birds) only, thus the method appears limited for general shape datasets, as the authors also argue in the paper. Yet, the architectural contributions have potential merit.----------------It would be nice to see if multiple layers of foreground (occluding foregrounds) are ever generated with this layered model or it is just figure-ground aware.', 'The authors propose a method that generates naturally looking images by first generating the background and then conditioned on the previous layer one or multiple foreground objects. Additionally they add a image transformer layer that allows the model to more easily model different appearances.----------------I would like to see some discussion about the choice of foreground+mask rather than just predicting foreground directly. For MNIST, for example the foreground seems completely irrelevant. For CUB and CIFAR of course the fg adds the texture and color while the masks ensures a crisp boundary. --------- Is the mask a binary mask or a alpha blending mask?--------- I find the fact that the model learns to decompose images this nicely and learns to produce crisp foreground masks w/o too much spurious elements (though there are some in CIFAR) pretty fascinating.----------------The proposed evaluation metric makes sense and seems reasonable. However, AFAICT, theoretically it would be possible to get a high score even though the GAN produces images not recognizable to humans, but only to the classifier network that produces P_g. E.g. if the Generator encodes the class in some subtle way (though this shouldn\'t happen given the training with an adversarial network).----------------Fig 3 shows indeed nicely that the decomposition is much nicer when spatial transformers are used. However, it also seems to indicate that the foreground prediction and the foreground mask are largely redundant. For the final results the ""niceness"" of the decomposition appears to be largely irrelevant.----------------Furthermore, the transformation layer seems to have a small effect, judging from the transformed masked foreground objects. They are mainly scaled down.----------------- What is the 3rd & 6th column in Fig 9? It is not clear if the final composed images are really as bad as ""advertised"".----------------Regarding the eval experiment using AMT it is not clear why it is better to provide the users with L2 minimized NN matches rather than random pairs.----------------I assume that Tab 1 Adversarial Divergence for Real images was not actually evaluated? It would be interesting to see how close to 0 multiple differently initialized networks actually are. Also please mention how the confidences/std where generated, i.e. different training sets, initialisations, eval sets, and how many runs.']",0.82666015625,0.173583984375,1,0.95703125,0.042999267578125,1,0.297607421875,0.7021484375,0,0.91796875,0.082275390625,1,0.055267333984375,0.94482421875,0,0.244873046875,0.7548828125,0
25,25,25,25,25,25,https://openreview.net/forum?id=HJ5PIaseg,"Can we eliminate the length factor during the annotation?.Otherwise, it is not surprise that the correlation..A more reasonable way is to show the results both with and without averaging.

BLEU, based on N-gram overlap, etc.).This paper propose a new evaluation metric for dialogue systems, and show it has a higher correlation with human annotation..However, the proposed solution depends on a reasonably good dialogue model to begin with, which is not guaranteed, rendering the new metric possibly meaningless.","['This paper addresses the issue of how to evaluate automatic dialogue responses. This is an important issue because current practice to automatically evaluate (e.g. BLEU, based on N-gram overlap, etc.) is NOT correlated well with the desired quality (i.e. human annotation). The proposed approach is based on the use of an LSTM encoding of dialogue context, reference response and model response with appropriate scoring, with the essence of training one dialogue model to evaluate another model. However, the proposed solution depends on a reasonably good dialogue model to begin with, which is not guaranteed, rendering the new metric possibly meaningless. ', 'This paper propose a new evaluation metric for dialogue systems, and show it has a higher correlation with human annotation. I agree the MT based metrics like BLEU are too simple to capture enough semantic information, but the metric proposed in this paper seems to be too compliciated to explain.----------------On the other hand, we could also use equation 1 as a retrieval based dialogue system. So what is suggested in this paper is basically to train one dialogue model to evaluate another model. Then, the high-level question is why we should trust this model? This question is also relevant to the last item of my detail comments.----------------Detail comments:----------------- How to justify what is captured/evaluated by this metric? In terms of BLEU, we know it actually capture n-gram overlap. But for this model, I guess it is hard to say what is captured. If this is true, then it is also difficult to answer the question like: will the data dependence be a problem?--------- why not build model incrementally? As shown in equation (1), this metric uses both context and reference to compute a score. Is it possible to show the score function using only reference? It will guarantee this metric use the same information source as BLEU or ROUGE. --------- Another question about equation (1), is it possible to design the metric to be a nonlinear function. Since from what I can tell, the comparison between BLEU (or ROUGE) and the new metric in Figure 3 is much like a comparison between the exponential scale and the linear scale.--------- I found the two reasons in section 5.2 are not convincing if we put them together. Based on these two reasons, I would like to see the correlation with average score. A more reasonable way is to show the results both with and without averaging. --------- In table 6, it looks like the metric favors the short responses. If that is true, this metric basically does the opposite of BLEU, since BLEU will panelize short sentences. On the other hand, human annotators also tends to give short respones high scores, since long sentences will have a higher chance to contain some irrelevant words. Can we eliminate the length factor during the annotation? Otherwise, it is not surprise that the correlation.', 'Overall the paper address an important problem: how to evaluate more appropriately automatic dialogue responses given the fact that current practice to automatically evaluate (BLEU, METEOR, ...) is often insufficient and sometimes misleading. The proposed approach using an LSTM-based encoding of dialogue context, reference response and model response(s) that are then scored in a linearly transformed space. While the overall approach is simple it is also quite intuitiv and allows end-to-end training. As the authors rightly argue simplicity is a feature both for interpretation as well as for speed. ----------------The experimental section reports on quite a range of experiments that seem fine to me and aim to convince the reader about the applicability of the approach. As mentioned also by others more insights from the experiments would have been great. I mentioned an in-depth failure case analysis and I would also suggest to go beyond the current dataset to really show generalizability of the proposed approach. In my opinion the paper is somewhat weaker on that front that it should have been.----------------Overall I like the ideas put forward and the approach seems sensible though and the paper can thus be accepted. ']",0.3408203125,0.6591796875,0,0.9638671875,0.03631591796875,1,0.126953125,0.873046875,0,0.90087890625,0.0989990234375,1,0.130615234375,0.86962890625,0,0.310791015625,0.689453125,0
26,26,26,26,26,26,https://openreview.net/forum?id=HJWHIKqgl,"-Experiments only on toy data sets and on binarized MNIST


-It would be interesting to know in what way this approach fails on e.g..---3) By exploiting kernel in the objective, the generated algorithm, t-GMMN, training can be improved from the GMMN..---2) The algorithm is only tested on MNIST dataset as model criticism and learning objective.

This paper provides an interesting idea to use the optimized MMD for generative model evaluation and learning..Moreover, they also provided an efficient implementation of perturbation tests for empirical MMD..This is an interesting paper containing three contributions:


-1) An expression for the variance of the quadratic-time MMD estimate, which can be efficiently minimized for kernel selection.","['This paper provides an interesting idea to use the optimized MMD for generative model evaluation and learning. Starting from the test power, the authors justified the criterion. Moreover, they also provided an efficient implementation of perturbation tests for empirical MMD. ----------------Pros:----------------1) The criterion is principled which is derived from the test power. --------2) The criterion can be used to detect the difference template by incorporating ARD technique. --------3) By exploiting kernel in the objective, the generated algorithm, t-GMMN, training can be improved from the GMMN.----------------Cons:----------------1) How to train the provided t objective is not clear. --------2) The algorithm is only tested on MNIST dataset as model criticism and learning objective. Comprehensive empirical comparison to the state-of-the-art criteria, e.g., log-likelihood, and other learning objectives is missing. ', 'A well written paper that proposes to use MMD to distinguish generated and reference data. The primary contribution of this paper is to derive a way to optimize the MMD kernels to maximize the test power of the two sample test. ----------------Pros----------------Principled approach; derivations start from first principles and the theoretical results will probably be applicable to other applications of two sample tests.----------------Well written; puts the contributions and related approaches into context and explains connections to previous work; especially to GANs.----------------Cons: I don’t expect that this work will have a big impact in the field:----------------The two sample test are still quadratic in the number of samples. ----------------Experiments only on toy data sets and on binarized MNIST----------------It would be interesting to know in what way this approach fails on e.g. image data (or other complex, high dimensional data where neural network generalize well). I could imagine that the neural network based discriminators in GANs generalize better than kernel based MMD methods. I would like to see follow up work that investigates this in more detail (and potentially profes my intuition wrong).', 'This is an interesting paper containing three contributions:----------------1) An expression for the variance of the quadratic-time MMD estimate, which can be efficiently minimized for kernel selection.----------------2) Advanced computational optimizations for permutation tests for the quadratic MMD statistic.----------------3) Crystal-clear examples on the importance of reducing the variance in two-sample testing (Figure 2)----------------Regarding my criticisms,----------------1) The conceptual advances in this submission are modest: the use of MMD to train generative models, as well as the importance of variance-reduction in MMD were already known.----------------2) The quadratic-time MMD test may lead to a discriminator that is ""too good"" in practical applications. Since MMD quickly picks up on pixel-level artifacts, I wonder if its use would be possible to train generators properly on realistic (non-binarized) data. This of course could be addressed by regularizing (smoothing) the kernel bandwidths, and for sure raises an interesting question/trade-off in generative modeling. ----------------Overall, the submission is technically sound and well-written: I recommend it for publication in ICLR 2017.']",0.7451171875,0.2548828125,1,0.367919921875,0.6318359375,0,0.0838623046875,0.916015625,0,0.84423828125,0.156005859375,1,0.08392333984375,0.916015625,0,0.285888671875,0.71435546875,0
27,27,27,27,27,27,https://openreview.net/forum?id=HJpfMIFll,"""SemEval-2010 task 14: Word sense induction & disambiguation."".---3..-The paper is a little dense reading at times, and some things are hard to understand, but the perspective is original enough and the results are good enough that I think it belongs in ICLR.

On the plus side, the paper proposes a mathematically interesting model for a context of a word (i.e., a Grassmanian manifold)..The experiments in this paper done on SemEval-2010 are not very persuasive..The paper's result with ""2 clusters"" (with an average of about 1.9) seems to be close to MFS.","['On the plus side, the paper proposes a mathematically interesting model for a context of a word (i.e., a Grassmanian manifold).----------------On the minus side, the paper mostly ignores the long history of Word Sense Induction (WSI) and Word Sense Disambiguation (WSD), citing and comparing only some relatively recent papers. The experiments in this paper done on SemEval-2010 are not very persuasive. (It\'s difficult to evaluate the experiments done on the 2016 data, since they are not directly comparable to published results).----------------For example, going back to the SemEval-2010 WSI task in [1], the best system seems to be UoY [2]. The F-measure seems to be a poor metric: always assigning one sense to every word (""MFS"") yields the highest F-measure of 63.5%. The paper\'s result with ""2 clusters"" (with an average of about 1.9) seems to be close to MFS. So I don\'t think we can use F-measure to compare.----------------The V-measure seems to be tilted towards systems that have high number of senses per word. UoY has V=15.7%, while the paper (with ""5 clusters"") has 14.4%. That isn\'t very convincing that the proposed method has captured the geometry of polysemy.----------------In general, I have often wondered why people work on pure unsupervised WSI and WSD. The assessment is very difficult (as described above). More importantly, some very weakly supervised systems (with minimal labels) can work pretty well to bootstrap. See, e.g., the classic paper [3].----------------If the authors used the Grassmannian idea to solve higher-level NLP problems directly (such as analogies), that would be very persuasive. However, that\'s a very different paper than what was submitted. For an example of application of Grassmannian manifolds to analogies, see [4].----------------References:--------1. Manandhar, Suresh, et al. ""SemEval-2010 task 14: Word sense induction & disambiguation."" Proceedings of the 5th international workshop on semantic evaluation. Association for Computational Linguistics, 2010.--------2. Korkontzelos, Ioannis, and Suresh Manandhar. ""Uoy: Graphs of unambiguous vertices for word sense induction and disambiguation."" Proceedings of the 5th international workshop on semantic evaluation. Association for Computational Linguistics, 2010.--------3. Yarowsky, David. ""Unsupervised word sense disambiguation rivaling supervised methods."" Proceedings of the 33rd annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1995.--------4. Mahadevan, Sridhar,  and Sarath Chandar Reasoning about Linguistic Regularities in Word Embeddings using Matrix Manifolds https://arxiv.org/abs/1507.07636 ', 'This paper describe a new method to capture word polysemy with word embeddings. In order to disambiguate a word in a given sentence, the word is represented by the subspace spanned by the word vectors of the context in which it appears. This departs from a traditional approach were the context is represented as a (weighted) sum of the word vectors. A clustering algorithm (very similar to k-means), is then used to cluster the different usages of a given word, and discover the different senses (each sense corresponding to a cluster). The proposed method is evaluated on various word sense induction datasets. It is compared to other word embedding techniques which model word polysemy.----------------The method proposed in the paper to represent words in context is really interesting, simple to apply and seems very effective, based on the strong experimental results reported in the paper. My main concern about this paper is the writing, which is sometimes a bit verbose, making it hard to follow the description of the method. Some of the justification (""intersection hypothesis"", ""polysemy intersection hypothesis"") might feel a bit like hand waving.----------------Overall, the work presented in the paper looks solid.----------------Pros:-------- - I really liked the idea of representing a word in context by a subspace (as opposed to a weighted sum). Indeed, such representations captures much more information than a single vector.-------- - The proposed method also obtain very good results, compared to existing polysemous word embeddings.-------- - It can be used with any word vectors, making its application very easy.----------------Cons:-------- - I felt that the paper is sometimes a bit verbose and some justifications might be a bit hand waving.-------- - I am also wondering how much of the improvement over existing approaches is due to the quality of the word2vec embeddings, or due to the proposed approach. It would therefore be nice to have a comparison with a regular k-means approach, where context are represented as sum of word vectors using the same embeddings.', 'This paper presents a study of the spaces around existing word embeddings. I proposes something unorthodox: instead of representing a word token by a vector, represent it by the subspace spanned by embeddings of the context word types around that token. These subspaces are fairly low-dimensional and are shown to capture some notions of polysemy (subspaces for tokens of the same sense should all roughly intersect in the same direction). While thinking about the subspace spanned by the context is fairly similar to thinking about a linear combination of the context embeddings, the subspace picture allows for a little more information to be preserved which can improve downstream semantic tasks.----------------The paper is a little dense reading at times, and some things are hard to understand, but the perspective is original enough and the results are good enough that I think it belongs in ICLR.']",0.88525390625,0.1148681640625,1,0.9404296875,0.05950927734375,1,0.37109375,0.62890625,0,0.7939453125,0.2061767578125,1,0.0504150390625,0.94970703125,0,0.291259765625,0.708984375,0
28,28,28,28,28,28,https://openreview.net/forum?id=HJrDIpiee,"-The paper is well written and the use of traces in deep RL is indeed underexplored, but the experiments in the paper are too limited and do not answer the most interesting questions..-The experiments provide quantitative results and detailed qualitative intuition for how and why the methods perform as they do..-As pointed out in the questions, n-step returns have been shown to work better than 1-step returns both in the classical RL literature and more recently with deep networks.

This direction is worth exploring, and the experiments demonstrate the benefit from using eligibility traces and Adam on two Atari games..-[1] ""Asynchronous Methods for Deep Reinforcement Learning"", ICML 2016..Showing error bars from multiple random seeds would also improve the paper.","['This paper combines DRQN with eligibility traces, and also experiment with the Adam optimizer for optimizing the q-network. This direction is worth exploring, and the experiments demonstrate the benefit from using eligibility traces and Adam on two Atari games. The methods themselves are not novel. Thus, the primary contributions are (1) applying eligibility traces and Adam to DRQN and (2) the experimental evaluation. The paper is well-written and easy to understand.----------------The experiments provide quantitative results and detailed qualitative intuition for how and why the methods perform as they do. However, with only two Atari games in the results, it is difficult to tell how well it the method would perform more generally. Showing results on several more games and/or other domains would significantly improve the paper. Showing error bars from multiple random seeds would also improve the paper.', ""The paper presents a deep RL with eligibility traces. The authors combine DRQN with eligibility traces for improved training. The new algorithm is evaluated on a two problems, with a single set of hyper-parameters, and compared with DQN.----------------The topic is very interesting. Adding eligibility traces to RL updates is not novel, but this family of the algorithms have not been explored for deep RL. The paper is written clearly, and the related literature is well-covered. More experiments would make this promising paper much stronger. As this is an investigative, experimental paper, it is crucial for it to contain a wider range of problems, different hyper-parameter settings, and comparison with vanilla DRQN, Deepmind's DQN implementation, as well as other state of the art methods. "", 'This paper investigates the use of eligibility traces with recurrent DQN agents. As in other recent work on deep RL, the forward view of Sutton and Barto is used to make eligibility traces practical to use with neural networks. Experiments on the Atari games Pong and Tennis show that traces work better than standard Q-learning.----------------The paper is well written and the use of traces in deep RL is indeed underexplored, but the experiments in the paper are too limited and do not answer the most interesting questions.----------------As pointed out in the questions, n-step returns have been shown to work better than 1-step returns both in the classical RL literature and more recently with deep networks. [1] shows that using n-step returns in the forward view with neural networks leads to big improvements on both Atari and TORCS. Their n-step Q-learning method also combines returns of different length in expectation, while traces do this explicitly. This paper does not compare traces with n-step returns and simply shows that traces used in the forward view help on two Atari games. This is not a very significant result. It would be much more interesting to see whether traces improve on what is already known to work well with neural networks.----------------The other claimed contribution of the paper is showing the strong effect of optimization. As with traces, I find it hard to make any conclusions from experiments on two games with fixed hyperparameter settings. This has already been demonstrated with much more thorough experiments in other papers. One could argue that these experiments show that importance of hyperparameter values and not of the optimization algorithm itself. Without tuning the optimization hyperparameters it\'s hard to claim anything about the relative merits of the methods.----------------[1] ""Asynchronous Methods for Deep Reinforcement Learning"", ICML 2016.']",0.94580078125,0.0543212890625,1,0.9599609375,0.03985595703125,1,0.56103515625,0.43896484375,1,0.91796875,0.08221435546875,1,0.236572265625,0.76318359375,0,0.475341796875,0.5244140625,0
29,29,29,29,29,29,https://openreview.net/forum?id=Hk3mPK5gg,"The results are relatively unsurprising..Maybe they are novel for this problem, though..-The enhancements together lead to a clear win as demonstrated by the competition results.

The paper describes approaches taken to train learning agents for the 3D game Doom..The authors propose a number of performance enhancements (curriculum learning, attention (zoomed-in centered) frames, reward shaping, game variables, post-training rules) inspired by domain knowledge..From Fig 4, the curriculum learning clearly helps with learning over increasingly difficult settings.","[""The paper describes approaches taken to train learning agents for the 3D game Doom. The authors propose a number of performance enhancements (curriculum learning, attention (zoomed-in centered) frames, reward shaping, game variables, post-training rules) inspired by domain knowledge.----------------The enhancements together lead to a clear win as demonstrated by the competition results. From Fig 4, the curriculum learning clearly helps with learning over increasingly difficult settings. A nice result is that there is no overfitting to the harder classes once they have learned (probably because the curriculum is health and speed). The authors conclude from Fig 5 that the adaptive curriculum is better and more stable that pure A3C; however, this is a bit of a stretch given that graph. They go on to say that Pure A3C doesn't learn at all in the harder map but then show no result/graph to back this claim. Tbl 5 shows a clear benefit of the post-training rules.----------------If the goal is to solve problems like these (3D shooters), then this paper makes a significant contribution in that it shows which techniques are practical for solving the problem and ultimately improving performance in these kinds of tasks. Still, I am just not excited about this paper, mainly because it relies so heavily of many sources of domain knowledge, it is quite far from the pure reinforcement learning problem. The results are relatively unsurprising. Maybe they are novel for this problem, though.----------------I'm not sure we can realistically draw any conclusions about Figure 6 in the paper's current form. I recommend the authors increase the resolution or run some actual metrics to determine the fuzziness/clarity of each row/image: something more concrete than an arrow of already low-resolution images.------------------- Added after rebuttal:----------------I still do not see any high-res images for Figure 6 or any link to them, but I trust that the authors will add them if accepted."", 'This paper basically applies A3C to 3D spatial navigation tasks. ----------------- This is not the first time A3C has been applied to 3D navigation. In fact the original paper reported these experiments. Although the experimental results are great, I am not sure if this paper has any additional insights to warrant itself as a conference paper. It might make more sense as a workshop paper-----------------  Are the graphs in Fig 5 constructed using a single hyper-parameter sweep? I think the authors should report results with many random initializations to make the comparisons more robust----------------- Overall the two main ideas in this paper -- A3C and curriculums -- are not really novel but the authors do make use of them in a real system. ', 'This is a solid paper that applies A3C to Doom, enhancing it with a collection of tricks so as to win one of the VizDoom competitions. I think it is fair to expect the competition aspect to overshadow the more scientific approach of justifying every design decision in isolation, but in fact the authors do a decent job at the latter.----------------Two of my concerns have remained unanswered (see AnonReviewer2, below). ----------------In addition, the citation list is rather thin, for example reward shaping has a rich literature, as do incrementally more difficult task setups, dating back at least to Mark Ring’s work in the 1990s. There has also been a lot of complementary work on other FPS games. I’m not asking that the authors do any direct comparisons, but to give the reader a sense of context in which to place this.']",0.92626953125,0.073974609375,1,0.75,0.250244140625,1,0.261474609375,0.73828125,0,0.908203125,0.0919189453125,1,0.1251220703125,0.875,0,0.404052734375,0.59619140625,0
30,30,30,30,30,30,https://openreview.net/forum?id=Hk4kQHceg,"Authors show experimental results on character-level language modeling tasks..-At the fourth paragraph of Section 4.4, where the authors connect dynamic evaluation with fast weights is misleading..For instance, in machine translation, we don't know what's the best translation at the end, unlike weather forecasting.

* Brief Summary: 


-This paper explores an extension of multiplicative RNNs to the LSTM type of models..The resulting proposal is very similar to [1]..---- The results are encouraging.","['* Brief Summary: ----------------This paper explores an extension of multiplicative RNNs to the LSTM type of models. The resulting proposal is very similar to [1]. Authors show experimental results on character-level language modeling tasks. In general, I think the paper is well-written and the explanations are quite clear.----------------* Criticisms:----------------- In terms of contributions, the paper is weak. The motivation makes sense, however, very similar work has been done in [1] and already an extension over [2]. Because of that this paper mainly stands as an application paper.--------- The results are encouraging. On the other hand, they are still behind the state of art without using dynamic evaluation. --------- There are some non-standard choices on modifications on the standard algorithms, such as ""l"" parameter of RMSProp and multiplying output gate before the nonlinearity.--------- The experimental results are only limited to character-level language modeling only. ----------------* An Overview of the Review:----------------Pros:--------- A simple modification that seems to reasonably well in practice.--------- Well-written.----------------Cons:--------- Lack of good enough experimental results.--------- Not enough contributions (almost trivial extension over existing algorithms).--------- Non-standard modifications over the existing algorithms.----------------[1] Wu Y, Zhang S, Zhang Y, Bengio Y, Salakhutdinov RR. On multiplicative integration with recurrent neural networks. InAdvances in Neural Information Processing Systems 2016 (pp. 2856-2864).--------[2] Sutskever I, Martens J, Hinton GE. Generating text with recurrent neural networks. InProceedings of the 28th International Conference on Machine Learning (ICML-11) 2011 (pp. 1017-1024).', ""This paper proposes an extension of the multiplicative RNN [1] where the authors apply the same reparametrization trick to the weight matrices of the LSTM. ----------------The paper proposes some interesting tricks, but none of them seems to be very crucial. For instance, in Eq. (16), the authors propose to multiply the output gate inside the activation function in order to alleviate the saturation problem in logistic sigmoid or hyperbolic tangent. Also, the authors share m_t across the inference of different gating units and cell-state candidates, at the end this brings only 1.25 times increase on the number of model parameters. Lastly, the authors use a variant of RMSProp where they add an additional hyper-parameter  and schedule it across the training time. It would be nicer to apply the same tricks to other baseline models and show the improvement with regard to each trick.----------------With the new architectural modification to the LSTM and all the tricks combined, the performance is not as great as we would expect. Why didn’t the authors apply batch normalization, layer normalization or zoneout to their models? Was there any issue with applying one of those regularization or optimization techniques? ----------------At the fourth paragraph of Section 4.4, where the authors connect dynamic evaluation with fast weights is misleading. I find it a bit hard to connect dynamic evaluation as a variant of fast weights. Fast weights do not use test error signal. In the paper, the authors claim that “dynamic evaluation uses the error signal and gradients to update the weights, which potentially increases its effectiveness, but also limits its scope to conditional generative modelling, when the outputs can be observed after they are predicted”, and I am afraid to tell that this assumption is very misleading. We should never assume that test label information is given at the inference time. The test label information is there to evaluate the generalization performance of the model. In some applications, we may get the label information at test time, e.g., stock prediction, weather forecasting, however, in many other applications, we don’t. For instance, in machine translation, we don't know what's the best translation at the end, unlike weather forecasting. Also, it would be fair to apply dynamic evaluation to all the other baseline models as well to compare with the BPC score 1.19 achieved by the proposed mLSTM.----------------The quality of the work is not that bad, but the novelty of the paper is not that good either. The performance of the proposed model is oftentime worse than other methods, and it is only better when dynamic evaluation is coupled together. However, dynamic evaluation can improve the other methods as well.----------------[1] Ilya et al., “Generating Text with Recurrent Neural Networks”, ICML’11"", 'Pros:--------* Clearly written.--------* New model mLSTM which seems to be useful according to the results.--------* Some interesting experiments on big data.----------------Cons:--------* Number of parameters in comparisons of different models is missing.--------* mLSTM is behind some other models in most tasks.']",0.83154296875,0.1685791015625,1,0.95751953125,0.042510986328125,1,0.245849609375,0.75439453125,0,0.849609375,0.150390625,1,0.1219482421875,0.8779296875,0,0.460205078125,0.53955078125,0
31,31,31,31,31,31,https://openreview.net/forum?id=Hk8TGSKlg,"-Summary : this work introduces a novel memory based artificial neural network for reading comprehension..For the paper, I have some comments:


-1..According to my understanding, for the adaptive computation,  it would stop when the P_T <0.

I might be mistaken but I don't see you reporting anything on the actual number of loops necessary in the reported experiments..This paper proposed an iterative query updating mechanism for cloze-style QA..-3.","[""First I would like to apologize for the delay in reviewing.----------------Summary : this work introduces a novel memory based artificial neural network for reading comprehension. Experiments show improvement on state of the art.--------The originality of the approach seems to be on the implementation of an iterative procedure with a loop testing that the current answer is the correct one.----------------In order to get a better sense of the reason for improvement it would be interesting to have a complexity and/or a time analysis of the algorithm. I might be mistaken but I don't see you reporting anything on the actual number of loops necessary in the reported experiments.----------------The dataset description in section 2.2, should be moved to section 4 where the other datasets are described."", 'Thie paper proposed an iterative memory updating model for cloze-style question-answering task. The approach is interesting, and result is good. For the paper, I have some comments:----------------1. Actually the model in the paper is not single model, it proposed two models. One consists of ""reading"", ""writing"", ""adaptive computation"" and "" Answer module 2"", the other one is ""reading"", ""composing"", ""writing"", ""gate querying"" and ""Answer module 1"". Based on the method section and the experiment, it seems the ""adaptive computation"" model is simpler and performs better. And without two time memory update in single iteration and composing module, the model is similar to neural turing machine.----------------2. What is the MLP setting in the composing module? ----------------3. This paper tested different size of hidden state:[256, 368, 436, 512], I do not find any relation between those numbers, how could you find 436? Is there any tricks helping you find those numbers?----------------4. It needs more ablation study about using different T such as T=1,2..----------------5. According to my understanding, for the adaptive computation,  it would stop when the P_T <0. So what is the distribution of T in the testing data?', 'This paper proposed an iterative query updating mechanism for cloze-style QA. The approach is novel and interesting and while it is only verified in the paper for two Cloze-style tasks (CBT and WDW), the concept of read/compose/write operations seem to be more general and can be potentially applied to other reasoning tasks beyond Cloze-style QA. Another advantage of the proposed model is to learn when to terminate the iteration by the so-called adaptive computation model, such that it avoids the issue of treating the number of iterations as another hyper-parameter, which is a common practice of iterative models/multi-hop reasoning in previous papers.----------------There are a couple places that this paper can improve. First, I would like to see the results from CNN/Daily Mail as well to have a more comprehensive comparison. Secondly, it will be useful to visualize the entire M^q sequence over time t (not just z or the query gating) to help understand better the query regression and if it is human interpretable.']",0.38525390625,0.61474609375,0,0.85400390625,0.146240234375,1,0.11907958984375,0.880859375,0,0.84716796875,0.152587890625,1,0.23828125,0.76171875,0,0.281982421875,0.7177734375,0
32,32,32,32,32,32,https://openreview.net/forum?id=HkE0Nvqlg,"-This seems to be the first interesting use of the backprop of forward-backward/inside-outside (Stoyanov et al..These latent variables are modeled as a graphical model with potentials derived from a neural network..-Minor comments/typos:
---- last paragraph of sec 1: ""standard attention attention""
---- third paragraph of sec 3.2: ""the on log-potentials""
---- sec 4.1, Results: ""... as it has no information about the source ordering"" -- what do you mean here?

This is a very nice paper..The writing of the paper is clear..It starts from the traditional attention mechanism case.","['This is a very nice paper. The writing of the paper is clear. It starts from the traditional attention mechanism case. By interpreting the attention variable z as a distribution conditioned on the input x and query q, the proposed method naturally treat them as latent variables in graphical models. The potentials are computed using the neural network.----------------Under this view, the paper shows traditional dependencies between variables (i.e. structures) can be modeled explicitly into attentions. This enables the use of classical graphical models such as CRF and semi-markov CRF in the attention mechanism to capture the dependencies naturally inherit in the linguistic structures.----------------The experiments of the paper prove the usefulness of the model in various level — seq2seq and tree structure etc. I think it’s solid and the experiments are carefully done. It also includes careful engineering such as normalizing the marginals in the model.----------------In sum, I think this is a solid contribution and the approach will benefit the research in other problems.', 'This is a solid paper that proposes to endow attention mechanisms with structure (the attention posterior probabilities becoming structured latent variables). Experiments are shown with segmental atention (as in semi-Markov models) and syntactic attention (as in projective dependency parsing), both in a synthetic task (tree transduction) and real world tasks (neural machine translation and natural language inference). There is a small gain in using structured attention over simple attention in the latter tasks. A clear accept.----------------The paper is very clear, the approach is novel and interesting, and the experiments seem to give a good proof of concept. However, the use of structured attention in neural MT seems doesn\'t seem to be fully exploited here: segmental attention could be a way of approaching neural phrase-based MT, and syntactic attention offers a way of incorporating latent syntax in MT -- these seem very promising directions. In particular it would be interesting to try to add some (semi-)supervision on these attention mechanisms (e.g. posterior marginals computed by an external parser) to see if that helps learning the attention components of the network, or at least help initializing them. ----------------This seems to be the first interesting use of the backprop of forward-backward/inside-outside (Stoyanov et al. 2011). As stated in sec 3.3., for general probabilistic models the forward step over structured attention corresponds to the computation of first-order moments (posterior marginals) while the backprop step corresponds to second-order moments (gradients of marginals wrt log-potentials, i.e., Hessian of log-partition function). This extends the applicability of the proposed approach to arbitrary graphical models where these quantities can be computed efficiently. E.g. is there a generalized matrix-tree formula that allows to do backprop for non-projective syntax? On the negative side, I suspect the need for second-order statistics may bring some numerical instability in some problems, caused by the use of the signed log-space field. Was this seen in practice?----------------Minor comments/typos:--------- last paragraph of sec 1: ""standard attention attention""--------- third paragraph of sec 3.2: ""the on log-potentials""--------- sec 4.1, Results: ""... as it has no information about the source ordering"" -- what do you mean here?', 'The authors propose to extend the “standard” attention mechanism, by extending it to consider a distribution over latent structures (e.g., alignments, syntactic parse trees, etc.). These latent variables are modeled as a graphical model with potentials derived from a neural network.----------------The paper is well-written and clear to understand. The proposed methods are evaluated on various problems, and in each case the “structured attention” models outperform baseline models (either one without attention, or using simple attention). For the two real-world tasks, the improvements obtained from the proposed approach are relatively small compared to the “simple” attention models, but the techniques are nonetheless interesting.----------------Main comments:--------1. In the Japanese-English Machine Translation example, the relative difference in performance between the Sigmoid attention model, and the Structured attention model appears to be relatively small. In this case, I’m curious if the authors analyzed the attention alignments to determine whether the structured models resulted in better alignments. In other words, if ground-truth alignments are available for the dataset, or if they can be human-annotated for some test examples, it would be interesting to measure the quality of the alignments in addition to the BLEU metric.--------2. In the final experiment on natural language inference, I thought it was a bit surprising that using pretrained syntactic attention layers did not appear to improve model performance, but instead appear to degrade performance. I was curious if the authors have any hypotheses for why this is the case?----------------Minor comments:--------1. Typographical error: Equation 1: “p(z | x, q” → “p(z | x, q)”--------2. Section 3.3: “Past work has demonstrated that the techniques necessary for this approach, … ” →  “Past work has demonstrated the techniques necessary for this approach, … ”']",0.7158203125,0.2841796875,1,0.52001953125,0.47998046875,1,0.264892578125,0.73486328125,0,0.91796875,0.08197021484375,1,0.032623291015625,0.96728515625,0,0.25830078125,0.74169921875,0
33,33,33,33,33,33,https://openreview.net/forum?id=HkEI22jeg,"is introducing neural network modeling to a field that isn't currently using it, and where it should prove very effective..-I think it would be very interesting to see the results of applying a framework like this one with LFP and other neurons as input and on a shorter discretization time scale..While the results show that these simplified models can capture a lot of the spike train characteristics, couldn’t a model with free parameters eventually outperform this one (with correspondingly more training data)?

This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models..A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images..-Fig.","['This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models.  A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images.  The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models.----------------This work is an important step in understanding the nonlinear response properties of visual neurons.  Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models.  So this presents an important challenge to the field.  If we even had *a* model that works, it would be a starting point.  So this work should be seen in that light.  The challenge now of course is to tease apart what the rnn is doing.  Perhaps it could now be pruned and simplified to see what parts are critical to performance.  It would have been nice to see such an analysis.   Nevertheless this result is a good first start and I think important for people to know about.----------------I am a bit confused about what is being called a ""movie.""  My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each.  But then it is stated that the ""frame rate"" is 1/8.33 ms.  I think this must refer to the refresh rate of the monitor, right?   ----------------I would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies.  Here I would expect the rnn to have an even more profound effect, and potentially be much more informative.', 'This is a clearly written paper with a nice, if straightforward, result: RNNs can be good predictive models of neuron firing rates in the retina.----------------On the one hand, the primary scientific contribution seems to just be to confirm that this approach works. On this particular stimulus locked task the gains from using the RNN seemed relatively modest, and it hasn\'t yet taught us anything new about the biology.----------------On the other hand, this (along with the concurrent work of McIntosh et al.) is introducing neural network modeling to a field that isn\'t currently using it, and where it should prove very effective.----------------I think it would be very interesting to see the results of applying a framework like this one with LFP and other neurons as input and on a shorter discretization time scale.----------------I suspect followup work building on this proof of concept will be increasingly exciting.----------------Minor comments:--------Sec 3.2:--------I didn\'t understand the role of the 0.833 ms bins.--------Use ""epoch"" throughout, rather than alternating between ""epoch"" and ""pass through data"".----------------Fig. 4 would be better with the x-axis on a log scale.', 'This paper fits models to spike trains of retinal ganglion cells that are driven by natural images. I think the title should thus include the word “activity” at the end for otherwise it is actually formally incorrect.----------------Anyhow, this paper proposes more specifically a recurrent network for this time series prediction and compares it to what seems to be the previous approach of a generalized linear model. Overall the stated paradigm is that when one can predict the spikes well then one can look into the model and learn how nature does it. ----------------In general the paper sounds plausible, though I am not convinced that I learned a lot. The results in figure 2 show that the RNN model can predict the spikes a bit better. So this is nice. But now what? You have shown that a more complicated model can produce better fits to the data, though there are of course still some variations to the real data. Your initial outline was that a better predictive model helps you to better understand the neural processing in the retina. So tell us what you learned. I am not a specialist of the retina, but I know that there are several layers and recurrencies in the retina, so I am not so surprised that the new model is better than the GLM. ----------------It seems that more complicated recurrent models such as LSTM do not improve the performance according to a statement in the paper. However, comparisons on this level are also difficult as a more complex models needs more data. Hence, I would actually expect that more layers and even a more detailed model of the retina could eventually improve the prediction even further. --------I was also a bit puzzled that all the neurons in the network share all the same parameters (weights). While the results show that these simplified models can capture a lot of the spike train characteristics, couldn’t a model with free parameters eventually outperform this one (with correspondingly more training data)?']",0.66650390625,0.333740234375,1,0.88671875,0.1131591796875,1,0.1260986328125,0.8740234375,0,0.90771484375,0.0924072265625,1,0.1663818359375,0.83349609375,0,0.404296875,0.595703125,0
34,34,34,34,34,34,https://openreview.net/forum?id=HkIQH7qel,"-Given that there's no result on the test set reported, the grid search for hyperparameters on the dev set directly is also a concern, even though the authors did cross validation experiments..-It is unfortunate that the blind test results are not obtained yet due to the copyright issue..The authors proposed RASOR to address the problem of finding the best answer span according to a given question.

This paper proposes RaSoR, a method to efficiently representing and scoring all possible spans in an extractive QA task..While the test set results on SQuAD have not been released, it looks likely that they are not going to be state-of-the-art; with that said, the idea of enumerating all possible spans proposed in this paper could potentially improve many architectures..The paper is very well-written and the analysis/ablations in the final sections are mostly interesting (especially Figure 2, which confirms what we would intuitively believe).","['This paper proposes RaSoR, a method to efficiently representing and scoring all possible spans in an extractive QA task. While the test set results on SQuAD have not been released, it looks likely that they are not going to be state-of-the-art; with that said, the idea of enumerating all possible spans proposed in this paper could potentially improve many architectures. The paper is very well-written and the analysis/ablations in the final sections are mostly interesting (especially Figure 2, which confirms what we would intuitively believe). Based on its potential to positively impact other researchers working on SQuAD, I recommend that the paper is accepted. ', ""This paper presents an architecture for answer extraction task and evaluates on the SQUAD dataset. The proposed model builds fixed length representations of all spans in the answer document based on recurrent neural network. It outperforms a few baselines in exact match and F1 on SQUAD.----------------It is unfortunate that the blind test results are not obtained yet due to the copyright issue. There are quite a few other systems/submissions on the SQUAD leader board that were available for comparison.----------------Given that there's no result on the test set reported, the grid search for hyperparameters on the dev set directly is also a concern, even though the authors did cross validation experiments."", 'The authors proposed RASOR to address the problem of finding the best answer span according to a given question. The focus of the paper is mainly on how to model the relationship between question and the answer spans. The idea proposed by this paper is reasonable, but not ground breaking. The analysis is interesting and potentially useful. I would hope the authors can go extra miles to analyze different choices of boundary prediction models and make a more convincing case for the necessity of modeling the score of the span globally.----------------The main idea behind RASOR is to globally normalize and rank the scores of the possible answer spans. RASOR is able to achieve this by first modeling the hidden vectors of all words with LSTMs. Then, the representation of a text span is formed by concatenating the corresponding hidden vectors of the start and the end word of the corresponding chunk. The approach is reasonable, but not earth shattering. Also, the table 6 shows that the improvement over end-prediction point is not very large.----------------I appreciate the fact that authors conduct several analysis experiments as some of them are quite interesting. For example, it seems that question independent representation is also very import to the performance. In addition to the current analysis, I also want to get a clear idea on what makes the current model be better than the Match-LSTM. Is it hyper-parameter tuning? Or it is due to the use of the question independent representation?----------------Another good thing about the proposed model is that it is relatively simple, so there is a chance that the proposed techniques can be combined with other newly proposed ones. ']",0.91259765625,0.087158203125,1,0.48193359375,0.51806640625,0,0.42236328125,0.57763671875,0,0.90771484375,0.0924072265625,1,0.0718994140625,0.92822265625,0,0.25,0.75,0
35,35,35,35,35,35,https://openreview.net/forum?id=HksioDcxl,"Almahairi et al..-2) While modelling order in review text seems like the right choice, previous papers (e.g..2015) have shown that for rating prediction, modelling order in reviews might not be useful.

Interestingly, the model infers time-dependant user/item vectors by applying an RNN to previous histories..Those vectors are used to predict ratings, in a similar fashion to standard matrix factorization methods, and also bias a conditional RNN language model for reviews..The paper is well written and the architectural choices make sense.","['This paper proposes an RNN-based model for recommendation which takes into account temporal dynamics in user ratings and reviews. Interestingly, the model infers time-dependant user/item vectors by applying an RNN to previous histories. Those vectors are used to predict ratings, in a similar fashion to standard matrix factorization methods, and also bias a conditional RNN language model for reviews. The paper is well written and the architectural choices make sense. The main shortcomings of the paper are in the experiments:----------------1) The full model (rating+text) is only applied to one and relatively small dataset. Applying the model on multiple datasets with more data, e.g. Amazon reviews dataset (https://snap.stanford.edu/data/web-Amazon.html) would be more convincing.----------------2) While modelling order in review text seems like the right choice, previous papers (e.g. Almahairi et al. 2015) have shown that for rating prediction, modelling order in reviews might not be useful. A comparison with a similar model, but with bag-of-words reviews model would be nice in order to show the importance of the RNN-based review model, especially given previous literature.----------------Finally, this paper is an application paper applying well-established deep learning techniques, and I do not feel the paper offers new insights which the ICLR community in general would benefit from. This is not to undermine the importance of this paper, but I would like the authors to comment on why they think ICLR is a good avenue for their work.', 'This paper proposed a joint model for rate prediction and text generation.  The author compared the methods on a more realistic time based split setting, which requires “predict into the future.”--------One major flaw of the paper is that it does not address the impact of BOW vs the RNN based text model, specifically RRN(rating+text) already uses RNN for text modeling, so it is unclear whether the improvement comes from RNN(as opposed to BOW) or application of text information. A more clear study on impact of each component could make it more clear and benefit the readers.--------Another potential improvement direction of the paper is to support ranking objectives, as opposed to rate prediction, which is more realistic for recommendation settings.--------The overall technique is intuitive and novel, but can be improved to give more insights to the reader,.', 'The paper seeks to jointly model ratings and reviews in addition to temporal patterns. Existing papers have captured these aspects previously, but what\'s novel here is the particular combination of parts and choice of techniques. In particular, the use of LSTMs as opposed to ""bag-of-words"" models that have previously been used when combining the same components.----------------A criticism is made of existing models that use bag-of-words features as being too ""coarse"" to capture the real dynamics of reviews. This seems a valid criticsm, though it\'s not clear exactly what features those existing models miss. Something missing from this paper is any interpretation of what the model ""learns"" that may explain its better performance.----------------I also don\'t know about the significance of predicting ""future"" ratings as opposed to random splitting of the data. Lots of work on temporal recommender systems uses a variety of hold-out strategies besides random sampling, e.g. holding out the final ratings. Is there a methodological contribution associated with this change?----------------The experiments evaluate the extent to which the model achieves good performance due to its ability to capture evolving temporal patterns at the level of items, and the ability of the model to capture additional dynamics from text. Text makes a small but significant contribution; I was surprised by how large an improvement is achieved given how little text was included.----------------Overall this is a reasonably strong experimental comparison, though could be improved in two dimensions:--------(a) There\'s no comparison to models that use ratings + text (e.g. CoBaFi, JMARS, etc.). These methods have reported substantial improvements over rating-only models in the past, perhaps even larger improvements than what is reported here. Such an experiment is important given the claim that existing models (in particular bag-of-words models) are too coarse to capture the dynamics of text.--------(b) The restriction to movie datasets is okay, but these datasets are somewhat outliers when it comes to recommendation. In particular, they\'re extremely dense datasets that support very parameter-rich models. I\'d question whether the results would hold on sparser data (though in fact I\'d suspect they would hold, since on a sparser dataset the contribution due to using reviews ought to be larger).----------------Otherwise the experiments are fine. Perplexity results are nice but essentially what we\'d expect. It is a shame that unlike other papers that use text to inform recommender systems there\'s no high-level analysis here of what the model has uncovered.']",0.91796875,0.08209228515625,1,0.9482421875,0.0516357421875,1,0.222412109375,0.77783203125,0,0.81494140625,0.18505859375,1,0.171630859375,0.828125,0,0.35693359375,0.64306640625,0
36,36,36,36,36,36,https://openreview.net/forum?id=Hku9NK5lx,"-Regularly, the most frequent value in the weight matrix is set to zero to encourage sparsity..-I think the biggest fault this paper has is the number of different things going on in the approach that are not well explored independently..-The result tables are a bit confusing unfortunately.

This work introduces a number of techniques to compress fully-connected neural networks while maintaining similar performance, including a density-diversity penalty and associated training algorithm..Despite introducing a more efficient means of computing the gradient with respect to the diversity penalty, the authors still find it necessary to apply the penalty with some low probability (1-5%) per mini-batch..I wonder if the cost of having to sort weights (even for only 1 or 2 out of 100 mini-batches) might make this method intractable for larger networks.","['The method proposes to compress the weight matrices of deep networks using a new density-diversity penalty together with a computing trick (sorting weights) to make computation affordable and a strategy of tying weights.----------------This density-diversity penalty consists of an added cost corresponding to the l2-norm of the weights (density) and the l1-norm of all the pairwise differences in a layer.----------------Regularly, the most frequent value in the weight matrix is set to zero to encourage sparsity.----------------As weights collapse to the same values with the diversity penalty, they are tied together and then updated using the averaged gradient.----------------The training process then alternates between training with 1. the density-diversity penalty and untied weights, and 2. training without this penalty but with tied weights.----------------The experiments on two datasets (MNIST for vision and TIMIT for speech) shows that the method achieves very good compression rates without loss of performance.------------------------The paper is presented very clearly,  presents very interesting ideas and seems to be state of the art for compression. The approach opens many new avenues of research and the strategy of weight-tying may be of great interest outside of the compression domain to learn regularities in data.----------------The result tables are a bit confusing unfortunately.----------------minor issues:----------------p1--------english mistake: “while networks *that* consist of convolutional layers”.----------------p6-p7--------Table 1,2,3 are confusing. Compared to the baseline (DC), your method (DP) seems to perform worse:-------- In Table 1 overall, Table 2 overall FC, Table 3 overall, DP is less sparse and more diverse than the DC baseline. This would suggest a worse compression rate for DP and is inconsistent with the text which says they should be similar or better.--------I assume the sparsity value is inverted and that you in fact report the number of non-modal values as a fraction of the total.', ""This work introduces a number of techniques to compress fully-connected neural networks while maintaining similar performance, including a density-diversity penalty and associated training algorithm. The core technique of this paper is to explicitly penalize both the overall magnitude of the weights as well as diversity between weights. This approach results in sparse weight matrices comprised of relatively few unique values. Despite introducing a more efficient means of computing the gradient with respect to the diversity penalty, the authors still find it necessary to apply the penalty with some low probability (1-5%) per mini-batch.----------------The approach achieves impressive compression of fully connected layers with relatively little loss of accuracy. I wonder if the cost of having to sort weights (even for only 1 or 2 out of 100 mini-batches) might make this method intractable for larger networks. Perhaps the sparsity could help remove some of this cost?----------------I think the biggest fault this paper has is the number of different things going on in the approach that are not well explored independently. Sparse initialization, weight tying, probabilistic application of density-diversity penalty and setting the mode to 0, and alternating schedule between weight tied standard training and diversity penalty training. The authors don't provide enough discussion of the relative importance of these parts. Furthermore, the only quantitative metric shown is the compression rate which is a function of both sparsity and diversity such that they cannot be compared on their own. I would really like to see how each component of the algorithm affects diversity, sparsity, and overall compression. ----------------A quick verification: Section 3.1 claims the density-diversity penalty is applied with a fixed probability per batch while 3.4 implies structured phases alternating between application of density-diversity and weight tied standard cross entropy. Is this scheme in 3.4 only applying the density-diversity penalty probabilistically when it is in the density-diversity phase?----------------Preliminary rating:--------I think this is an interesting paper but lacks sufficient empirical evaluation of its many components. As a result, the algorithm has the appearance of a collection of tricks that in the end result in good performance without fully explaining why it is effective.----------------Minor notes:--------Please resize equation 4 to fit within the margins (\\resizebox{\\columnwidth}{!}{ blah } works well in latex for this)"", 'The paper shows promising results but it is difficult to read and follow. It presents different things closely related and it is difficult to asses the performance of each one. Diversity, sparsity, regularization term, tying weights. Anyway results are good.']",0.8994140625,0.1004638671875,1,0.92529296875,0.07452392578125,1,0.64111328125,0.359130859375,1,0.9130859375,0.0869140625,1,0.11700439453125,0.8828125,0,0.355224609375,0.64453125,0
37,37,37,37,37,37,https://openreview.net/forum?id=HyAbMKwxe,"The paper proposes new bounds on the misclassification error..-While the paper is easy to read and well-written overall, in a second read I found it difficult to fully understand because two problems are somewhat mixed together (here considering only binary classification for simplicity): 
---(a) the optimization of the classification error of a *randomized* classifier, which predicts 1 with probability P(1|x, theta), and 
---(b) the optimization of the deterministic classifier, which predicts sign(P(1|x, theta) - 0.5), in a way that is robust to outliers/underfitting..Experiments are performed to confirm the therotical intuition and motivation.

The paper analyses the misclassification error of discriminators and highlights the fact that while uniform probability prior of the classes makes sense early in the optimization, the distribution deviates from this prior significantly as the parameters move away from the initial values..---Consequently, the optimized upper bound (log-loss) gets looser..-As a fix, an optimization procedure based on recomputing the bound is proposed.","[""The paper analyses the misclassification error of discriminators and highlights the fact that while uniform probability prior of the classes makes sense early in the optimization, the distribution deviates from this prior significantly as the parameters move away from the initial values. --------Consequently, the optimized upper bound (log-loss) gets looser. ----------------As a fix, an optimization procedure based on recomputing the bound is proposed. The paper is well written. While the main observation made in this paper is a well-known fact, it is presented in a clear and refreshing way that may make it useful to a wide audience at this venue. ----------------I would like to draw the author's attention to the close connections of this framework with curriculum learning. More on this can be found in [1] (which is a relevant reference that should be cited). A discussion on this could enrich the quality of the paper. ----------------There is a large body of work on directly optimizing task losses[2][3] and the references therein. These should also be discussed and related particularly to section 3 (optimizing the ROC curve).----------------[1] Training Highly Multiclass Classifiers, Gupta et al. 2014.--------[2] Direct Loss Minimization for Structured Prediction, McAllester et al. --------[3] Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss, McAllester and Keshet.----------------Final comment:--------I believe the material presented in this paper is of interest to a wide audience at ICLR.--------The problem studied is interesting and the proposed approach is sound. --------I recommend to accept the paper and increase my score (from 7 to 8). "", 'The paper proposes new bounds on the misclassification error. The bounds lead to training classifiers with an adaptive loss function, and the algorithm operates in successive steps: the parameters are trained by minimizing the log-loss weighted by the probability of the observed class as given by the parameters of the previous steps. The bound improves on standard log-likelihood when outliers/underfitting prevents the learning algorithm to properly optimize the true classification error. Experiments are performed to confirm the therotical intuition and motivation. They show different cases where the new algorithm leads to improved classification error because underfitting occurs when using standard log-loss, and other cases where the new bounds do not lead to any improvement because the log-loss is sufficient to fit the dataset.----------------The paper also discusses the relationship between the proposed idea and reinforcement learning, as well as with classifiers that have an ""uncertain"" label. ----------------While the paper is easy to read and well-written overall, in a second read I found it difficult to fully understand because two problems are somewhat mixed together (here considering only binary classification for simplicity): --------(a) the optimization of the classification error of a *randomized* classifier, which predicts 1 with probability P(1|x, theta), and --------(b) the optimization of the deterministic classifier, which predicts sign(P(1|x, theta) - 0.5), in a way that is robust to outliers/underfitting. ----------------The reason why I am confused is that ""The standard approach to supervised classification"", as is mentioned in the abstract, is to use deterministic classifiers at test time, and the log-loss (up to constants) is an upper bound on the classification error of the deterministic classifier. However, the bounds discussed in the paper only concern the randomized classifier.----------------=== question:--------In the experiments, what kind of classifier is used? The randomized one (as would the sentence in the first page suggest ""Assuming the class is chosen according to p(y|X, θ)""), or the more standard deterministic classifier argmax_y P(y|x, theta) ?----------------As far as I can see, there are two cases: either (i) the paper deals with learning randomized classifiers, in which case it should compare the performances with the deterministic counterparts that people use in practice, or (ii) the paper makes sense as soon as we accept that the optimization of criterion (a) is a good surrogate for (b). In both cases,  I think the write-up should be made clearer (because in case (ii) the algorithm does not minimize an upper bound on the classification error, and in case (i) what is done does not correspond to what is usually done in binary classification). ----------------=== comments:--------- The section ""allowing uncertainty in the decision"" may be improved by adding some references, e.g. Bartlett & Wegkamp (2008) ""Classification with a Reject Option using a Hinge Loss"" or Sayedi et al. (2010) ""Trading off Mistakes and Don’t Know Predictions"".----------------- there seems to be a ""-"" sign missing in the P(1|x, theta) in L(theta, lambda) in Section 3.----------------- The idea presented in the paper is interesting and original. While I give a relatively low score for now, I am willing to increase this score if the clarifications are made.----------------Final comments:--------I think the paper is clear enough in its current form, even though there should still be improvement in the justification of why and to what extent the error of the randomized classifier is a good surrogate for the error of the true classifier. While the ""smoothed"" version of the 0/1 loss is an acceptable explanation in the standard classification setup, it is less clear in the section dealing with an additional ""uncertain"" label. I increase my score from 5 to 6.', 'The paper proposes an alternative to conditional max. log likelihood for training discriminative classifiers. The argument is that the conditional log. likelihood is an upper bound of the Bayes error which becomes lousy during training. The paper then proposes better bounds computed and optimized in an iterative algorithm. Extensions of this idea are developed for regularized losses and a weak form of policy learning. Tests are performed on different datasets.----------------An interesting aspect of the contribution is to revisit a well-accepted methodology for training classifiers. The idea looks fine and some of the results seem to validate it. This is however still a preliminary work and one would like to see the ideas pushed further. Globally, the paper lacks coherence and depth: the part on policy learning is not well connected to the rest of the paper and the link with RL is not motivated in the two examples (ROC optimization and uncertainties). The experimental part needs a rewriting, e.g. I did not find a legend for identifying the different curves in the figures, which makes difficult to appreciate the results.']",0.91943359375,0.08062744140625,1,0.68994140625,0.309814453125,1,0.57421875,0.42578125,1,0.89306640625,0.10687255859375,1,0.216552734375,0.78369140625,0,0.324462890625,0.67578125,0
38,38,38,38,38,38,https://openreview.net/forum?id=HyGTuv9eg,"---The paper claims that this 
---a) improves texture synthesis for textures with long-range regular structures, that are not preserved with the Gatys et al..that matches correlations between spatially shifted feature responses in addition to the correlations between feature responses at the same position in the feature maps..(http://arxiv.org/abs/1605.01141) that tackles the same problem by adding constraints to the Fourier spectrum of the synthesised texture shows comparable or better results while being far more efficient.

-I agree with claim a)..The authors suggest that, as future work, a possible evaluation method could be based on a classification task -- this is a potentially interesting approach that merits some further investigation..This issue is partially circumvented by performing inpainting experiments, by which the synthesised paths needs to stay coherent with the borders (as the authors did).","['The paper introduces a variation to the CNN-based texture synthesis procedure of Gatys et al. that matches correlations between spatially shifted feature responses in addition to the correlations between feature responses at the same position in the feature maps. --------The paper claims that this --------a) improves texture synthesis for textures with long-range regular structures, that are not preserved with the Gatys et al. method--------b) improves performance on texture inpainting tasks compared to the Gatys et al. method--------c) improves results in season transfer when combined with the style transfer method by Gatys et al. --------Furthermore the paper shows that--------d) by matching correlations between spatially flipped feature maps, symmetry properties around the flipping axis can be preserved.----------------I agree with claim a). However, the generated textures still have some issues such as greyish regions so the problem is not solved. Additionally, the procedure proposed is very costly which makes an already slow texture synthesis method substantially slower. For example, in comparison, the concurrent work by Liu et al. (http://arxiv.org/abs/1605.01141) that tackles the same problem by adding constraints to the Fourier spectrum of the synthesised texture shows comparable or better results while being far more efficient.----------------Also with b) the presented results constitute an improvement over the Gatys et al. method but again the results are not too exciting - one would not prefer this model to other inpainting algorithms.----------------With c) I don’t see a clear advantage of the proposed method to the existing Gatys et al. algorithm.----------------Finally, d) is a neat idea and the initial results look interesting but they don’t go much further than that. ----------------All in all I think it is decent work but neither its originality and technical complexity nor the quality of the results are convincing enough for acceptance.--------That said, I could imagine this to be a nice contribution to the workshop track though.', 'The paper investigates a simple extension of Gatys et al. CNN-based texture descriptors for image generation. Similar to Gatys et al., the method uses as texture descriptor the empirical intra-channel correlation matrix of the CNN feature response at some layer of a deep network. Differently from Gatys et al., longer range correlations are measured by introducing a shift between the correlated feature responses, which translates in a simple modification of the original architecture.----------------The idea is simple but has interesting effects on the generated textures and can be extended to transformations other than translation. While longer range correlations could be accounted for by considering the response of deeper CNN features in the original method by Gatys et al., the authors show that modelling them explicitly using shallower features is more effective, which is reasonable.----------------An important limitation that this work shares with most of its peers is the lack of a principled quantitative evaluation protocol, such that judging the effectiveness of the approach remains almost entirely a qualitative affair. While this should not be considered a significant drawback of the paper due to the objective difficulty of solving this open issue, nevertheless it is somewhat limiting that no principled evaluation method could be devised and implemented. The authors suggest that, as future work, a possible evaluation method could be based on a classification task -- this is a potentially interesting approach that merits some further investigation.', 'This paper proposes a modification of the parametric texture synthesis model of Gatys et al. to take into account long-range correlations of textures. To this end the authors add the Gram matrices between spatially shifted feature vectors to the synthesis loss. Some of the synthesised textures are visually superior to the original Gatys et al. method, in particular on textures with very structured long-range correlations (such as bricks).----------------The paper is well written, the method and intuitions are clearly exposed and the authors perform quite a wide range of synthesis experiments on different textures.----------------My only concern, which is true for all methods including Gatys et al., is the variability of the samples. Clearly the global minimum of the proposed objective is the original image itself. This issue is partially circumvented by performing inpainting experiments, by which the synthesised paths needs to stay coherent with the borders (as the authors did). There are no additional insights into this problem in this paper, which would have been a plus.----------------All in all, this work is a simple and nice modification of Gatys at al. which is worth publishing but does not constitute a major breakthrough.']",0.4794921875,0.5205078125,0,0.95654296875,0.043365478515625,1,0.1337890625,0.8662109375,0,0.81396484375,0.1859130859375,1,0.1661376953125,0.833984375,0,0.317138671875,0.6826171875,0
39,39,39,39,39,39,https://openreview.net/forum?id=HyNxRZ9xg,"The embedding vector is passed through a series of SUM/MULT gates and K-most important interactions are identified (K-max pooling)..This by itself is not grounds for rejection if the paper outperforms established baselines..When comparing among LR, FFM, CCPM, FNN, and proposed approach, the number of parameters (i.e., model capacity) are not shown.

A method for click prediction is presented..multiple layers) and the final feature is passed into a fully connected layer to output the click prediction rate..This process is repeated multiple times (i.e.","['A method for click prediction is presented. Inputs are a categorical variables and output is the click-through-rate. The categorical input data is embedded into a feature vector using a discriminative scheme that tries to predict whether a sample is fake or not. The embedding vector is passed through a series of SUM/MULT gates and K-most important interactions are identified (K-max pooling). This process is repeated multiple times (i.e. multiple layers) and the final feature is passed into a fully connected layer to output the click prediction rate. ----------------Authors claim:--------(1) Use of gates and K-max pooling allow modeling of interactions that lead to state of art results. --------(2) It is not straightforward to apply ideas in papers like word2vec to obtain feature embeddings and consequently they use the idea of discriminating between fake and true samples for feature learning. ----------------Theoretically convolutions can act as “sum” gates between pairs of input dimensions. Authors make these interactions explicit (i.e. imposed structure) by using gates. Now, the merit of the proposed method can be tested if a network using gates outperforms a network without gates. This baseline is critically missing – i.e. Embedding Vector followed by a series of convolution/pooling layers. ----------------Another related issue is that I am not sure if the number of parameters in the proposed model and the baseline models is similar or not. For instance – what is the total number of parameters in the CCPM model v/s the proposed model? ----------------Overall, there is no new idea in the paper. This by itself is not grounds for rejection if the paper outperforms established baselines.  However, such comparison is weak and I encourage authors to perform these comparisons. ', ""The paper proposes a way to learn continuous features for input data which consists of multiple categorical data. The idea is to embed each category in a learnable low dimensional continuous space, explicitly compute the pair-wise interaction among different categories in a given input sample (which is achieved by either taking a component-wise dot product or component-wise addition), perform k-max pooling to select a subset of the most informative interactions, and repeat the process some number of times, until you get the final feature vector of the given input. This feature vector is then used as input to a classifier/regressor to accomplish the final task. The embeddings of the categories are learnt in the usual way. In the experiment section, the authors show on a synthetic dataset that their procedure is indeed able to select the relevant interactions in the data. On one real world dataset (iPinYou) the model seems to outperform a couple of simple baselines. ----------------My major concern with this paper is that their's nothing new in it. The idea of embedding the categorical data having mixed categories has already been handled in the past literature, where essentially one learns a separate lookup table for each class of categories: an input is represented by concatenation of the embeddings from these lookup table, and a non-linear function (a deep network) is plugged on top to get the features of the input. The only rather marginal contribution is the explicit modeling of the interactions among categories in equations 2/3/4/5. Other than that there's nothing else in the paper. ----------------Not only that, I feel that these interactions can (and should) automatically be learned by plugging in a deep convolutional network on top of the embeddings of the input. So I'm not sure how useful the contribution is. ----------------The experimental section is rather weak. They authors test their method on a single real world data set against a couple of rather weak baselines. I would have much preferred for them to evaluate against numerous models proposed in the literature which handle similar problems, including wsabie. ----------------While the authors argued in their response that wsabie was not suited for their problem, i strongly disagree with that claim. While the original wsabie paper showed experiments using images as inputs, their training methodology can easily be extended to other types of data sets, including categorical data. For instance, I conjecture that the model i proposed above (embed all the categorical inputs, concatenate the embeddings, plug a deep conv-net on top and train using some margin loss) will perform as well if not better than the hand coded interaction model proposed in this paper. Of course I could be wrong, but it would be far more convincing if their model was tested against such baselines. "", 'In this paper, the author proposed an approach for feature combination of two embeddings v1 and v2. This is done by first computing the pairwise combinations of the elements of v1 and v2 (with complicated nonlinearity), and then pick the K-Max as the output vector. For triple (or higher-order) combinations, two (or more) consecutive pairwise combinations are performed to yield the final representations. It seems that the approach is not directly related to categorical data, and can be applied to any embeddings (even if they are not one-hot). So is there any motivation that brings about this particular approach? What is the connection? ----------------There are many papers with similar ideas. CCPM (A convolutional click prediction model) that the authors have compared against, also proposes very similar network structure (conv + K-max + conv + K-max). In the paper, the author does not mention their conceptual similarity and difference versus CCPM. Compact Bilinear Pooling, https://arxiv.org/abs/1511.06062 has been proposed a year ago and yields state-of-the-art performances in Visual Question Answering https://arxiv.org/abs/1606.01847. So the author might need to compare against those methods. I understand that the proposed approach incorporates more nonlinear operations (rather than bilinear) in pairwise combination, but it is not clear whether bilinear operations is sufficient to achieve the same level of performance, and whether complicated operations (e.g., Eqn. 4) are needed.----------------In the experiment, the performance seems to be not as impressive. There is about 1%-2% difference in performance between the proposed approach and baselines (e.g., in Tbl. 2, and Tbl. 3). Is that a big deal for click-rate prediction? When comparing among LR, FFM, CCPM, FNN, and proposed approach, the number of parameters (i.e., model capacity) are not shown. This could be unfair since the proposed model could have more parameters (note that the authors seems to misunderstand the questions). Besides, claiming that previous approaches does not learn representations seems to be a bit restrictive, since typical deep models learn the representation implicitly (e.g., CCPM and FNN listed in the paper as baselines). ']",0.51611328125,0.48388671875,1,0.7646484375,0.235107421875,1,0.0994873046875,0.900390625,0,0.85693359375,0.14306640625,1,0.04296875,0.95703125,0,0.302490234375,0.697265625,0
40,40,40,40,40,40,https://openreview.net/forum?id=HyenWc5gx,"For example Table 2 is missing ""joint training"", and Table 3 is missing GRU trained on the target task..leveraging a network trained on some original task A in learning a new task B, which not only improves performance on the new task B, but also tries to avoid degradation in performance on A..-Overall this is a nice paper but does not fully address how robust the proposed technique is.

The proposed technique is straightforward to describe and can also leverage external labeling systems perhaps based on logical rules..The paper is clearly written and the experiments seem relatively thorough..8 as ""joint training"".","['This paper proposes a method for transfer learning, i.e. leveraging a network trained on some original task A in learning a new task B, which not only improves performance on the new task B, but also tries to avoid degradation in performance on A. The general idea is based on encouraging a model trained on A, while training on the new task B, to match fake targets produced by the model itself but when it is trained only on the original task A.--------Experiments show that this method can help in improving the result on task B, and is better than other baselines, including standard fine-tuning.------------------------General comments/questions:--------- As far as I can tell, there is no experimental result supporting the claim that your model still performs well on the original task. All experiments show that you can improve on the new task only. --------- The introduction makes a strong statements about the distilling logical rule engine into a neural network, which I find a bit misleading. The approach in the paper is not specific to transferring from logical rules (as stated in the Sec 2) and is simply relying on the rule engine to provide labels for unlabelled data.--------- One of the obvious baselines to compare with your approach is standard multi-task learning on both tasks A and B together. That is, you train the model from scratch on both tasks simultaneously (which sharing parameters). It is not clear this is the same as what is referred to in Sec. 8 as ""joint training"". Can you please explain more clearly what you refer to as joint training?--------- Why can\'t we find the same baselines in both Table 2 and Table 3? For example Table 2 is missing ""joint training"", and Table 3 is missing GRU trained on the target task.--------- While the idea is presented as a general method for transfer learning, experiments are focused on one domain (sentiment analysis on SemEval task). I think that either experiments should include applying the idea on at least one other different domain, or the writing of the paper should be modified to make the focus more specific to this domain/task.------------------------Writing comments--------- The writing of the paper in general needs some improvement, but more specifically in the experiment section, where experiment setting and baselines should be explained more concisely.--------- Ensemble methodology paragraph does not fit the flow of the paper. I would rather explain it in the experiments section, rather than including it as part of your approach.--------- Table 1 seems like reporting cross-validation results, and I do not think is very informative to general reader.', 'This paper proposes a regularization technique for neural network training that relies on having multiple related tasks or datasets in a transfer learning setting. The proposed technique is straightforward to describe and can also leverage external labeling systems perhaps based on logical rules. The paper is clearly written and the experiments seem relatively thorough. ----------------Overall this is a nice paper but does not fully address how robust the proposed technique is. For each experiment there seems to be a slightly different application of the proposed technique, or a lot of ensembling and cross validation. I can’t figure out if this is because the proposed technique does not work well in general and thus required a lot of fiddling to get right in experiments, or if this is simply an artifact of ad-hoc experiments to try and get the best performance overall. If more datasets or addressing this issue directly in discussion was able to show this the strengths and limitations of the proposed technique more clearly, this could be a great paper. ----------------Overall the proposed method seems nice and possibly useful for other problems. However in the details of logical rule distillation and various experiment settings it seems like there is a lot of running the model many times or selecting a particular way of reusing the models and data that makes me wonder how robust the technique is or whether it requires a lot of trying various approaches, ensembling, or picking the best model from cross validation to show real gains. The authors could help by discussing this explicitly for all experiments in one place rather than listing the various choices / approaches in each experiment. As an example, these sorts of phrases make me very unsure how reliable the method is in practice versus how much the authors had to engineer this regularizer to perform well:--------“We noticed that equation 8 is actually prone to overfitting away from a good solution on the test set although it often finds a pretty good one early in training. “----------------The introduction section should first review the definitions of transfer learning vs multi-task learning to make the discussion more clear. It also deems justification why “catastrophic forgetting” is actually a problem. If the final target task is the only thing of interest then forgetting the source task is not an issue and the authors should motivate why forgetting matters in their setting. This paper explores sequential transfer so it’s not obvious why forgetting the source task matters.----------------Section 7 introduces the logical rules engine in a fairly specific context. Rather it would be good state more generally what this system entails to help people figure out how this method would apply to other problems.', 'This paper introduces a new method for transfer learning that avoids the catastrophic forgetting problem. --------It also describes an ensembling strategy for combining models that were learned using transfer learning from different sources.--------It puts all of this together in the context of recurrent neural networks for text analytics problems, to achieve new state-of-the-art results for a subtask of the SemEval 2016 competition.--------As the paper acknowledges, 1.5% improvement over the state-of-the-art is somewhat disappointing considering that it uses an ensemble of 5 quite different networks.----------------These are interesting contributions, but due to the many pieces, unfortunately, the paper does not seem to have a clear focus. From the title and abstract/conclusion I would\'ve expected a focus on the transfer learning problem. However, the description of the authors\' approach is merely a page, and its evaluation is only another page. In order to show that this idea is a new methodological advance, --------it would\'ve been good to show that it also works in at least one other application (e.g., just some multi-task supervised learning problem). Rather, the paper takes a quite domain-specific approach and discusses the pieces the authors used to obtain state-of-the-art performance for one problem. That is OK, but I would\'ve rather expected that from a paper called something like ""Improved knowledge transfer and distillation for text analytics"". If accepted, I encourage the authors to change the title to something along those lines.----------------The many pieces also made it hard for me to follow the authors\' train of thought. I\'m sure the authors had a good reason for their section ordering, but I didn\'t see the red thread in it. How about re-organizing the sections as follows to discuss one contribution at a time?--------1,2,4,3,8 including 6, put 9 into an appendix and point to it from here, 7, 5, 10. That would first discuss the transfer learning piece (4, and experiments potentially in a subsection with previous sections 3,8,6), then discuss the distillation of logical rules (7), and then discuss ensembling and experiments for it (5 and 10). One clue that the current structure is suboptimal is that there are 11 sections...----------------I like the authors\' idea for transfer learning without catastropic forgetting, and I must admit I would\'ve rather liked to read a paper solely about that (studying where it works, and where it fails) than about the many other topics of the paper. I weakly vote for acceptance since I like the ideas, but if the paper does not make it in, I would suggest that the authors consider splitting it into two papers, each of which could hopefully be more focused.  ']",0.47021484375,0.52978515625,0,0.900390625,0.09942626953125,1,0.1248779296875,0.875,0,0.83740234375,0.1627197265625,1,0.08001708984375,0.919921875,0,0.258544921875,0.74169921875,0
41,41,41,41,41,41,https://openreview.net/forum?id=Hyvw0L9el,"That said, I could be totally wrong, and the advantages stated in the paper could outweigh the disadvantages..---(2) The extension of PixelCNN to conditioning on additional data is fairly straightforward..---(2) Qualitative comparison in Figure 9 suggests that PixelCNN and GAN-based methods may make different kinds of mistakes, with PixelCNN being more robust against introducing artifacts.

This paper proposes an extension of PixelCNN method that can be conditioned on text and spatially-structured constraints (segmentation / keypoints)..It is similar to Reed 2016a except the extension is built on top of PixelCNN instead of GAN..I lean toward accepting because it is very relevant to ICLR community and it provides a good opportunity for future investigation and comparison between different deep image synthesis methods.","[""This paper proposes an extension of PixelCNN method that can be conditioned on text and spatially-structured constraints (segmentation / keypoints). It is similar to Reed 2016a except the extension is built on top of PixelCNN instead of GAN. After reading the author's comment, I realized the argument is not that conditional PixelCNN is much better than conditional GAN. I think the authors can add more discussions about pros and cons of each model in the paper. I agree with the other reviewer that some analysis of training and generation time would be helpful. I understand it takes O(N) instead of O(1) for PixelCNN method to do sampling, but is that the main reason that the experiments can only be conducted in low resolution (32 x 32)? I also think since there are not quantitative comparisons, it makes more sense to show more visualizations than 3 examples. Overall, the generated results look reasonably good and have enough diversity. The color mistake is an issue where the author has provided some explanations in the comment. I would say the technical novelty is incremental since the extension is straightforward and similar to previous work. I lean toward accepting because it is very relevant to ICLR community and it provides a good opportunity for future investigation and comparison between different deep image synthesis methods. "", 'This work focuses on conditional image synthesis in the autoregressive framework.  Based on PixelCNN, it trains models that condition on text as well as segmentation masks or keypoints.  Experiments show results for keypoint conditional synthesis on the CUB (birds) and MHP (human pose) dataset, and segmentation conditional synthesis on MS-COCO (objects).  This extension to keypoint/segment conditioning is the primary contribution over existing PixelCNN work.  Qualitative comparison is made to GAN approaches for synthesis.----------------Pros:--------(1) The paper demonstrates additional capabilities for image generation in the autoregressive framework, suggesting that it can keep pace with the latest capabilities of GANs.--------(2) Qualitative comparison in Figure 9 suggests that PixelCNN and GAN-based methods may make different kinds of mistakes, with PixelCNN being more robust against introducing artifacts.--------(3) Some effort is put forth to establish quantitative evaluation in terms of log-likelihoods (Table 1).----------------Cons:--------(1) Comparison to other work is difficult and limited to qualitative results.  The qualitative results can still be somewhat difficult to interpret.  I believe supplementary material or an appendix with many additional examples could partially alleviate this problem.--------(2) The extension of PixelCNN to conditioning on additional data is fairly straightforward.  This is a solid engineering contribution, but not a surprising new concept.', '""First, it allows us to assess whether auto-regressive models are able to match the GAN results of Reed et al. (2016a)."" Does it, though? Because the resolution is so bad. And resolution limitations aren\'t addressed until the second-to-last paragraph of the paper. And Figure 9 only shows 3 results (picked randomly? Picked to be favorable to PixelCNN?). That hardly seems conclusive.----------------The segmentation masks and keypoints are pretty strong input constraints. It\'s hard to tell how much coherent object and scene detail is emerging because the resolution is so low. For example, the cows in figure 5 look like color blobs, basically. Any color blob that follows an exact cow segmentation mask will look cow-like.----------------The amount of variation is impressive, though.----------------How can we assess how much the model is ""replaying"" training data? Figure 8 tries to get at this, but I wonder how much each of the ""red birds"", for instance, is mostly copied from a particular training example.----------------I\'m unsatisfied with the answers to the pre-review questions. You didn\'t answer my questions. The paper would benefit from concrete numbers on training time / epochs and testing time. You didn\'t say why you can\'t make high resolution comparisons. Yes, it\'s slower at test time. Is it prohibitively slow? Or is it slightly inconvenient? There really aren\'t that many comparisons in the paper, anyway, so it if takes an hour to generate a result that doesn\'t seem prohibitive. ----------------To be clear about my biases: I don\'t think PixelCNN is ""the right way"" to do deep image generation. Texture synthesis methods used these causal neighborhoods with some success, but only because there wasn\'t a clear way to do the optimization more globally (Kwatra et al, Texture Optimization for Example-based Synthesis being one of the first alternatives). It seems simply incorrect to make hard decisions about what pixel values should be in one part of the image before synthesizing another part of the image (Another texture synthesis strategy to help fight back against this strict causality was coarse-to-fine synthesis. And I do see some deep image synthesis methods exploring that). It seems much more correct to have a deeper network and let all output pixels be conditioned on all other pixels (this conditioning will implicitly emerge at intermediate parts of the network). That said, I could be totally wrong, and the advantages stated in the paper could outweigh the disadvantages. But this paper doesn\'t feel very honest about the disadvantages.----------------Overall, I think the results are somewhat tantalizing, especially the ability to generate diverse outputs. But the resolution is extremely low, especially compared to the richness of the inputs. The network gets a lot of hand holding from rich inputs (it does at least learn to obey them). ----------------The deep image synthesis literature is moving very quickly. The field needs to move on from ""proof of concept"" papers (the first to show a particular result is possible) to more thorough comparisons. This paper has an opportunity to be a more in depth comparison, but it\'s not very deep in that regard. There isn\'t really an apples to apples comparison between PixelCNN and GAN nor is there a conclusion statement about why that is impossible. There isn\'t any large scale comparison, either qualitative or quantified by user studies, about the quality of the results.']",0.92138671875,0.07843017578125,1,0.89501953125,0.104736328125,1,0.334716796875,0.66552734375,0,0.916015625,0.08416748046875,1,0.1767578125,0.8232421875,0,0.375,0.625,0
42,42,42,42,42,42,https://openreview.net/forum?id=S1LVSrcge,"-The abstract claims that the model is computationally more efficient than regular RNNs..This makes the paper interesting enough by itself but the claims of computational gains are misleading without actual results to back them up..Originality is one of this paper’s strongest points.

Even if the superior performance is due to this extra regularization controlling parameter it can actually be seen as a useful part of the architecture but it would be nice to know how sensitive the model is to its precise value..The new architecture seems to outperform vanilla RNNs on various sequence modelling tasks..I also like the investigation of the effect of imposing a pattern of computational budget assignment which uses prior knowledge about the task.","['TLDR: The authors present Variable Computation in Recurrent Neural Networks (VCRNN). VCRNN is similar in nature to Adaptive Computation Time (Graves et al., 2016). Imagine a vanilla RNN, at each timestep only a subset (i.e., ""variable computation"") of the state is updated. Experimental results are not convincing, there is limited comparison to other cited work and basic LSTM baseline.----------------=== Gating Mechanism ===--------At each timestep, VCRNN generates a m_t vector which can be seen as a gating mechanism.  Based off this m_t vector, a D-first (D-first as in literally the first D RNN states) subset of the vanilla RNN state is gated to be updated or not. Extra hyperparams epsilon and \\bar{m} are needed -- authors did not give us a value or explain how this was selected or how sensitive and critical these hyperparms are.----------------This mechanism while novel, feels a bit clunky and awkward. It does not feel well principled that only the D-first states get updated, rather than a generalized solution where any subset of the state can be updated.----------------A short section in the text comparing to the soft-gating mechanisms of GRUs/LSTMs/Multiplicative RNNs (Wu et al., 2016) would be nice as well.----------------=== Variable Computation ===--------One of the arguments made is that their VCRNN model can save computation versus vanilla RNNs. While this may be technically true, in practice this is probably not the case. The size of the RNNs they compare to do not saturate any modern GPU cores. In theory computation might be saved, but in practice there will probably be no difference in wallclock time. The authors also did not report any wallclock numbers, which makes this argument hard to sell.----------------=== Evaluation ===--------This reviewer wished there was more citations to other work for comparison and a stronger baseline (than just a vanilla RNN). First, LSTMs are very simple and quite standard nowadays -- there is a lack of comparison to any basic stacked LSTM architecture in all the experiments.----------------The PTB BPC numbers are quite discouraging as well (compared to state-of-the-art). The VCRNN does not beat the basic vanilla RNN baseline. The authors also only cite/compare to a basic RNN architecture, however there has been many contributions since a basic RNN architecture that performs vastly better. Please see Chung et al., 2016 Table 1. Chung et al., 2016 also experimented w/ PTB BPC and they cite and compare to a large number of other (important) contributions.----------------One cool experiment the authors did is graph the per-character computation of VCRNN (i.e., see Figure 2). It shows after a space/word boundary, we use more computation! Cool! However, this makes me wonder what a GRU/LSTM does as well? What is the magnitude of the of the change in the state vector after a space in GRU/LSTM -- I suspect them to do something similar.----------------=== Minor ===--------* Please add Equations numbers to the paper, hard to refer to in a review and discussion!----------------References--------Chung et al., ""Hierarchical Multiscale Recurrent Neural Networks,"" in 2016.--------Graves et al., ""Adaptive Computation Time for Recurrent Neural Networks,"" in 2016.--------Wu et al., ""On Multiplicative Integration with Recurrent Neural Networks,"" in 2016.', ""This is high novelty work, and an enjoyable read.----------------My concerns about the paper more or less mirror my pre-review questions. I certainly agree that the learned variable computation mechanism is obviously doing something interesting. The empirical results really need to be grounded with respect to the state of the art, and LSTMs are still an elephant in the room. (Note that I do not consider beating LSTMs, GRUs, or any method in particular as a prerequisite for acceptance, but the comparison nevertheless should be made.)----------------In pre-review responses the authors brought up that LSTMs perform more computation per timestep than Elman networks, and while that is true, this is an axis along which they can be compared, this factor controlled for (at least in expectation, by varying the number of LSTM cells), etc. A brief discussion of the proposed gating mechanism in light of the currently popular ones would strengthen the presentation.---------------------------2017/1/20: In light of my concerns being addressed I'm modifying my review to a 7, with the understanding that the manuscript will be amended to include the new comparisons posted as a comment."", ""This paper describes a simple but clever method for allowing variable amounts of computation at each time step in RNNs. The new architecture seems to outperform vanilla RNNs on various sequence modelling tasks. Visualizations of the assignment of computational resources over time support the hypothesis that the model is able to learn to assign more computations whenever longer longer term dependencies need to be taken into account.----------------The proposed model is evaluated on a multitude of tasks and its ability to outperform similar architectures seems consistent. Some of the tasks allow for an interesting analysis of the amount of computation the model requests at each time step. It’s very interesting to see how the model seems to use more resources at the start of each word or ASCII character. I also like the investigation of the effect of imposing a pattern of computational budget assignment which uses prior knowledge about the task. The superior performance of the architecture is impressive but I’m not yet convinced that the baseline models had an equal number of hyperparameters to tune. I’ll come back to this point in the next paragraph because it’s mainly a clarity issue.----------------The abstract claims that the model is computationally more efficient than regular RNNs. There are no wall time measurements supporting this claim. While the model is theoretically able to save computations, the points made by the paper are clearly more conceptual and about the ability of the model to choose how to allocate its resources. This makes the paper interesting enough by itself but the claims of computational gains are misleading without actual results to back them up. I also find it unfortunate that it’s not clear from the text how the hyperparameter \\bar{m} was chosen. Whether it was chosen randomly or set using a hyperparameter search on held-out data influences the fairness of a comparison with RNNs which did not have a similar type of hyperparameter for controlling regularization like for example dropout or weight noise (even if regularization of RNNs is a bit tricky). I don’t consider this a very serious flaw because I’m impressed enough by the fact that the new architecture achieves roughly similar performance while learning to allocate resources but I do think that details of this type are too important to be absent from the text. Even if the superior performance is due to this extra regularization controlling parameter it can actually be seen as a useful part of the architecture but it would be nice to know how sensitive the model is to its precise value.----------------To my knowledge, the proposed architecture is novel. The way the amount of computation is determined is unlike other methods for variable computation I have seen and quite inventive. Originality is one of this paper’s strongest points. ----------------It’s currently hard to predict whether this method for variable computation will be used a lot in practice given that this also depends on how feasible it is to obtain actual computational gains at the hardware level. That said, the architecture may turn out to be useful for learning long-term dependencies. I also think that the interpretability of the value m_t is a nice property of the method and that it’s visualizations are very interesting. It might shed some more light into what makes certain tasks difficult for RNNs. ----------------Pros:--------Original clever idea.--------Nice interesting visualizations.--------Interesting experiments.----------------Cons:--------Some experimental details are not clear.--------I’m not convinced of the strength of the baseline.--------The paper shouldn’t claim actual computational savings without reporting wall-clock times.----------------Edit:--------I'm very positively impressed by the way the authors ended up addressing the biggest concerns I had about the paper and raised my score. Adding an LSTM baseline and results with a GRU version of the model significantly improves the empirical quality of the paper. On top of that, the authors addressed my question about some experimental detail I found important and promised to change the wording of the paper to remove confusion about whether the computational savings are conceptual or in actual wall time. I think it's fine that they are conceptual only as long as this is clear from the paper and abstract. I want to make clear to the AC that since the changes to the paper are currently still promises, my new score should be assumed to apply to an updated version of the paper in which the aforementioned concerns have indeed been addressed. ----------------Edit: --------Since I didn't know that the difference with the SOTA for some of these tasks was so large, I had to lower my score again after learning about this. I still think it's a good paper but with these results I cannot say that it stands out.""]",0.9296875,0.07025146484375,1,0.962890625,0.037078857421875,1,0.3642578125,0.6357421875,0,0.8876953125,0.1121826171875,1,0.23681640625,0.76318359375,0,0.53515625,0.464599609375,1
43,43,43,43,43,43,https://openreview.net/forum?id=S1TER2oll,"5 with L1 regularisation on the filter weights does not seem entirely fair, since the resulting shape would have to be encompassed in a 5x5 window whereas Fig..4, especially about the fact that some of the filter shapes end up having many fewer nonzeros than the algorithm enforces (e.g..My intuition is that creating efficient implementations of various non-square convolutions for each new problem might end up not being worth the effort, but I could be wrong here.

The main idea being that choosing convolution filters that sum over correlated features results in lower Gaussian complexity and thus the learner has higher ability to generalize..While I am no expert in theoretical analysis of learning algorithms – there are parts of proof that look sound, but there are parts that are rather hand wavy (for eg, extension to networks using max-pooling from average pooling)..I am willing to overlook the paucity in rigor in some parts of the theoretical arguments because the empirical evidence looks convincing.","['Authors propose a mechanism for selecting the design of filters in convolutional layers. The basic idea is that convolution should be applied to input feature dimensions that are highly correlated in order to detect rare events. For example, adjacent pixels in images are correlated and edges are rare events of interest to be detected. Authors argue that square filters are therefore appropriate in images. However, in data such as bird songs high correlations might exist between non-adjacent harmonics and a convolution filter should take a weighted summation over these input feature dimensions. Such an operation can thus be thought of computing data-dependent dilated convolutions.----------------Paper theoretically motivates this choice using the idea of Gaussian complexity of the learner (i.e. a CNN in this case). The main idea being that choosing convolution filters that sum over correlated features results in lower Gaussian complexity and thus the learner has higher ability to generalize. While I am no expert in theoretical analysis of learning algorithms – there are parts of proof that look sound, but there are parts that are rather hand wavy (for eg, extension to networks using max-pooling from average pooling). Also, the theory is not directly applicable to choosing filters when number of layers are more than 1. I am willing to overlook the paucity in rigor in some parts of the theoretical arguments because the empirical evidence looks convincing. ----------------The method of choosing the filter shape can be briefly summarized as:--------(a) The covariance matrix of the input features is computed. --------(b) Using the covariance matrix, feature dimensions with highest correlations are determined by solving equation (7). A hard limit on maximum number of filter dimensions is imposed (typically ~ 10-15).  This leads to choice of a single design for all filters in the layer. --------(c) Authors extend the framework to work with multiple layers in the following way: A subset of feature dimensions cannot account for all variance in the inputs and there is some residual variance. The filter design of the next layer attempts to minimize this residual variance. This process is repeated iteratively by solving eq (8) to obtain filter designs for all the layers. ----------------Ideally for determining filter designs of different layers – one should have computed the covariance statistics of outputs of the previous layer. However this assumes that filters of the previous layer are already known and this is not computationally feasible to implement. Authors instead use the method described in (c).--------A question which comes to my mind is – a single feature design is chosen for each layer. Have the authors considered using the process in (c) to chose different filter designs for different filters in the same layer as opposed to using the same filter design for all the filters? ----------------Regarding baselines:--------B1. It would be great to see a comparison with randomly chosen filter designs. Two comparisons could be made – (1a) A single random design is chosen for each layer. (1b) The design of each filter is chosen randomly (i.e. allowing for different designs of filter within each layer). ----------------B2. Since the theory is not really applicable to CNNs with more that one layer – I wonder how much of the benefit is obtained by choosing the filter design just in a single layer v/s all the layers. A good comparison would be when filter design of the first layer are chosen using the described method and filters in higher layers are chosen to be square. ----------------B3. Authors mention the use L1 regularization in the baselines. Was the L1 penalty cross-validated? If so, then upto what range? ----------------Somethings which are unclear:--------- “exclude data that represent obvious noise”--------- DFMax mentioned in the supplementary materials----------------Overall I think this is very interesting idea for filter design. The authors have done a fair set of experiments but I would really like to see results of B1, B2 and the answer to question in B3.  I have currently set my rating to a weak reject, but I am happy to raise my ratings to – “Good paper, accept” if the authors provide results of experiments and answers to questions in my comments above. ', 'This work proposes a way how to learn filter shapes for CNNs in an unsupervised manner for multiple tasks by solving a lasso problem. Even though this method does not seem to be applicable for image classification CNNs (as image data generally do not have bias towards anisotropic structures), it gives an empirical methodology to design filter shapes for tasks with different input data structure. Authors show that this method is applicable for spectrogram classification and gene sequence classification. ----------------The paper is well written and and is of interest to the community as it presents a unsupervised method applicable to problems with less training data. Authors compare the performance of the proposed method against reasonable baselines (i.e. handcrafted filter sizes) and based on the evaluation it seems to improve the results and help to avoid over-fitting (probably due to reduced filter size and thus number of parameters). In this way it is an interesting combination of unsupervised methods for a supervised training.----------------Unfortunately, I am not able to validate correctness of the theoretical justification.----------------As a side note:--------* It would be useful to give some reference showing that using spectrogram for sound classification is a reasonable choice', 'The paper proposes a method for optimising the shape of filters in convolutional neural network layers, i.e. the structure of their receptive fields. CNNs for images almost invariably feature small square filters (e.g. 3x3, 5x5, ...) and this paper provides an algorithm to optimise this aspect of the model architecture (which is often treated as fixed) based on data. It is argued that this is especially useful for data modalities where the assumption of locality breaks down, as in e.g. spectrograms, where correlations between harmonics are often relevant to the task at hand, but they are not local in frequency space.----------------Improved performance is demonstrated on two tasks that are fairly non-standard, but I think that is fine given that the proposed approach probably isn\'t useful for the vast majority of popular benchmark datasets (e.g. MNIST, CIFAR-10), where the locality assumption holds and a square filter shape is probably close to optimal anyway. Fig. 1 is a nice demonstration of this.----------------The paper spends quite a bit of space on a theoretical argument for the proposed method based on Gaussian complexity, which is interesting but maybe doesn\'t warrant quite so much detail. In contrast, section 3.3 (about how to deal with pooling) is quite handwavy in comparison. This is probably fine but the level of detail in the preceding sections makes it a bit suspicious.----------------I\'m also not 100% convinced that the theoretical argument is particularly relevant, because it seems to rely on some assumptions that are clearly untrue for practical CNNs, such as 1-norm weight constraints and the fact that it is probably okay to swap out the L1 norm for the L2 norm.----------------I would also like to see a bit more discussion about Fig. 4, especially about the fact that some of the filter shapes end up having many fewer nonzeros than the algorithm enforces (e.g. 3 nonzeros for layers 6 and 7, whereas the maximum is 13). Of course this is a perfectly valid outcome as the algorithm doesn\'t force the solution to have an exact number of nonzeros, but surely the authors will agree that it is a bit surprising/unintuitive? The same figure also features an interesting phase transition between layers 1-4 and 5-8, with the former 4 layers having very similar, almost circular/square filter shapes, and the later having very different, spread out shapes. Some comments about why this happens would be welcome.----------------Regarding my question about computational performance, I still think that this warrants some discussion in the paper as well. For many new techniques, whether they end up being adopted mainly depends on the ratio between the amount of work that goes into implementing them, and the benefit they provide. I\'m not convinced that the proposed approach is very practical. My intuition is that creating efficient implementations of various non-square convolutions for each new problem might end up not being worth the effort, but I could be wrong here.------------------------Minor comments:----------------- please have the manuscript proofread for spelling and grammar.----------------- there is a bit of repetition in sections 2 and 3, e.g. the last paragraphs of sections 2.1 and 2.2 basically say the same thing, it would be good to consolidate this. ----------------- a few things mentioned in the paper that were unclear to me (""syllables"", ""exclude data that represent obvious noise"", choice of ""max nonzero elements"" parameter) have already been adequately addressed by the authors in their response to my questions, but it would be good to include these answers in the manuscript as well.----------------- the comparison in Fig. 5 with L1 regularisation on the filter weights does not seem entirely fair, since the resulting shape would have to be encompassed in a 5x5 window whereas Fig. 4 shows that the filter shapes found by the algorithm often extend beyond that. I appreciate that training nets with very large square filters is problematic in many ways, but the claim ""L1 regularization cannot achieve the same effect as filter shaping"" is not really convincingly backed up by this experiment.']",0.69677734375,0.30322265625,1,0.95751953125,0.042236328125,1,0.143310546875,0.85693359375,0,0.84423828125,0.1558837890625,1,0.10308837890625,0.89697265625,0,0.271240234375,0.728515625,0
44,44,44,44,44,44,https://openreview.net/forum?id=S1di0sfgl,"Empirical results on computational savings are not given, and hierarchy beyond a single level (where the data contains separators such as spaces and pen up/down) does not seem to be demonstrated..--On a minor note, I have few remarks/complaints about the writing and the related work:


-- In the introduction:
---“One of the key principles of learning in deep neural networks as well as in the human brain” : please provide evidence for the “human brain” part of this claim..-Pros:
---- Paper is well-motivated, exceptionally well-composed
---- Provides promising initial results on learning hierarchical representations through visualizations and thorough experiments on language modeling and handwriting generation
---- The annealing trick with the straight-through estimator also seems potentially useful for other tasks containing discrete variables, and the trade-off in the flush operation is innovative.

This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network..The paper is well written..In order to select one of three operations at each time step, the authors propose using the straight-through estimator with a slope-annealing trick during training.","['This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated within a recurrent neural network framework, and shows the state-of-the-art performance on several benchmarks. The paper is well written.----------------Question) Can you extend it to bidirectional RNN? ', ""The paper proposes a modified RNN architecture with multiple layers, where higher layers are only passed lower layer states if a FLUSH operation is predicted, consisting of passing up the state and reseting the lower layer's state. In order to select one of three operations at each time step, the authors propose using the straight-through estimator with a slope-annealing trick during training. Empirical results and visualizations illustrate that the modified architecture performs well at boundary detection.----------------Pros:--------- Paper is well-motivated, exceptionally well-composed--------- Provides promising initial results on learning hierarchical representations through visualizations and thorough experiments on language modeling and handwriting generation--------- The annealing trick with the straight-through estimator also seems potentially useful for other tasks containing discrete variables, and the trade-off in the flush operation is innovative.--------Cons:--------- In a couple cases the paper does not fully deliver. Empirical results on computational savings are not given, and hierarchy beyond a single level (where the data contains separators such as spaces and pen up/down) does not seem to be demonstrated.--------- It's unclear whether better downstream performance is due to use of hierarchical information or due to the architecture changes acting as regularization, something which could hopefully be addressed."", 'This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).----------------Their model is organized as a set of layers that aim at capturing the information form different “level of abstraction”. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. ----------------The experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.----------------Overall this paper presents a strong and novel model with promising experimental results.--------------------------------On a minor note, I have few remarks/complaints about the writing and the related work:----------------- In the introduction:--------“One of the key principles of learning in deep neural networks as well as in the human brain” : please provide evidence for the “human brain” part of this claim.--------“For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances” I believe you re missing Mikolov et al. 2010 in the references.--------“in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data”: missing reference to Lin et al., 1996----------------- in the related work:--------“A more recent model, the clockwork RNN (CW-RNN) (Koutník et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)” : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.--------While the above models focus on online prediction problems, where a prediction needs to be made…”: I believe there is a lot of missing references, in particular to Socher’s work or older recursive networks.--------“The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)”: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.----------------Missing references:--------“Recurrent neural network based language model.”, Mikolov et al. 2010--------“Learning long-term dependencies in NARX recurrent neural networks”, Lin et al. 1996--------“Sequence labelling in structured domains with hierarchical recurrent neural networks“, Fernandez et al. 2007--------“Learning sequential tasks by incrementally adding  higher  orders”, Ring, 1993']",0.72265625,0.277587890625,1,0.9052734375,0.09466552734375,1,0.160400390625,0.83984375,0,0.91259765625,0.08746337890625,1,0.134033203125,0.86572265625,0,0.2958984375,0.7041015625,0
45,45,45,45,45,45,https://openreview.net/forum?id=SJ6yPD5xg,"2
---- Please explain what ""Clip"" means for dueling networks in the legend of Figure 3
---- I would have liked to see more ablated versions on Atari, to see in particular if the same patterns of individual contribution as on Labyrinth were observed
---- In the legend of Figure 3 the % mentioned are for Labyrinth, which is not clear from the text..2 refers to the loss L_PC but that term is not defined and is not (explicitly) in eq..For example L_PC should be explicitly mentioned, before reaching the appendix.

This paper proposes a way of adding unsupervised auxiliary tasks to a deep RL agent like A3C..Authors propose a bunch of auxiliary control tasks and auxiliary reward tasks and evaluate the agent in Labyrinth and Atari..Proposed UNREAL agent performs significantly better than A3C and also learns faster.","['This paper proposes a way of adding unsupervised auxiliary tasks to a deep RL agent like A3C. Authors propose a bunch of auxiliary control tasks and auxiliary reward tasks and evaluate the agent in Labyrinth and Atari. Proposed UNREAL agent performs significantly better than A3C and also learns faster. This is definitely a good contribution to the conference. However, this is not a surprising result since adding additional auxiliary tasks that are relevant to the goal should always help in better and faster feature shaping. This paper is a proof of concept for this idea.----------------The paper is well written and easy to follow by any reader with deep RL expertise.----------------Can authors comment about the computational resources needed to train the UNREAL agent?----------------The overall architecture is quite complicated. Are the authors willing to release the source code for their model?--------------------------------------------------------------------------------After rebuttal:--------No change in the review.', 'This paper is about improving feature learning in deep reinforcement learning, by augmenting the main policy\'s optimization problem with terms corresponding to (domain-independent) auxiliary tasks. These tasks are about control (learning other policies that attempt to maximally modify the state space, i.e. here the pixels), immediate reward prediction, and value function replay. Except for the latter, these auxiliary tasks are only used to help shape the features (by sharing the CNN+LSTM feature extraction network). Experiments show the benefits of this approach on Atari and Labyrinth problems, with in particular much better data efficiency than A3C.----------------The paper is well written, ideas are sound, and results pretty convincing, so to me this is a clear acceptance. At high level I only have few things to say, none being of major concern:--------- I believe you should say something about the extra computational cost of optimizing these auxiliary tasks. How much do you lose in terms of training speed? Which are the most costly components?--------- If possible, please try to make it clearer in the abstract / intro that the agent is learning different policies for each task. When I read in the abstract that the agent ""also maximises many other pseudo-reward functions simultaneously by reinforcement learning"", my first understanding was that it learned a single policy to optimize all rewards together, and I realized my mistake only when reaching eq. 1.--------- The ""feature control"" idea is not validated empirically (the preliminary experiment in Fig. 5 is far from convincing as it only seems to help slightly initially). I like that idea but I am worried by the fact the task is changing during learning, since the extracted features are being modified. There might be stability / convergence issues at play here.--------- Since as you mentioned, ""the performance of our agents is still steadily improving"", why not keep them going to see how far they go? (at least the best ones)--------- Why aren\'t the auxiliary tasks weight parameters (the lambda_*) hyperparameters to optimize? Were there any experiments to validate that using 1 was a good choice?--------- Please mention the fact that auxiliary tasks are not trained with ""true"" Q-Learning since they are trained off-policy with more than one step of empirical rewards (as discussed in the OpenReview comments)----------------Minor stuff:--------- ""Policy gradient algorithms adjust the policy to maximise the expected reward, L_pi = -..."" => that\'s actually a loss to be minimized--------- In eq. 1 lambda_c should be within the sum--------- Just below eq. 1 r_t^(c) should be r_t+k^(c)--------- Figure 2 does not seem to be referenced in the text, also Figure 1(d) should be referenced in 3.3--------- ""the features discovered in this manner is shared"" => are shared--------- The text around eq. 2 refers to the loss L_PC but that term is not defined and is not (explicitly) in eq. 2--------- Please explain what ""Clip"" means for dueling networks in the legend of Figure 3--------- I would have liked to see more ablated versions on Atari, to see in particular if the same patterns of individual contribution as on Labyrinth were observed--------- In the legend of Figure 3 the % mentioned are for Labyrinth, which is not clear from the text.--------- In 4.1.2: ""Figure 3 (right) shows..."" => it is actually the top left plot of the figure. Also later ""This is shown in Figure 3 Top"" should be Figure 3 Top Right.--------- ""Figure 5 shows the learning curves for the top 5 hyperparameter settings on three Labyrinth navigation levels"" => I think it is referring to the left and middle plots of the figure, so only on two levels (the text above might also need fixing)--------- In 4.2: ""The left side shows the average performance curves of the top 5 agents for all three methods the right half shows..."" => missing a comma or something after ""methods""--------- Appendix: ""Further details are included in the supplementary materials."" => where are they?--------- What is the value of lambda_PC? (=1 I guess?)----------------[Edit] I know some of my questions were already answered in Comments, no need to re-answer them', 'This work proposes to train RL agents to also perform auxiliary tasks, positing that doing so will help models learn stronger features.--------They propose two pseudo-control tasks, control the change in pixel intensity, and control the activation of latent features. They also propose a supervised regression task, predict immediate reward following a sequence of events. The latter is learned offline via a skewed sampling of an experience replay buffer in order to balance seeing reward or not to 1/2 chance.--------Such agents perform significantly well on discrete-action-continuous-space RL tasks, and reach baseline performance in 10x less iterations. ----------------This work contrasts with traditional ""passive"" unsupervised or model-based learning. Instead of forcing the model to learn a potentially useless representation of the input, or to learn the possibly impossible (due to partial observability) task-modelling objective, learning to control local and internal features of the environment complements learning the optimal control policy.----------------To me the approach is novel and proposes a very interesting alternative to unsupervised learning that takes advantage of the ""possibility"" of control that an agent has over the environment.--------The proposed tasks are explained at a rather high level, which is convenient to understand intuition, but I think some lower level of detail might be useful. For example L_PC should be explicitly mentioned, before reaching the appendix. Otherwise this work is clear and easily understandable by readers familiar with Deep RL.--------The methodology is sound, on one hand hand the distribution of best hyperparameters might be different for A3C and UNREAL, but also measuring top-3 ensures that, presuming that the both best hyperparameters for A3C and for UNREAL are within the explored intervals, the per-method best hyperparameters are found.--------I think one weakness of the paper (or rather, considering the number of things that can fit in a paper, crucially needed future work) is that there is very little experimental analysis of the effect of the auxiliary tasks appart from their (very strong) effect on performance. In the same vein, pixel/feature control seems to have the most impact, in Labyrinth just A3C+PC beats anything else (except UNREAL), I think it would have been worth looking at this, either in isolation or in more depth, measuring more than just performance on RL tasks.']",0.8935546875,0.10638427734375,1,0.8974609375,0.10272216796875,1,0.2178955078125,0.7822265625,0,0.87890625,0.12109375,1,0.19677734375,0.80322265625,0,0.428955078125,0.5712890625,0
46,46,46,46,46,46,https://openreview.net/forum?id=SJCscQcge,"However the approach and results are not very surprising..----Pros:
---* Interesting topic
---* Black-box setup is most relevant
---* Multiple experiments
---* Shows that with flipping only 1~5% of pixels, adversarial images can be created



----Cons:
---* Too long, yet key details are not well addressed
---* Some of the experiments are of little interest
---* Main experiments lack key measures or additional baselines
---* Limited technical novelty







Quality: the method description and experimental setup leave to be desired..-The idea has some merit, however, as mentioned by one of the reviewers, the field has been studied widely, including black box setups.

The paper is verbosely written and I feel like the findings could be summarized much more succinctly..At the very least, the outcome is not surprising to me at all..----Significance: the work is incremental, the issues in the experiments limit potential impact of this paper.","['Paper summary:--------This work proposes a new algorithm to generate k-adversarial images by modifying a small fraction of the image pixels and without requiring access to the classification network weight.------------------------Review summary:--------The topic of adversarial images generation is of both practical and theoretical interest. This work proposes a new approach to the problem, however the paper suffers from multiple issues. It is too verbose (spending long time on experiments of limited interest); disorganized (detailed description of the main algorithm in sections 4 and 5, yet a key piece is added in the experimental section 6); and more importantly the resulting experiments are of limited interest to the reader, and the main conclusions are left unclear.--------This looks like an interesting line of work that has yet to materialize in a good document, it would need significant re-writing to be in good shape for ICLR.------------------------Pros:--------* Interesting topic--------* Black-box setup is most relevant--------* Multiple experiments--------* Shows that with flipping only 1~5% of pixels, adversarial images can be created------------------------Cons:--------* Too long, yet key details are not well addressed--------* Some of the experiments are of little interest--------* Main experiments lack key measures or additional baselines--------* Limited technical novelty----------------------------------------Quality: the method description and experimental setup leave to be desired. ------------------------Clarity: the text is verbose, somewhat formal, and mostly clear; but could be improved by being more concise.------------------------Originality: I am not aware of another work doing this exact same type of experiments. However the approach and results are not very surprising.------------------------Significance: the work is incremental, the issues in the experiments limit potential impact of this paper.------------------------Specific comments:--------* I would suggest to start by making the paper 30%~40% shorter. Reducing the text length, will force to make the argumentation and descriptions more direct, and select only the important experiments.--------* Section 4 seems flawed. If the modified single pixel can have values far outside of the [LB, UB] range; then this test sample is clearly outside of the training distribution; and thus it is not surprising that the classifier misbehaves (this would be true for most classifiers, e.g. decision forests or non-linear SVMs). These results would be interesting only if the modified pixel is clamped to the range [LB, UB].--------* [LB, UB] is never specified, is it ? How does p = 100, compares to [LB, UB] ? To be of any use, p should be reported in proportion to [LB, UB]--------* The modification is done after normalization, is this realistic ? --------* Alg 2, why not clamping to [LB, UB] ?--------* Section 6, “implementing algorithm LocSearchAdv”, the text is unclear on how p is adjusted; new variables are added. This is confusion.--------* Section 6, what happens if p is _not_ adjusted ? What happens if a simple greedy random search is used (e.g. try 100 times a set of 5 random pixels with value 255) ?--------* Section 6, PTB is computed over all pixels ? including the ones not modified ? why is that ? Thus LocSearchAdv PTB value is not directly comparable to FGSM, since it intermingles with #PTBPixels (e.g. “in many cases far less average perturbation” claim).--------* Section 6, there is no discussion on the average number of model evaluations. This would be equivalent to the number of requests made to a system that one would try to fool. This number is important to claim the “effectiveness” of such black box attacks. Right now the text only mentions the upper bound of 750 network evaluations. --------* How does the number of network evaluations changes when adjusting or not adjusting p during the optimization ?--------* Top-k is claimed as a main point of the paper, yet only one experiment is provided. Please develop more, or tune-down the claims.--------* Why is FGSM not effective for batch normalized networks ? Has this been reported before ? Are there other already published techniques that are effective for this scenario ? Comparing to more methods would be interesting.--------* If there is little to note from section 4 results, what should be concluded from section 6 ? That is possible to obtain good results by modifying only few pixels ? What about selecting the “top N” largest modified pixels from FGSM ? Would these be enough ? Please develop more the baselines, and the specific conclusions of interest.------------------------Minor comments:--------* The is an abuse of footnotes, most of them should be inserted in the main text.--------* I would suggest to repeat twice or thrice the meaning of the main variables used (e.g. p, r, LB, UB)--------* Table 1,2,3 should be figures--------* Last line of first paragraph of section 6 is uninformative.--------* Very tiny -> small', 'The paper presents a method for generating adversarial input images for a convolutional neural network given only black box access (ability to obtain outputs for chosen inputs, but no access to the network parameters).  However, the notion of adversarial example is somewhat weakened in this setting: it is k-misclassification (ensuring the true label is not a top-k output), instead of misclassification to any desired target label.----------------A similar black-box setting is examined in Papernot et al. (2016c).  There, black-box access is used to train a substitute for the network, which is then attacked.  Here, black-box access in instead exploited via local search.  The input is perturbed, the resulting change in output scores is examined, and perturbations that push the scores towards k-misclassification are kept.----------------A major concern with regard to novelty is that this greedy local search procedure is analogous to gradient descent; a numeric approximation (observe change in output for corresponding change in input) is used instead of backpropagation, since one does not have access to the network parameters.  As such, the greedy local search algorithm itself, to which the paper devotes a large amount of discussion, is not surprising and the paper is fairly incremental in terms of technical novelty.', 'The authors propose a method to generate adversarial examples w/o relying on knowledge of the network architecture or network gradients.----------------The idea has some merit, however, as mentioned by one of the reviewers, the field has been studied widely, including black box setups.----------------My main concern is that the first set of experiments allows images that are not in image space. The authors acknowledge this fact on page 7 in the first paragraph. In my opinion, this renders these experiments completely meaningless. At the very least, the outcome is not surprising to me at all.----------------The greedy search procedure remedies this issue. The description of the proposed method is somewhat convoluted. AFAICT, first a candidate set of pixels is generated by using PERT. Then the pixels are perturbed using CYCLIC.--------It is not clear why this approach results in good/minimal perturbations as the candidate pixels are found using a large ""p"" that can result in images outside the image space. The choice of this method does not seem to be motivated by the authors.----------------In conclusion, while the authors to an interesting investigation and propose a method to generate adversarial images from a black-box network, the overall approach and conclusions seem relatively straight forward. The paper is verbosely written and I feel like the findings could be summarized much more succinctly.']",0.87548828125,0.12445068359375,1,0.9794921875,0.020416259765625,1,0.490478515625,0.509765625,0,0.8876953125,0.1121826171875,1,0.2239990234375,0.77587890625,0,0.47216796875,0.52783203125,0
47,47,47,47,47,47,https://openreview.net/forum?id=SJGCiw5gl,"The correct comparison in accuracies would be if we the original network was also trained for N + M iterations..This is essentially what eqn 7 conveys and is likely to be reason why just removing weights that result in small activations is not as good of a pruning strategy (as shown by results in the paper)..This may be more complicated as it would need to change the implementation of convolution operator, but can lead to further speedup.

Authors propose a strategy for pruning weights with the eventual goal of reducing GFLOP computations..The pruning strategy is well motivated using the taylor expansion of the neural network function with respect to the feature activations..-(A) Ideally the gradient of the output with respect to the activation functions should be 0 at the optimal, but as a result of stochastic gradient evaluations this would practically never be zero.","[""Authors propose a strategy for pruning weights with the eventual goal of reducing GFLOP computations. The pruning strategy is well motivated using the taylor expansion of the neural network function with respect to the feature activations. The obtained strategy removes feature maps that have both a small activation and a small gradient (eqn 7). ----------------(A) Ideally the gradient of the output with respect to the activation functions should be 0 at the optimal, but as a result of stochastic gradient evaluations this would practically never be zero. Small variance in the gradient across mini-batches indicates that irrespective of input data the specific network parameter is unlikely to change - intuitively these are parameters that are closer to convergence. Parameters/weights that are close to convergence and also result in a small activation are intuitively good candidates for pruning. This is essentially what eqn 7 conveys and is likely to be reason why just removing weights that result in small activations is not as good of a pruning strategy (as shown by results in the paper). There are two kind of differences in weights that are removed by activation v/s taylor expansion:--------1. Weights with high-activations but very low gradients will be removed by taylor expansion, but not by activation alone. --------2. Weights with low-activation but high gradients will be removed by activation criterion, but not by taylor expansion. --------It will be interesting to analyze which of (1) or (2) contribute more to the differences in weights that are removed by the taylor expansion v/s activation criterion. Intuitively it seems that weight that satisfy (1) are important because they are converged and contribute significantly to network's activation. It is possible that a modified criterion - eqn (7) + \\lambda feature activation, (where \\lambda needs to be found by cross-validation) may lead to even better results at the cost of more parameter tuning. --------  --------(B) Another interesting comparison is with the with the optimal damage framework - where the first order gradients are assumed to be zero and pruning is performed using the second-order information (also discussed by authors in the appendix). Critically, only the diagonal of the Hessian is computed. There is no comparison with optimal damage as authors claim it is memory and computation inefficient. Back of envelope calculations suggest that this would result only in 50% increase in memory and computation during pruning, but no loss in efficiency during testing. Therefore from a standpoint of deployment, I don't think this missing comparison is justified. ----------------(C) The eventual goal of the authors is to reduce GFLOPs. Some recent papers have proposed using lower precision computation for this. A comparison in GFLOPs with lower precision v/s pruning would be a great. While both these approaches are complementary and it is expected that combining both of them can lead to superior performance than either of the two - it is unclear when we are operating in the low-precision regime how much pruning can be performed. Any analysis on this tradeoff would be great (but not necessary).----------------(D) On finetuning, authors report results of AlexNet and VGG on two different datasets - Flowers and Birds respectively. Why is this the case? It would be great to see the results of both the networks on both the datasets. ----------------(E) Authors report there is only a small drop in performance after pruning. Suppose the network was originally trained with N iterations, and then M finetuning iterations were performed during pruning. This means that pruned networks were trained for N + M iterations. The correct comparison in accuracies would be if we the original network was also trained for N + M iterations. In figure 4, does the performance at 100% parameters reports accuracy after N+M iterations or after N iterations? ----------------Overall I think the paper is technically and empirically sound, it proposes a new strategy for pruning:--------(1) Based on taylor expansion--------(2) Feature normalization to reduce parameter tuning efforts. --------(3) Iterative finetuning. --------However, I would like to see some comparisons mentioned in my comments above. If those comparisons are made I would change my ratings to an accept. "", 'This paper presents a novel way of pruning filters from convolutional neural networks with a strong theoretical justification. The proposed methods is derived from the first order Taylor expansion of the loss change while pruning a particular unit. This leads to simple weighting of the unit activation with its gradient w.r.t. loss function and performs better than simply using the activation magnitude as the heuristic for pruning. This intuitively makes sense, as we would like to remove not only the filters with low activation, but also filters where the incorrect activation value would not have small influence on the target loss.----------------Authors thoroughly investigate multiple baselines, including an oracle which sets an upper bound on the target performance even though it is computationally expensive. The devised method seems to be quite elegant and authors show that it generalizes well on multiple tasks and is computationally more than feasible as it is easy to combine with traditional fine tuning procedure. Also, the work clearly shows the trade-offs of increased speed and decreased performance, which is useful for practical applications.----------------It would be also useful to compare against different baselines, e.g. [1]. However this method seems to be more useful as it does not involve training of a new network (and thus is probably much faster).----------------Suggestion - maybe it can be extended in the future towards also removing only parts of the filters(e.g. for the 3D convolution)? This may be more complicated as it would need to change the implementation of convolution operator, but can lead to further speedup.----------------[1] https://arxiv.org/abs/1503.02531', 'Authors propose a neural pruning technique starting from trained models using an approximation of change in the cost function and outperform other criteria. Authors obtain solid speedups while maintaining reasonable accuracy thanks to finetuning after pruning. Comparisons to existing methods is weak as GFLOPS graphs only show a couple simple baselines and no prior work baselines. I would be more convinced of the superiority of the approach with such comparison.']",0.91650390625,0.08349609375,1,0.93896484375,0.06103515625,1,0.226806640625,0.7734375,0,0.90380859375,0.09637451171875,1,0.06842041015625,0.931640625,0,0.320556640625,0.67919921875,0
48,48,48,48,48,48,https://openreview.net/forum?id=SJIMPr9eg,"Therefore the baselines should include the state of art Res Nets and Dense Convolutional networks hence current results are preliminary..-It'd be easier to sell this method either as an option for scarce labeled datasets where data augmentation is non-trivial (but then for most image-related applications, random crops and reflections are easy and valid), but that would necessitate different benchmarks, and comparison against simpler methods like said data augmentation, dropout (especially, due to the ensemble interpretation), and so on..-The method is not sufficiently novel since the steps of Deep Incremental Boosting are slightly adopted.

This paper proposes a boosting based ensemble procedure for residual networks by adopting the Deep Incremental Boosting method that was used for CNN's(Mosca & Magoulas, 2016a)..Instead of adding a layer to the end of the network, this version adds a block of layers to a position p_t (starts at a selected position p_0) and merges layer accordingly hence slightly adopts DIB..It is not clear whether the improvements (if there is) of the ensemble disappear after data-augmentation.","[""This paper proposes a boosting based ensemble procedure for residual networks by adopting the Deep Incremental Boosting method that was used for CNN's(Mosca & Magoulas, 2016a). At each step t, a new block of layers are added to the network at a position p_t and the weights of all layers are copied to the current network to speed up training.----------------The method is not sufficiently novel since the steps of Deep Incremental Boosting are slightly adopted. Instead of adding a layer to the end of the network, this version adds a block of layers to a position p_t (starts at a selected position p_0) and merges layer accordingly hence slightly adopts DIB. ----------------The empirical analysis does not use any data-augmentation. It is not clear whether the improvements (if there is) of the ensemble disappear after data-augmentation.  Also, one of the main baselines, DIB has no-skip connections therefore this can negatively affect the fair comparison. The authors argue that they did not involve state of art Res Nets since their analysis focuses on the ensemble approach, however any potential improvement of the ensemble can be compensated with an inherent feature of Res Net variant. The boosting procedure can be computationally restrictive in case of ImageNet training and Res Net variants may perform much better in that case too. Therefore the baselines should include the state of art Res Nets and Dense Convolutional networks hence current results are preliminary.----------------In addition, it is not clear how sensitive the boosting to the selection of injection point.----------------This paper adopts DIB to Res Nets and provides some empirical analysis however the contribution is not sufficiently novel and the empirical results are not satisfactory for demonstrating that the method is significant.----------------Pros---------provides some preliminary results for boosting of Res Nets--------Cons---------not sufficiently novel: an incremental approach ---------empirical analysis is not satisfactory"", 'The authors mention that they are not aiming to have SOTA results.--------However, that an ensemble of resnets has lower performance than some of single network results, indicates that further experimentation preferably on larger datasets is necessary.--------The literature review could at least mention some existing works such as wide resnets https://arxiv.org/abs/1605.07146 or the ones that use knowledge distillation for ensemble of networks for comparison on cifar.--------While the manuscript is well-written and the idea is novel, it needs to be extended with experiments.', ""The paper under consideration proposes a set of procedures for incrementally expanding a residual network by adding layers via a boosting criterion.----------------The main barrier to publication is the weak empirical validation. The tasks considered are quite small scale in 2016 (and MNIST with a convolutional net is basically an uninteresting test by this point). The paper doesn't compare to the literature, and CIFAR-10 results fail to improve upon rather simple, single-network published baselines (Springenberg et al, 2015 for example, obtains 92% without data augmentation) and I'm pretty sure there's a simple ResNet result somewhere that outshines these too. The CIFAR100 results are a little bit interesting as they are better than I'm used to seeing (I haven't done a recent literature crawl), and this is unsurprising -- you'd expect ensembles to do well when there's a dearth of labeled training data, and here there are only a few hundred per label. But then it's typical on both CIFAR10 and CIFAR100 to use simple data augmentation schemes which aren't employed here, and these and other forms of regularization are a simpler alternative to a complicated iterative augmentation scheme like this.----------------It'd be easier to sell this method either as an option for scarce labeled datasets where data augmentation is non-trivial (but then for most image-related applications, random crops and reflections are easy and valid), but that would necessitate different benchmarks, and comparison against simpler methods like said data augmentation, dropout (especially, due to the ensemble interpretation), and so on.""]",0.88525390625,0.11474609375,1,0.30908203125,0.69091796875,0,0.237060546875,0.76318359375,0,0.92236328125,0.077392578125,1,0.2357177734375,0.76416015625,0,0.353515625,0.646484375,0
49,49,49,49,49,49,https://openreview.net/forum?id=SJJN38cge,"-This paper proposes a transfer learning method addressing optimization complexity and class imbalance..-Overall, I believe the proposed idea of reweighing is interesting, but the work can be globally improved/clarified..-The main problem with this paper is the writing.

This work proposes to use basic probability assignment to improve deep transfer learning..The authors also suggest learning the convolutional filters separately to break non-convexity..I do not buy the explanation about the use of both training and validation sets to compute BPA.","[""This work proposes to use basic probability assignment to improve deep transfer learning. A particular re-weighting scheme inspired by Dempster-Shaffer and exploiting the confusion matrix of the source task is introduced. The authors also suggest learning the convolutional filters separately to break non-convexity. ----------------The main problem with this paper is the writing. There are many typos, and the presentation is not clear. For example, the way the training set for weak classifiers are constructed remains unclear to me despite the author's previous answer. I do not buy the explanation about the use of both training and validation sets to compute BPA. Also, I am not convinced non-convexity is a problem here and the author does not provide an ablation study to validate the necessity of separately learning the filters. One last question is CIFAR has three channels and MNIST only one: How it this handled when pairing the datasets in the second set of experiments?----------------Overall, I believe the proposed idea of reweighing is interesting, but the work can be globally improved/clarified. ----------------I suggest a reject. "", ""Update: I thank the author for his comments! At this point, the paper is still not suitable for publication, so I'm leaving the rating untouched.----------------This paper proposes a transfer learning method addressing optimization complexity and class imbalance.----------------My main concerns are the following:----------------1. The paper is quite hard to read due to typos, unusual phrasing and loose use of terminology like “distributed”, “transfer learning” (meaning “fine-tuning”), “softmax” (meaning “fully-connected”), “deep learning” (meaning “base neural network”),  etc. I’m still not sure I got all the details of the actual algorithm right.----------------2. The captions to the figures and tables are not very informative – one has to jump back and forth through the paper to understand what the numbers/images mean.----------------3. From what I understand, the authors use “conventional transfer learning” to refer to fine-tuning of the fully-connected layers only (I’m judging by Figure 1). In this case, it’s essential to compare the proposed method with regimes when some of the convolutional layers are also updated. This comparison is not present in the paper.----------------Comments on the pre-review questions:----------------1. Question 1: If the paper only considers the case |C|==|L|, it would be better to reduce the notation clutter.----------------2. Question 2: It is still not clear what the authors mean by distributed transfer learning. Figure 1 is supposed to highlight the difference from the conventional approach (fine-tuning of the fully-connected layers; by the way, I don’t think, Softmax is a conventional term for fully-connected layers). From the diagram, it follows that the base CNN has the same number of convolutional filters at every layer and, in order to obtain a distributed ensemble, we need to connect (for some reason) filters with the same indices. This does not make a lot of sense to me but I’m probably misinterpreting the figure. Could the authors revise the diagram to make it clearer?----------------Overall, I think the paper needs significant refinement in order improve the clarity of presentation and thus cannot be accepted as it is now."", 'This paper proposed to use the BPA criterion for classifier ensembles.----------------My major concern with the paper is that it attempts to mix quite a few concepts together, and as a result, some of the simple notions becomes a bit hard to understand. For example:----------------(1) ""Distributed"" in this paper basically means classifier ensembles, and has nothing to do with the distributed training or distributed computation mechanism. Granted, one can train these individual classifiers in a distributed fashion but this is not the point of the paper.----------------(2) The paper uses ""Transfer learning"" in its narrow sense: it basically means fine-tuning the last layer of a pre-trained classifier.----------------Aside from the concept mixture of the paper, other comments I have about the paper are:----------------(1) I am not sure how BPA address class inbalance better than simple re-weighting. Essentially, the BPA criteria is putting equal weights on different classes, regardless of the number of training data points each class has. This is a very easy thing to address in conventional training: adding a class-specific weight term to each data point with the value being the inverse of the number of data points will do.----------------(2) Algorithm 2 is not presented correctly as it implies that test data is used during training, which is not correct: only training and validation dataset should be used. I find the paper\'s use of ""train/validation"" and ""test"" quite confusing: why ""train/validation"" is always presented together? How to properly distinguish between them?----------------(3) If I understand correctly, the paper is proposing to compute the BPA in a batch fashion, i.e. BPA can only be computed when running the model over the full train/validation dataset. This contradicts with the stochastic gradient descent that are usually used in deep net training - how does BPA deal with that? I believe that an experimental report on the computation cost and timing is missing.----------------In general, I find the paper not presented in its clearest form and a number of key definitions ambiguous.']",0.8173828125,0.182373046875,1,0.89404296875,0.10589599609375,1,0.2005615234375,0.79931640625,0,0.89892578125,0.1009521484375,1,0.0557861328125,0.9443359375,0,0.279052734375,0.72119140625,0
50,50,50,50,50,50,https://openreview.net/forum?id=SJQNqLFgl,"-There are two aspects of the paper that I particularly valued: firstly, the excellent review of recent works, which made me realize how many things I have been missing myself..The authors explore a number of architectures on CIFAR-10 and CIFAR-100 guided by these principles..-However I think this work is still half-done, and even though working on this project is a great idea, the authors do not yet do it properly.

The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems..Essentially, it reads like a review paper about modern CNN architectures..It also proposes a few new architectural ideas inspired by these rules.","['The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems. Essentially, it reads like a review paper about modern CNN architectures. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CIFAR-100, but seem to achieve relatively poor performance on these datasets (Table 1), so their merit is unclear to me.----------------I\'m not sure if such a collection of rules extracted from prior work warrants publication as a research paper. It is not a bad idea to try and summarise some of these observations now that CNNs have been the model of choice for computer vision tasks for a few years, and such a summary could be useful for newcomers. However, a lot of it seems to boil down to common sense (e.g. #1, #3, #7, #11). The rest of it might be more suited for an ""introduction to training CNNs"" course / blog post. It also seems to be a bit skewed towards recent work that was fairly incremental (e.g. a lot of attention is given to the flurry of ResNet variants).----------------The paper states that ""it is universal in all convolutional neural networks that the activations are downsampled and the number of channels increased from the input to the final layer"", which is wrong. We already discussed this previously re: my question about design pattern 5, but I think the answer that was given (""the nature of design patterns is that they only apply some of the time"") does not excuse making such sweeping claims. This should probably be removed.----------------""We feel that normalization puts all the layer\'s input samples on more equal footing, which allows backprop to train more effectively"" (section 3.2, 2nd paragraph) is very vague language that has many possible interpretations and should probably be clarified. It also seems odd to start this sentence with ""we feel"", as this doesn\'t seem like the kind of thing one should have an opinion about. Such claims should be corroborated by experiments and measurements. There are several other instances of this issue across the paper.----------------The connection between Taylor series and the proposed Taylor Series Networks seems very tenuous and I don\'t think the name is appropriate. The resulting function is not even a polynomial as all the terms represent different functions -- f(x) + g(x)**2 + h(x)**3 + ... is not a particularly interesting object, it is just a nonlinear function of x.----------------Overall, the paper reads like a collection of thoughts and ideas that are not very well delineated, and the experimental results are unconvincing.', 'The authors have grouped recent work in convolutional neural network design (specifically with respect to image classification) to identify core design principles guiding the field at large. The 14 principles they produce (along with associated references) include a number of useful and correct observations that would be an asset to anyone unfamiliar with the field. The authors explore a number of architectures on CIFAR-10 and CIFAR-100 guided by these principles.----------------The authors have collected a quality set of references on the subject and grouped them well which is valuable for young researchers. Clearly the authors explored a many of architectural changes as part of their experiments and publicly available code base is always nice.----------------Overall the writing seems to jump around a bit and the motivations behind some design principles feel lost in the confusion. For example, ""Design Pattern 4: Increase Symmetry argues for architectural symmetry as a sign of beauty and quality"" is presented as one of 14 core design principles without any further justification. Similarly ""Design Pattern 6: Over-train includes any training method where the network is trained on a harder problem than necessary to improve generalization performance of inference"" is presented in the middle of a paragraph with no supporting references or further explanation.----------------The experimental portion of this paper feels scattered with many different approaches being presented based on subsets of the design principles. In general, these approaches either are minor modifications of existing networks (different FractalNet pooling strategies) or are novel architectures that do not perform well. The exception being the Fractal-of-Fractal network which achieves slightly improved accuracy but also introduces many more network parameters (increased capacity) over the original FractalNet.------------------------Preliminary rating:--------It is a useful and perhaps noble task to collect and distill research from many sources to find patterns (and perhaps gaps) in the state of a field; however, some of the patterns presented do not seem well developed and include principles that are poorly explained. Furthermore, the innovative architectures motivated by the design principles either fall short or achieve slightly better accuracy by introducing many more parameters (Fractal-Of-Fractal networks). For a paper addressing the topic of higher level design trends, I would appreciate additional rigorous experimentation around each principle rather than novel architectures being presented.', 'The authors take on the task of figuring out a set of design patterns for current deep architectures - namely themes that are recurring in the literature.  If one may say so, a distributed representation of deep architectures. ----------------There are two aspects of the paper that I particularly valued: firstly, the excellent review of recent works, which made me realize how many things I have been missing myself.  Secondly, the ""community service"" aspect of helping someone who starts figure out the ""coordinate system"" for deep architectures - this could potentially be more important than introducing yet-another trick of the trade, as most other submissions may do.----------------However I think this work is still half-done, and even though working on this project is a great idea, the authors do not yet do it properly. ----------------Firstly, I am not too sure how the choice of these 14 patterns was made. Maxout for instance (pattern 14) is one of the many nonlinearities (PreLU, ReLU, ...) and I do not see how it stands on the same grounds as something as general as ""3 Strive for simplicity"".----------------Similarly some of the patterns are as vague as ""Increase symmetry"" and are backed up by statements such as ""we noted a special degree of elegance in the FractalNet"". I do not see how this leads to a design pattern that can be applied to a new architecture - or if it applies to anything other than the FractalNet. ----------------Some other patterns are phrased with weird names ""7 Cover the problem space"" - which I guess stands for dataset augmentation; or ""6 over-train"" which is not backed up by a single reference. Unless the authors relate it to regularization (text preceding ""overtrain""), which then has no connection to the description of ""over-train"" provided by the authors (""training a network on a harder problem to improve generalization""). If ""harder problem"" means one where one adds an additional term (i.e. the regularizer), the authors are doing harm to the unexperienced reader, confusing ""regularization"" with something that sounds like ""overfitting"" (i.e. the exact opposite).----------------Furthermore, the extensions proposed in Section 4 seem a bit off tune - in particular I could not figure out ---------how the Taylor Series networks stem from any of the design patterns proposed in the rest of the paper. ---------whether the text between 4.1 and 4.1.1 is another of the architecture innovations (and if yes, why it is not in the 4.1.2, or 4.1.0) ---------and, most importantly, how these design patterns would be deployed in practice to think of a new network. ----------------To be more concrete, the authors mention that they propose the ""freeze-drop-path"" variant from ""symmetry considerations"" to ""drop-path"". --------Is this an application of the ""increase symmetry"" pattern? How would ""freeze-drop-path"" be more symmetric that ""drop-path""?-------- Can this be expressed concretely, or is it some intuitive guess? If the second, it is not really part of applying a pattern, in my understanding. If the first, this is missing. ------------------------What I would have appreciated more (and would like to see in a revised version) would have been a table of ""design patterns"" on one axis, ""Deep network"" on another, and a breakdown of which network applies which design pattern. ----------------A big part of the previous work is also covered in cryptic language - some minimal explanation of what is taking place in the alternative works would be useful. ------------------------  ']",0.8564453125,0.143798828125,1,0.892578125,0.10736083984375,1,0.2286376953125,0.771484375,0,0.91796875,0.081787109375,1,0.0970458984375,0.90283203125,0,0.4345703125,0.5654296875,0
51,51,51,51,51,51,https://openreview.net/forum?id=SJRpRfKxx,"---The rational for concatenating the features extracted from the original clip,
---and the features extracted from the saliency weighted clip seems to contradict the initial hypothesis that `eliminating or down-weighting pixels that are not important' will improve performance..-The authors should also mention the current state-of-the-art results in Table 4, for comparison..-One weak point is the action recognition section, where the comparison between the two (1)(2) and (3) seems unfair.

The authors formulate a recurrent deep neural network to predict human fixation locations in videos as a mixture of Gaussians..They train the model using maximum likelihood with actual fixation data..Apart from evaluating how good the model performs at predicting fixations, they combine the saliency predictions with the C3D features for action recognition.","['The authors formulate a recurrent deep neural network to predict human fixation locations in videos as a mixture of Gaussians. They train the model using maximum likelihood with actual fixation data. Apart from evaluating how good the model performs at predicting fixations, they combine the saliency predictions with the C3D features for action recognition.----------------quality: I am missing a more thorough evaluation of the fixation prediction performance. The center bias performance in Table 1 differs significantly from the on in Table 2. All the state-of-the-art models reported in Table 2 have a performance worse than the center bias performance reported in Table 1. Is there really no other model better than the center bias? Additionally I am missing details on how central bias and human performance are modelled. Is human performance cross-validated?----------------You claim that your ""results are very close to human performance (the difference is only 3.2%). This difference is actually larger than the difference between Central Bias and your model reported in Table 1. Apart from this, it is dangerous to compare AUC performance differences due to e.g. saturation issues.----------------clarity: the explanation for Table 3 is a bit confusing, also it is not clear why the CONV5 and the FC6 models differ in how the saliency map is used. At least one should also evaluate the CONV5 model when multiplying the input with the saliency map to see how much of the difference comes from the different ways to use the saliency map and how much from the different features.----------------Other issues:----------------You cite Kümmerer et. al 2015 as a model which ""learns ... indirectly rather than from explicit information of where humans look"", however the their model has been trained on fixation data using maximum-likelihood.----------------Apart from these issues, I think the paper make a very interesting contribution to spatio-temporal fixation prediction. If the evaluation issues given above are sorted out, I will happily improve my rating.', ""This work proposes to a spatiotemporal saliency network that is able to mimic human fixation patterns,--------thus helping to prune irrelevant information from the video and improve action recognition.----------------The work is interesting and has shown state-of-the-art results on predicting human attention on action videos.--------It has also shown promise for helping action clip classification.----------------The paper would benefit from a discussion on the role of context in attention.--------For instance, if context is important, and people give attention to context, why is it not incorporated automatically in your model?----------------One weak point is the action recognition section, where the comparison between the two (1)(2) and (3) seems unfair.--------The attention weighted feature maps in fact reduce the classification performance, and only improve performance when doubling the feature and associated model complexity by concatenating the weighted maps with the original features.----------------Is there a way to combine the context and attention without concatenation?--------The rational for concatenating the features extracted from the original clip,--------and the features extracted from the saliency weighted clip seems to contradict the initial hypothesis that `eliminating or down-weighting pixels that are not important' will improve performance.----------------The authors should also mention the current state-of-the-art results in Table 4, for comparison.----------------# Other comments:--------# Abstract--------- Typo: `mixed with irrelevant ...'--------``Time consistency in videos ... expands the temporal domain from few frames to seconds'' - These two points are not clear, probably need a re-write.----------------# Contributions--------- 1) `The model can be trained without having to engineer spatiotemporal features' - you would need to collect training data from humans though.. ----------------# Section 3.1--------The number of fixation points is controlled to be fixed for each frame - how is this done?----------------In practice we freeze the layers of the C3D network to values pretrained by Tran etal.--------What happens when you allow gradients to flow back to the C3D layers?--------Is it not better to allow the features to be best tuned for the final task?----------------The precise way in which the features are concatenated needs to be clarified in section 3.4.----------------Minor typo:--------`we added them trained central bias'"", 'This paper proposes a new method for estimating visual attention in videos. The input clip is first processed by a convnet (in particular, C3D) to extract visual features. The visual features are then passed to LSTM. The hidden state at each time step in LSTM is used to generate the parameters in a Gaussian mixture model. Finally, the visual attention map is generated from the Gaussian mixture model.----------------Overall, the idea in this paper is reasonable and the paper is well written. RNN/LSTM has been used in lots of vision problem where the outputs are discrete sequences, there has not been much work on using RNN/LSTM for problems where the output is continuous like in this paper.----------------The experimental results have demonstrated the effectiveness of the proposed approach. In particular, it outperforms other state-of-the-art on the saliency prediction task on the Hollywood2 datasets. It also shows improvement over baselines (e.g. C3D + SVM) on the action recognition task.----------------My only ""gripe"" of this paper is that this paper is missing some important baseline comparisons. In particular, it does not seem to show how the ""recurrent"" part help the overall performance. Although Table 2 shows RMDN outperforms other state-of-the-art, it might be due to the fact that it uses strong C3D features (while other methods in Table 2 use traditional handcrafted features). Since saliency prediction is essentially a dense image labeling problem (similar to semantic segmentation). For dense image labeling, there has been lots of methods proposed in the past two years, e.g. fully convolution neural network (FCN) or deconvnet. A straightforward baseline is to simply take FCN and apply it on each frame. If the proposed method still outperforms this baseline, we can know that the ""recurrent"" part really helps.']",0.814453125,0.185302734375,1,0.89990234375,0.10015869140625,1,0.308837890625,0.69091796875,0,0.9052734375,0.09478759765625,1,0.1182861328125,0.8818359375,0,0.38916015625,0.61083984375,0
52,52,52,52,52,52,https://openreview.net/forum?id=SJTQLdqlg,"-Authors have addressed all my pre-review questions and I am ok with their response..-Using k-nearest neighbors for memory access is not completely new..---The paper is very well written and the results are convincing.

K-nearest neighbors based memory for one-shot learning has also been explored in [R1]..But besides that I think this is a very nice and useful paper..I hope the authors will publish their code.","['This paper proposes a new memory module for large scale life-long and one-shot learning. The module is general enough that the authors apply the module to several neural network architectures and show improvements in performance.----------------Using k-nearest neighbors for memory access is not completely new. This has been recently explored in Rae et al., 2016 and Chandar et al., 2016. K-nearest neighbors based memory for one-shot learning has also been explored in [R1]. This paper provides experimental evidence that such an approach can be applied to a variety of architectures.----------------Authors have addressed all my pre-review questions and I am ok with their response.----------------Are the authors willing to release the source code to reproduce the results? At least for omniglot experiments and synthetic task experiments?----------------References:----------------[R1] Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z. Leibo, Jack Rae, Daan Wierstra, Demis Hassabis: Model-Free Episodic Control. CoRR abs/1606.04460 (2016)', 'A new memory module based on k-NN is presented.--------The paper is very well written and the results are convincing. ----------------Omniglot is a good sanity test and the performance is surprisingly good.--------The artificial task shows us that the authors claims hold and highlight the need for better benchmarks in this domain.--------And the translation task eventually makes a very strong point on practical usefulness of the proposed model.----------------I am not a specialist in memory networks so I trust the authors to double-check if all relevant references have been included (another reviewer mentioned associative LSTM). But besides that I think this is a very nice and useful paper. I hope the authors will publish their code.', 'The paper proposes a new memory module to be used as an addition to existing neural network models.----------------Pros:--------* Clearly written and original idea.--------* Useful memory module, shows nice improvements.--------* Tested on some big tasks.----------------Cons:--------* No comparisons to other memory modules such as associative LSTMs etc.']",0.90185546875,0.0980224609375,1,0.9619140625,0.037994384765625,1,0.5302734375,0.469482421875,1,0.90478515625,0.094970703125,1,0.59130859375,0.40869140625,1,0.580078125,0.419921875,1
53,53,53,53,53,53,https://openreview.net/forum?id=SJZAb5cel,"-I admit that the successive regularization make sense intuitively and is a very interesting direction to try..-Novelty: the way the authors propose to do transfer learning is by plugging models corresponding to each task, in a way that respects the known hierarchy (in terms of NLP ""complexity"") of those tasks..-- The successive regularization does not appear to be important in Table 8; a variance analysis would help to conclude.

The authors propose a transfer learning approach applied to a number of NLP tasks; the set of tasks appear to have an order in terms of complexity (from easy syntactic tasks to somewhat harder semantic tasks)..In that respect, the overall architecture looks more like a cascaded architecture than a transfer learning one..In addition to the architecture, the authors propose a regularization technique they call ""successive regularization"".","['The authors propose a transfer learning approach applied to a number of NLP tasks; the set of tasks appear to have an order in terms of complexity (from easy syntactic tasks to somewhat harder semantic tasks).----------------Novelty: the way the authors propose to do transfer learning is by plugging models corresponding to each task, in a way that respects the known hierarchy (in terms of NLP ""complexity"") of those tasks. In that respect, the overall architecture looks more like a cascaded architecture than a transfer learning one. There are some existing literature in the area (first two Google results found: https://arxiv.org/pdf/1512.04412v1.pdf, (computer vision) and https://www.aclweb.org/anthology/P/P16/P16-1147.pdf (NLP)). In addition to the architecture, the authors propose a regularization technique they call ""successive regularization"".----------------Experiments:----------------- The authors performed a number of experimental analysis to clarify what parts of their architecture are important, which is very valuable;----------------- The information ""transferred"" from one task to the next one is represented both using a smooth label embedding and the hidden representation of the previous task. At this point there is no analysis of which one is actually important, or if they are redundant (update: the authors mentioned they would add something there). Also, it is likely one would have tried first to feed label scores from one task to the next one, instead of using the trick of the label embedding -- it is unclear what the latter is actually bringing.----------------- The successive regularization does not appear to be important in Table 8; a variance analysis would help to conclude.', 'this work investigates a joint learning setup where tasks are stacked based on their complexity. to this end, experimental evaluation is done on pos tagging, chunking, dependency parsing, semantic relatedness, and textual entailment. the end-to-end model improves over models trained solely on target tasks.----------------although the hypothesis of this work is an important one, the experimental evaluation lacks thoroughness:----------------first, a very simple multi-task learning baseline [1] should be implemented where there is no hierarchy of tasks to test the hypothesis of the tasks should be ordered in terms of complexity.----------------second, since the test set of chunking is included in training data of dependency parsing, the results related to chunking with JMT_all are not informative. ----------------third, since the model does not guarantee well-formed dependency trees, thus, results in table 4 are not fair. ----------------minor issue:--------- chunking is not a word-level task although the annotation is word-level. chunking is a structured prediction task where we would like to learn a structured annotation over a sequence [2].----------------[1] http://ronan.collobert.com/pub/matos/2011_nlp_jmlr.pdf--------[2] http://www.cs.cmu.edu/~nasmith/LSP/', 'The paper introduce a way to train joint models for many NLP tasks. Traditionally, we treat these tasks as “pipeline” — the later tasks will depending on the output of the previous tasks. Here, the authors propose a neural approach which includes all the tasks in one single model. The higher level tasks takes (1) the predictions from the lower level tasks and (2) the hidden representations of the lower level tasks. Also proposed in this paper, is the successive regularization. Intuitively, this means that, when training the high level tasks, we don’t want to change the model in the lower levels by too much so that the lower level tasks can keep a reasonable accuracy of prediction.----------------On the modeling side, I think the proposed model is very similar comparing to (Zhang and Weiss, ACL 2016) and SPINN (Bowman et al, 2016) in a even simpler way. The number of the experiments are good. But I am not sure I am convinced by the numbers in Table 1 since the patterns are not very clear there — sometimes, the performance of the higher level tasks even goes down when training with more tasks (sometimes it does go up, but also not very significant and stable). The dependency scores, although I don’t think this is a serious problem, comparing the UAS/LAS when the output is not guaranteed to be a well-formed tree isn’t strictly speaking fair.----------------I admit that the successive regularization make sense intuitively and is a very interesting direction to try. However, without a careful study of the training schema of such model, the current results on successive regularization do not convince me that it should be the right thing to do in such models (the current results are not strong enough to show that). The training methods need to be explored here including things as iteratively train on different tasks, and the relationship between the number of training iterations of a task and it’s training set size (and loss on this task etc).']",0.82421875,0.1756591796875,1,0.4716796875,0.5283203125,0,0.330810546875,0.66943359375,0,0.92724609375,0.07275390625,1,0.122802734375,0.876953125,0,0.3525390625,0.6474609375,0
54,54,54,54,54,54,https://openreview.net/forum?id=SJc1hL5ee,"The paper does not distinguish OPQ and PQ properly at multiple places especially in the experiments..In addition to the Gong et al..-More comments:
---- The title does not make it clear that the paper focuses on wide and shallow text classification models.

The paper presents a few tricks to compress a wide and shallow text classification model based on n-gram features..These tricks include (1) using (optimized) product quantization to compress embedding weights (2) pruning some of the vocabulary elements (3) hashing to reduce the storage of the vocabulary (this is a minor component of the paper)..The paper focuses on models with very large vocabularies and shows a reduction in the size of the models at a relatively minor reduction of the accuracy.","['The paper presents a few tricks to compress a wide and shallow text classification model based on n-gram features. These tricks include (1) using (optimized) product quantization to compress embedding weights (2) pruning some of the vocabulary elements (3) hashing to reduce the storage of the vocabulary (this is a minor component of the paper). The paper focuses on models with very large vocabularies and shows a reduction in the size of the models at a relatively minor reduction of the accuracy.----------------The problem of compressing neural models is important and interesting. The methods section of the paper is well written with good high level comments and references. However, the machine learning contributions of the paper are marginal to me. The experiments are not too convincing mainly focusing on benchmarks that are not commonly used. The implications of the paper on the state-of-the-art RNN text classification models is unclear.----------------The use of (optimized) product quantization for approximating inner product is not particularly novel. Previous work also considered doing this. Most of the reduction in the model sizes comes from pruning vocabulary elements. The method proposed for pruning vocabulary elements is simply based on the assumption that embeddings with larger L2 norm are more important. A coverage heuristic is taken into account too. From a machine learning point of view, the proper baseline to solve this problem is to have a set of (relaxed) binary coefficients for each embedding vector and learn the coefficients jointly with the weights. An L1 regularizer on the coefficients can be used to encourage sparsity. From a practical point of view, I believe an important baseline is missing: what if one simply uses fewer vocabulary elements (e.g based on subword units - see https://arxiv.org/pdf/1508.07909.pdf) and retrain a smaller models?----------------Given the lack of novelty and the missing baselines, I believe the paper in its current form is not ready for publication at ICLR.----------------More comments:--------- The title does not make it clear that the paper focuses on wide and shallow text classification models. Please revise the title.--------- The paper cites an ArXiv manuscript by Carreira-Perpinan and Alizadeh (2016) several times, which has the same title as the submitted paper. Please make the paper self-contained and include any supplementary material in the appendix.--------- In Fig 2 does the square mark PQ or OPQ? The paper does not distinguish OPQ and PQ properly at multiple places especially in the experiments.--------- The paper argues the wide and shallow models are the state of the art in small datasets. Is this really correct? What about transfer learning?', ""The paper proposes a series of tricks for compressing fast (linear) text classification models. The paper is clearly written, and the results are quite strong. The main compression is achieved via product quantization, a technique which has been explored in other applications within the neural network model compression literature. In addition to the Gong et al. work which was cited, it would be worth mentioning Quantized Convolutional Neural Networks for Mobile Devices (CVPR 2016, https://arxiv.org/pdf/1512.06473v3.pdf), which similarly incorporates fine tuning to mitigate losses due to quantization error.----------------As such, one criticism of the paper is that it is a more-or-less straightforward application of techniques that have already been shown to be effective elsewhere in the model compression literature, and so isn't particularly surprising or deep from a technical perspective. However, this is as far as I am aware the first work applying these techniques to text classification, and the results are strong enough that I think it will be of interest to those working on models for text-based tasks."", 'This paper describes how to approximate the FastText approach such that its memory footprint is reduced by several orders of magnitude, while preserving its classification accuracy. The original FastText approach was based on a linear classifier on top of bag-of-words embeddings. This type of method is extremely fast to train and test, but the model size can be quite large.----------------This paper focuses on approximating the original approach with lossy compression techniques. Namely, the embeddings and classifier matrices A and B are compressed with Product Quantization, and an aggressive dictionary pruning is carried out. Experiments on various datasets (either with small or large number of classes) are conducted to tune the parameters and demonstrate the effectiveness of the approach. With a negligible loss in classification accuracy, an important reduction in term of model size (memory footprint) can be achieved, in the order of 100~1000 folds compared to the original size.----------------The paper is well written overall. The goal is clearly defined and well carried out, as well as the experiments. Different options for compressing the model data are evaluated and compared (e.g. PQ vs LSH), which is also interesting. Nevertheless the paper does not propose by itself any novel idea for text classification. It just focuses on adapting existing lossy compression techniques, which is not necessarily a problem. Specifically, it introduces:--------  - a straightforward variant of PQ for unnormalized vectors,--------  - dictionary pruning is cast as a set covering problem (which is NP-hard), but a greedy approach is shown to yield excellent results nonetheless,--------  - hashing tricks and bloom filter are simply borrowed from previous papers.----------------These techniques are quite generic and could as well be used in other works. ------------------------Here are some minor problems with the paper:----------------  - it is not made clear how the full model size is computed. What is exactly in the model? Which proportion of the full size do the A and B matrices, the dictionary, and the rest, account for? It is hard to follow where is the size bottleneck, which also seems to depend on the target application (i.e. small or large number of test classes). It would have been nice to provide a formula to calculate the total model size as a function of all parameters (k,b for PQ and K for dictionary, number of classes).--------  --------  - some parts lack clarity. For instance, the greedy approach to prune the dictionary is exposed in less than 4 lines (top of page 5), though it is far from being straightforward. Likewise, it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of KB.--------  ----------------Overall this looks like a solid work, but with potentially limited impact research-wise.']",0.93017578125,0.06988525390625,1,0.87939453125,0.12066650390625,1,0.36474609375,0.63525390625,0,0.8525390625,0.1473388671875,1,0.130859375,0.869140625,0,0.427001953125,0.57275390625,0
55,55,55,55,55,55,https://openreview.net/forum?id=SJqaCVLxx,"What are its advantages/dis-advantages?.-Unfortunately the paper is extremely hard to understand and it is not at all clear what the exact training algorithm is..-- A single experiment on MNIST is too small to adequately describe the algorithm performance.

The authors seems to have proposed a genetic algorithm for learning the features of a convolutional network (LeNet-5 to be precise)..The authors seem to have merged the training and validation set of the MNIST dataset and use only a subset of it..The experimental section is equally confusing and unconvincing.","[""The authors seems to have proposed a genetic algorithm for learning the features of a convolutional network (LeNet-5 to be precise). The algorithm is validated on some version of the MNIST dataset. ----------------Unfortunately the paper is extremely hard to understand and it is not at all clear what the exact training algorithm is. Neither do the authors ever motivate why do such a training as opposed to the standard back-prop. What are its advantages/dis-advantages? Furthermore the experimental section is equally unclear. The authors seem to have merged the training and validation set of the MNIST dataset and use only a subset of it. It is not clear why is that the case and what subset they use. In addition, to the best of my understanding, the results reported are RMSE as opposed to classification error. Why is that the case? ----------------In short, the paper is extremely hard to follow and it is not at all clear what the training algorithm is and how is it better than standard way of training. The experimental section is equally confusing and unconvincing. ----------------Other comments: ---------- The figures still say LeCun-5---------- The legends of the plots are not in english. Hence I'm not sure what is going on there. ---------- The paper is riddled with typos and hard to understand phrasing. "", ""Unfortunately, this paper is very difficult to understand.  The current version of this paper seems improved compared to the initial version, but still far from a finished level.  I'd encourage the authors to keep editing over the language and presentation.----------------I also think it would be good to also try answering some of the following questions very clearly in the paper:----------------- What is the advantage, if any, of the proposed algorithm over SGD?  What is the motivation and goal of the work beyond MNIST benchmarking?----------------- Why are few training examples used?  Is this a scenario in which the system might have an advantage?----------------- Concretely describe the genetic algorithms terminology used in the algorithm descriptions, and what each term means in the context of the convolutional network.----------------- Try to make sure that the method, as described, can be understood by a reader without much prior background on genetic algorithms.----------------- A single experiment on MNIST is too small to adequately describe the algorithm performance.  Consider using a second or third dataset and/or experimental application.----------------Much work is still needed on the paper's writing before it can be understood well enough.  I hope that some of this might be useful in helping to improve. I would encourage the authors to try to find outside readers, preferably fluent in English, to work with on a frequent basis before resubmitting to another venue."", 'The paper is still extremely poorly written and presented despite multiple reviewers asking to address that issue. The frequent spelling mistakes and incoherent sentences and unclear presentation make reading and understanding the paper very difficult and time consuming. Consider getting help from someone with good english and presentation skills.']",0.845703125,0.154296875,1,0.70263671875,0.29736328125,1,0.137451171875,0.86279296875,0,0.87890625,0.12127685546875,1,0.11065673828125,0.88916015625,0,0.451171875,0.548828125,0
56,56,56,56,56,56,https://openreview.net/forum?id=SJttqw5ge,"---For example, when you say ""while the parameters of the subtask controller are
---frozen"", this sounds to me like you're having some kind of two-timescale stochastic gradient
---descent..---Konidaris also has a line of work  on ""parametrized skills"":
---[""Learning Parameterized Skills""..-Under section 3, you say ""cooperate with each other"" which sounds to me very much
---like a multi-agent setting, which your work does not explore in this way.

The experiments unfortunately are on a rather simplistic 2D maze, and it would have been worthwhile to see how the approach scaled to more complex tasks of the sort usually seen in deep RL papers these days (e.g, Atari, physics simulators etc.)..The skills are learned using a differential temporally extended memory networks with an attention mechanism..The authors demonstrate that their method generalizes well to sequences of varying length as well as to new combinations of sub-goals (i.e., if the agent knows how to pick up a diamond and how to visit an apple, it can also visit the diamond).","['Description:----------------This paper presents a reinforcement learning architecture where, based on ""natural-language"" input, a meta-controller chooses subtasks and communicates them to a subtask controller that choose primitive actions, based on the communicated subtask. The goal is to scale up reinforcement learning agents to large-scale tasks.----------------The subtask controller embeds the subtask definition (arguments) into vectors by a multi-layer perceptron including an ""analogy-making"" regularization. The subtask vectors are combined with inputs at each layer of a CNN. CNN outputs (given the observation and the subtask) are then fed to one of two MLPs; one to compute action probabilities in the policy (exponential falloff of MLP outputs) and the other to compute termination probability (sigmoid from MLP outputs).----------------The meta controller takes a list of sentences as instructions embeds them into a sequence of subtask arguments (not necessarily a one-to-one mapping). A context vector is computed by a CNN from the observation, the previous sentence embedding, the previous subtask and its completion state. The subtask arguments are computed from the context vector through further mechanisms involving instruction retrieval from memory pointers, and hard/soft decisions whether to update the subtask or not.----------------Training involves policy distillation+actor-critic training for the subtask controller, and actor-critic training for the meta controller keeping the subtask controller frozen.----------------The system is tested in a grid world where the agent moves and interacts with (picks up/transforms) various item/enemy types.--------It is compared to a) a flat controller not using a subtask controller, and b) subtask control by mere concatenation of the subtask embedding to the input with/without the analogy-making regularization.------------------------Evaluation:----------------The proposed architecture seems reasonable, although it is not clear why the specific way of combining subtask embeddings in the subtask controller would be the ""right"" way to do it.----------------I do not feel the grid world here really represents a ""large-scale task"": in particular the 10x10 size of the grid is very small. This is disappointing since this was a main motivation of the work.----------------Moreover, the method is not compared to any state of the art alternatives. This is especially problematic because the test is not on established benchmarks. It is not really possible, based on the shown results, to put the performance in context of other works.', 'This paper presents an architecture and corresponding algorithms for--------learning to act across multiple tasks, described in natural language.--------The proposed system is hierarchical and is closely related to the options--------framework. However, rather than learning a discrete set of options, it learns--------a mapping from natural instructions to an embedding which implicitly (dynamically)--------defines an option. This is a novel and interesting new perspective on options--------which had only slightly been explored in the linear setting (see comments below).--------I find the use of policy distillation particularly relevant for this setting.--------This, on its own, could be a takeaway for many RL readers who might not necessarily--------be interested about NLP applications.----------------In general, the paper does not describe a single, simple, end-to-end,--------recipe for learning with this architecture. It rather relies on many recent--------advances skillfully combined: generalized advantage estimation, analogy-making--------regularizers, L1 regularization, memory addressing, matrix factorization,--------policy distillation. I would have liked to see some analysis but--------understand that it would have certainly been no easy task.--------For example, when you say ""while the parameters of the subtask controller are--------frozen"", this sounds to me like you\'re having some kind of two-timescale stochastic gradient--------descent. I\'m also unsure how you deal with the SMDP structure in your gradient--------updates when you move to the ""temporal abstractions"" setting.----------------I am inclined to believe that this approach has the potential to scale up to--------very large domains, but paper currently does not demonstrate this--------empirically. Like any typical reviewer, I would be tempted to say that--------you should perform larger experiments. However, I\'m also glad that you have--------shown that your system also performs well in a ""toy"" domain. The characterization--------in figure 3 is insightful and makes a good point for the analogy regularizer--------and need for hierarchy.----------------Overall, I think that the proposed architecture would inspire other researchers--------and would be worth being presented at ICLR. It also contains novel elements--------(subtask embeddings) which could be useful outside the deep and NLP communities--------into the more ""traditional"" RL communities.----------------# Parameterized Options----------------Sutton et. al (1999) did not explore the concept--------of *parameterized* options originally. It only came later, perhaps first with--------[""Optimal policy switching algorithms for reinforcement--------learning, Comanici & Precup, 2010""] or--------[""Unified Inter and Intra Options Learning Using Policy Gradient Methods"", Levy & Shimkin, 2011].--------Konidaris also has a line of work  on ""parametrized skills"":--------[""Learning Parameterized Skills"". da Silva, Konidaris, Barto, 2012)]--------or [""Reinforcement Learning with Parameterized Actions"". Masson, Ranchod, Konidaris, 2015].----------------Also, I feel that there is a very important distinction to be made with--------the expression ""parametrized options"". In your work, ""parametrized"" comes in--------two flavors. In the spirit of policy gradient methods,--------we can have options whose policies and termination functions are represented--------by function approximators (in the same way that we have function approximation--------for value functions). Those options have parameters and we might call them--------""parameterized"" because of that. This is the setting of Comanicy & Precup (2010),--------Levy & Shimkin (2011) Bacon & Precup (2015), Mankowitz, Mann, and--------Mannor (2016) for example.----------------Now, there a second case where options/policies/skills take parameters *as inputs*--------and act accordingly. This is what Konidaris & al. means by ""parameterized"", whose--------meaning differs from the ""function approximation"" case above.--------In your work, the embedding of subtasks arguments is the ""input"" to your options--------and therefore behave as ""parameters"" in the sense of Konidaris.----------------# Related Work----------------I CTRL-F through the PDF but couldn\'t find references to any of S.R.K. Branavan\'s--------work. Branavan\'s PhD thesis had to do with using control techniques from RL--------in order to interpret natural instructions so as to achieve a goal. For example,--------in ""Reinforcement Learning for Mapping Instructions to Actions"", an RL agent--------learns from ""Windows troubleshooting articles"" to interact with UI elements--------(environment) through a Softmax policy (over linear features) learned by policy--------gradient methods.----------------As you mention under ""Instruction execution"" the focus of your work in--------on generalization, which is not treated explicitely (afaik) in Branavan\'s work.--------Still, it shares some important algorithmic and architectural similarities which--------should be discussed explicitly or perhaps even compared to in your experiments--------(as a baseline).----------------## Zero-shot and UVFA----------------It might also want to consider--------""Learning Shared Representations for Value Functions in Multi-task--------Reinforcement Learning"", Borsa, Graepel, Shawe-Taylor]--------under the section ""zero-shot tasks generalization"". ------------------------# Minor Issues----------------I first read the abstract without knowing what the paper would be about--------and got confused in the second sentence. You talk about ""longer sequences of--------previously seen instructions"", but I didn\'t know what clearly--------meant by ""instructions"" until the second to last sentence where you specify--------""instructions described by *natural language*."" You could perhaps--------re-order the sentences to make it clear in the second sentence that you are--------interested in NLP problems.----------------Zero-generalization: I was familiar with the term ""one-shot"" but not ""zero-shot"".--------The way that the second sentence ""[...] to have *similar* zero-shot [...]"" follows--------from the first sentence might as well hold for the ""one-shot"" setting. You--------could perhaps add a citation to ""zero-shot"", or define it more--------explicitly from the beginning and compare it to the one-shot setting. It could--------also be useful if you explain how zero-shot relates to just the notion of--------learning with ""priors"".----------------Under section 3, you say ""cooperate with each other"" which sounds to me very much--------like a multi-agent setting, which your work does not explore in this way.--------You might want to choose a different terminology or explain more precisely if there--------is any connection with the multi-agent setting.----------------The second sentence of section 6 is way to long and difficult to parse. You could--------probably split it in two or three sentences.', 'This paper can be seen as instantiating a famous paper by the founder of AI John McCarthy on learning to take advice (which was studied in depth by other later researchers, such as Jack Mostow in the card game Hearts). The idea is that the agent is given high level instructions on how to solve a problem, and must distill from it a low level policy. This is quite related to how humans learn complex tasks in many domains (e.g., driving, where a driving instructor may provide advice such as ""keep a certain distance from the car in front""). ----------------A fairly complex neural deep learning controller architecture is used, although the details of this system are somewhat confusing in terms of many details that are presented. A simpler approach might have been easier to follow, at least initially. The experiments unfortunately are on a rather simplistic 2D maze, and it would have been worthwhile to see how the approach scaled to more complex tasks of the sort usually seen in deep RL papers these days (e.g, Atari, physics simulators etc.). ----------------Nice overall idea, somewhat confusing description of the solution, and an inadequate set of experiments on a less than satisfactory domain of 2D grid worlds. ', ""The paper presents a hierarchical DRL algorithm that solves sequences of navigate-and-act tasks in a 2D maze domain. During training and evaluation, a list of sub-goals represented by text is given to the agent and its goal is to learn to use pre-learned skills in order to solve a list of sub-goals. The authors demonstrate that their method generalizes well to sequences of varying length as well as to new combinations of sub-goals (i.e., if the agent knows how to pick up a diamond and how to visit an apple, it can also visit the diamond). ----------------Overall, the paper is of high technical quality and presents an interesting and non-trivial combination of state-of-the-art advancements in Deep Learning (DL) and Deep Reinforcement Learning (DRL). In particular, the authors presents a DRL agent that is hierarchical in the sense that it can learn skills and plan using them. The skills are learned using a differential temporally extended memory networks with an attention mechanism. The authors also make a novel use of analogy making and parameter prediction. ----------------However, I find it difficult to understand from the paper why the presented problem is interesting and why hadn't it bee solved before. Since the domain being evaluated is a simple 2D maze, using deep networks is not well motivated. Similar problems have been solved using simpler models. In particular, there is a reach literature about planning with skills that had been ignored completely by the authors. Since all of the skills are trained prior to the evaluation of the hierarchical agent, the problem that is being solved is much more similar to supervised learning than reinforcement learning (since when using the pre-trained skills the reward is not particularly delayed). The generalization that is demonstrated seems to be limited to breaking a sentence (describing the subtask) into words (item, location, action). ----------------The paper is difficult to read, it is constantly switching between describing the algorithm and giving technical details. In particular, I find it to be overloaded with details that interfere with the general understanding of the paper. I suggest moving many of the implementation details into the appendix. The paper should be self-contained, please do not assume that the reader is familiar with all the methods that you use and introduce all the relevant notations. ----------------I believe that the paper will benefit from addressing the problems I described above and will make a better contribution to the community in a future conference. ""]",0.75537109375,0.2447509765625,1,0.70263671875,0.29736328125,1,0.174560546875,0.8251953125,0,0.86669921875,0.13330078125,1,0.0648193359375,0.93505859375,0,0.316162109375,0.68408203125,0
57,57,57,57,57,57,https://openreview.net/forum?id=Sk-oDY9ge,"The different architectures are explained on an
---intuitive level and might benefit from a clear mathematical
---definition..---The paper is well written and the results show improvements over reasonable baselines..---2) SNPtoVec still requires training a very fat autoencoder network on X --- I suppose this doesn't contribute to the size of the final run-time model and overfitting is avoided because the parameters are fit in an unsupervised manner?

The paper addresses the important problem (d>>n) in deep learning..The problem addressed here is practically important (supervised learning with n<<d), and as far as I know, the approach is novel..I thought their proposed solution was innovative, and I enjoyed the paper.","['The paper presents an application of deep learning to genomic SNP data--------with a comparison of possible approaches for dealing with the very--------high data dimensionality. The approach looks very interesting but the--------experiments are too limited to draw firm conclusions about the--------strengths of different approaches. The presentation would benefit from--------more precise math.------------------------Quality:----------------The basic idea of the paper is interesting and the applied deep--------learning methodology appears reasonable. The experimental evaluation--------is rather weak as it only covers a single data set and a very limited--------number of cross validation folds. Given the significant variation in--------the performances of all the methods, it seems the differences between--------the better-performing methods are probably not statistically--------significant. More comprehensive empirical validation could clearly--------strengthen the paper.------------------------Clarity:----------------The writing is generally good both in terms of the biology and ML, but--------more mathematical rigour would make it easier to understand precisely--------what was done. The different architectures are explained on an--------intuitive level and might benefit from a clear mathematical--------definition. I was ultimately left unsure of what the ""raw end2end""--------model is - given so few parameters it cannot work on raw 300k--------dimensional input but I could not figure out what kind of embedding--------was used.----------------The results in Fig. 3 might be clearer if scaled so that maximum for--------each class is 1 to avoid confounding from different numbers of--------subjects in different classes. In the text, please use the standard--------italics math font for all symbols such as N, N_d, ...------------------------Originality:----------------The application and the approach appear quite novel.------------------------Significance:----------------There is clearly strong interest for deep learning in the genomics--------area and the paper seeks to address some of the major bottlenecks--------here. It is too early to tell whether the specific techniques proposed--------in the paper will be the ultimate solution, but at the very least the--------paper provides interesting new ideas for others to work on.------------------------Other comments:----------------I think releasing the code as promised would be a must.', 'The paper addresses the important problem (d>>n) in deep learning. --------The proposed approach, based on lower-dimensional feature embeddings, is reasonable and makes applying deep learning methods to data with large d possible.--------The paper is well written and the results show improvements over reasonable baselines.', ""The problem addressed here is practically important (supervised learning with n<<d), and as far as I know, the approach is novel. I thought their proposed solution was innovative, and I enjoyed the paper. The presentation is clear and it has nice experiments. ----------------Comments/questions:--------1) What is the dimensionality of the feature embeddings? --------2) SNPtoVec still requires training a very fat autoencoder network on X --- I suppose this doesn't contribute to the size of the final run-time model and overfitting is avoided because the parameters are fit in an unsupervised manner?""]",0.72509765625,0.274658203125,1,0.4853515625,0.5146484375,0,0.1737060546875,0.826171875,0,0.89306640625,0.10699462890625,1,0.48291015625,0.51708984375,0,0.50048828125,0.49951171875,1
58,58,58,58,58,58,https://openreview.net/forum?id=Sk2Im59ex,"---+This paper presents careful ablation studies to analyze the effects of different components of the system, which is helpful for understanding the paper..----In addition to the face identities, it is also of great interest to analyze how well the facial attributes are preserved when mapping to target domain..---+The transferred images are visually impressive and quantitative results also show the image identities are preserved across domains to some degree.

This paper presents an unsupervised image transformation method that maps a sample from source domain to target domain..To avoid trivial solutions, this paper proposed two additional losses that penalize 1) the feature difference between a source sample and its transformed sample and 2) the pixel difference between a target sample and its re-generated sample..It feels like the proposed approach would break for more dissimilar domains.","['This paper presents an unsupervised image transformation method that maps a sample from source domain to target domain. The major contribution lies in that it does not require aligned training pairs from two domains. The model is based on GANs. To make it work in the unsupervised setting, this paper decomposes the generation function into two modules: an encoder that identify a common feature space between two domains and an decoder that generates samples in the target domain. To avoid trivial solutions, this paper proposed two additional losses that penalize 1) the feature difference between a source sample and its transformed sample and 2) the pixel difference between a target sample and its re-generated sample. This paper presents extensive experiments on transferring SVHN digit images to MNIST style and transferring face images to emoji style. --------+The proposed learning method enables unsupervised domain transfer that could be impactful in broad problem contexts. --------+This paper presents careful ablation studies to analyze the effects of different components of the system, which is helpful for understanding the paper. --------+The transferred images are visually impressive and quantitative results also show the image identities are preserved across domains to some degree. ---------It will be more interesting to show results in other domains such as texts and images. ---------In addition to the face identities, it is also of great interest to analyze how well the facial attributes are preserved when mapping to target domain. ', 'Update: thank you for running more experiments, and add more explanations in the manuscript. They addressed most of my concerns, so I updated the score accordingly. ------------------------The work aims at learning a generative function G that can maps input from source domain to the target domain, such that a given representation function f remain unchanged accepting inputs from either domain. The criteria is termed f-constancy. The proposed method is evaluated on two visual domain adaptation tasks. ----------------The paper is relatively easy to follow, and the authors provided quite extensive experimental results on the two datasets. f-constancy is the main novelty of the work. ----------------It seems counter-intuitive to force the function G to be of g o f, i.e., starting from a restricted function f which might have already lost information. As in the face dataset, f is learned to optimize the performance of certain task on some external dataset. It is not clear if an input from the source or target domain can be recovered from applying G as in equation (5) and (6). Also, the f function is learned with a particular task in mind. As in the two experiments, the representation function f is learned to identify the digits in the source SVHN dataset or the identity of some face dataset.  As a result, the procedure has to be repeated if we were to perform domain adaptation for the same domains but for different tasks, such as recognizing expressions instead of identity. ----------------Do the authors have insight on why the baseline method proposed in equation (1) and (2) perform so poorly?----------------Figure 5 shows some visual comparison between style transfer and the proposed method. It is not clear though which method is better. Will it be possible to apply style transfer to generate emojis from photos and repeat the experiments shown in table 4?', ""Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before.----------------This paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result.----------------Pros:--------1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f.----------------2. The proposed method produces visually appealing results on several datasets----------------3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task----------------4. The paper is well-written and easy to read----------------Cons:--------1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution)----------------2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain  2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f.----------------3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches.----------------I would also like to point out that using super-resolved outputs as opposed to the actual model’s outputs can produce a false impression of the visual quality of the transferred samples. I’d suggest moving original outputs from the appendix into the main part.""]",0.9189453125,0.0809326171875,1,0.908203125,0.09197998046875,1,0.279052734375,0.72119140625,0,0.86767578125,0.1324462890625,1,0.08673095703125,0.9130859375,0,0.374267578125,0.62548828125,0
59,59,59,59,59,59,https://openreview.net/forum?id=Sk2iistgg,"-The proposed alternating optimization approach in the current form is computationally intensive and seems hard to scale to even moderate sized data -- in every iteration one needs to compute the kernel matrix over S and perform full SVD over the kernel matrix (Algo 2)..-- The paper says that Geiger et al defined non linearities on a latent space of pre-defined dimensionality..-- The third paragraph of section 2 claims that this paper presents a novel energy minimization framework to solve problems of the general form of eq.

The paper proposes a nonlinear regularizer for solving ill-posed inverse problems..The latent variables (or causal factors) corresponding to the observed data are assumed to lie near a low dimensional subspace in an RKHS induced by a predetermined kernel..The proposed regularizer can be seen as an extension of the linear low-rank assumption on the latent factors.","['The paper proposes a nonlinear regularizer for solving ill-posed inverse problems. The latent variables (or causal factors) corresponding to the observed data are assumed to lie near a low dimensional subspace in an RKHS induced by a predetermined kernel. The proposed regularizer can be seen as an extension of the linear low-rank assumption on the latent factors. A nuclear norm penalty on the Cholesky factor of the kernel matrix is used as a relaxation for the dimensionality of the subspace. Empirical results are reported on two tasks involving linear inverse problems -- missing feature imputation, and estimating non-rigid 3D structures from a sequence of 2D orthographic projections -- and the proposed method is shown to outperform linear low-rank regularizer. ----------------The clarity of the paper has scope for improvement (particularly, Introduction) - the back and forth b/w dimensionality reduction techniques and inverse problems is confusing at times. Clearly defining the ill-posed inverse problem first and then motivating the need for a regularizer (which brings dimensionality reduction techniques into the picture) may be a more clear flow in my opinion. ----------------The motivation behind relaxation of rank() in Eq 1 to nuclear-norm in Eq 2 is not clear to me in this setting. The relaxation does not yield a convex problem over S,C (Eq 5) and also increases the computations (Algo 2 needs to do full SVD of K(S) every time). The authors should discuss pros/cons over the alternate approach that fixes the rank of C (which can be selected using cross-validation, in the same way as  is selected), leaving just the first two terms in Eq 5. For this simpler objective, an interesting question to ask would be -- are there kernel functions for which it can solved in a scalable manner? ----------------The proposed alternating optimization approach in the current form is computationally intensive and seems hard to scale to even moderate sized data -- in every iteration one needs to compute the kernel matrix over S and perform full SVD over the kernel matrix (Algo 2). Empirical evaluations are also not extensive -- (i) the dataset used for feature imputation is old and non-standard, (ii) for structure estimation from motion on CMU dataset, the paper only compares with linear low-rank regularization, (iii) there is no comment/study on the convergence of the alternating procedure (Algo 1). ', 'This paper considers an alternate formulation of Kernel PCA with rank constraints incorporated as a regularization term in the objective. The writing is not clear. The focus keeps shifting from estimating “causal factors”, to nonlinear dimensionality reduction to Kernel PCA to ill-posed inverse problems. The problem reformulation of Kernel PCA uses somewhat standard tricks and it is not clear what are the advantages of the proposed approach over the existing methods as there is no theoretical analysis of the overall approach or empirical comparison with existing state-of-the-art.  ----------------- Not sure what the authors mean by “causal factors”. There is a reference to it in Abstract and in Problem formulation on page 3 without any definition/discussion.----------------- In KPCA, I am not sure why one is interested in step (iii) outlined on page 2 of finding a pre-image for each----------------- Authors outline two key disadvantages of the existing KPCA approach. The first one, that of low-dimensional manifold assumption not holding exactly, has received lots of attention in the machine learning literature. It is common to assume that the data lies near a low-dimensional manifold rather than on a low-dimensional manifold. Second disadvantage is somewhat unclear as finding “a data point (pre-image) corresponding to each projection in the input space” is not a standard step in KPCA. ----------------- On page 3, you never define , , . Clearly, they cannot be cartesian products. I have to assume that notation somehow implies N-tuples. ----------------- On page 3, Section 2,  and  are sets. What do you mean by ----------------- On page 5,  is never defined. ----------------- Experiments: None of the standard algorithms for matrix completion such as OptSpace or SVT were considered ----------------- Experiments: There is no comparison with alternate existing approaches for Non-rigid structure from motion.  ----------------- Proof of the main result Theorem 3.1: To get from (16) to (17) using the Holder inequality (as stated) one would end up with a term that involves sum of fourth powers of weights w_{ij}. Why would they equal to one using the orthonormal constraints? It would be useful to give more details here, as I don’t see how the argument goes through at this point. ', 'This paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The authors proposed an iterative minimization approach in order to obtain a local optimum of a relaxed problem. --------The paper contains errors and the experimental evaluation is not convincing. Only old techniques are compared against in very toy datasets. ----------------The authors claim state-of-the-art, however, the oil dataset is not a real benchmark, and the comparisons are to very old approaches. --------The experimental evaluation should demonstrate robustness to more complex noise and outliers, as this was one of the motivations in the introduction.----------------The authors do not address the out-of-sample problem. This is a problem of kernel-based methods vs LVMs, and thus should be address here.------------------------The paper contains errors:----------------- The last paragraph of section 1 says that this paper proposes a closed form solution to robust KPCA. This is simply wrong, as the proposed approach consists of iteratively solving iterativey a set of closed form updates  and Levenberg-Marquard optimizationd. This is not any more closed form!----------------- In the same paragraph (and later in the text) the authors claim that the proposed approach can be trivially generalized to incorporate other cost functions. This is not true, as in general there will be no more inner loop closed form updates and the authors will need to solve a much more complex optimization problem. ----------------- The third paragraph of section 2 claims that this paper presents a novel energy minimization framework to solve problems of the general form of eq. (2). However, this is not what the authors solve at the end. They solve a different problem that has been subject to at least two relaxations. It is not clear how solving for a local optima of this double relaxed problem is related to the original problem they want to solve. ----------------- The paper says that Geiger et al defined non linearities on a latent space of pre-defined dimensionality. This is wrong. This paper discovers the dimensionality of the latent space by means of a regularizer that encourages the singular values to be sparse. Thus, it does not have a fixed dimensionality, the latent space is just bounded to be smaller or equal than the dimensionality of the original space. ------------------------It is not clear to me why the author say for LVMs such as GPLVM that ""the latent space is learned a priority with clean training data"". One can use different noise models within the GP framework. Furthermore, the proposed approach assumes Gaussian noise (see eq. 6), which is also the trivial case for GP-based LVMs.  ------------------------It is not clear what the authors mean in the paper by ""pre-training"" or saying that techniques do not have a training phase. KPCA is trained via a closed-form update, but there is still training.  ']",0.401611328125,0.59814453125,0,0.7578125,0.2421875,1,0.25439453125,0.74560546875,0,0.85595703125,0.144287109375,1,0.0965576171875,0.9033203125,0,0.283203125,0.716796875,0
60,60,60,60,60,60,https://openreview.net/forum?id=SkCILwqex,"--1/19/17 UPDATE AFTER REBUTTAL:
---Given that experiments were added to the latest version of the paper, I'm increasing my review from 5 -> 6..-I would have liked this work to include more quantitative results (e.g., extract adversarial examples at different layers, add them to the training set, train networks, compare on test set), in addition to the visualizations present..--EDIT after rebuttal: thanks to the authors for addressing the experimental validation concerns.

-In the latter case, LOTS is used to generate adversarial examples, moving from an origin image just far enough toward a target image to cause the classification to flip..And does operating directly in softmax space result in smaller perturbations than IP2?).While it’s an interesting computation to perform, the value of the visualizations is not very clear.","[""This paper proposes the Layerwise Origin Target Synthesis (LOTS) method, which entails computing a difference in representation at a given layer in a neural network and then projecting that difference back to input space using backprop. Two types of differences are explored: linear scalings of a single input’s representation and difference vectors between representations of two inputs, where the inputs are of different classes.----------------In the former case, the LOTS method is used as a visualization of the representation of a specific input example, showing what it would mean, in input space, for the feature representation to be supressed or magnified. While it’s an interesting computation to perform, the value of the visualizations is not very clear.----------------In the latter case, LOTS is used to generate adversarial examples, moving from an origin image just far enough toward a target image to cause the classification to flip. As expected, the changes required are smaller when LOTS targets a higher layer (in the limit of targetting the last layer, results similar to the original adversarial image results would be obtained).----------------The paper is an interesting basic exploration and would probably be a great workshop paper. However, the results are probably not quite compelling enough to warrant a full ICLR paper.----------------A few suggestions for improvement:-------- - Several times it is claimed that LOTS can be used as a method for mining for diverse adversarial examples that could be used in training classifiers more robust to adversarial perturbation. But this simple experiment of training on LOTS generated examples isn’t tried. Showing whether the LOTS method outperforms, say, FGS would go a long way toward making a strong paper.-------- - How many layers are in the networks used in the paper, and what is their internal structure? This isn’t stated anywhere. I was left wondering whether, say, in Fig 2 the CONV2_1 layer was immediately after the CONV1_1 layer and whether the FC8 layer was the last layer in the network.-------- - In Fig 1, 2, 3, and 4, results of the application of LOTS are shown for many intermediate layers but miss for some reason applying it to the input (data) layer and the output/classification (softmax) layer. Showing the full range of possible results would reinforce the interpreatation (for example, in Fig 3, are even larger perturbations necessary in pixel space vs CONV1 space? And does operating directly in softmax space result in smaller perturbations than IP2?)-------- - The PASS score is mentioned a couple times but never explained at all. E.g. Fig 1 makes use of it but does not specify such basics as whether higher or lower PASS scores are associated with more or less severe perturbations. A basic explanation would be great.-------- - 4.2 states “In summary, the visualized internal feature representations of the origin suggest that lower convolutional layers of the VGG Face model have managed to learn and capture features that provide semantically meaningful and interpretable representations to human observers.” I don’t see that this follows from any results. If this is an important claim to the paper, it should be backed up by additional arguments or results.--------------------------------1/19/17 UPDATE AFTER REBUTTAL:--------Given that experiments were added to the latest version of the paper, I'm increasing my review from 5 -> 6. I think the paper is now just on the accept side of the threshold."", ""The paper presents a new exciting layerwise origin-target synthesis method both for generating a large number of diverse adversarials as well as for understanding the robustness of various layers. The methodology is then used to visualize the amount of perturbation necessary for producing a change for higher level features.----------------The approach to match the features of another unrelated image is interesting and it goes beyond producing adversarials for classification. It can also generate adversarials for face-recognition and other models where the result is matched with some instance from a database.----------------Pro: The presented approach is definitely sound, interesting and original. --------Con: The analyses presented in this paper are relatively shallow and don't touch the most obvious questions. There is not much experimental quantitative evidence for the efficacy of this method compared with other approaches to produce adversarials. The visualization is not very exciting and it is hard to any draw any meaningful conclusions from them.----------------It would definitely improve the paper if it would present some interesting conclusions based on the new ideas."", 'This paper presents a relatively novel way to visualize the features / hidden units of a neural network and generate adversarial examples. The idea is to do gradient descent in the pixel space, from a given hidden unit in any layer. This can either be done by choosing a pair of images and using the difference in activations of the unit as the thing to do gradient descent over or just the activation itself of the unit for a given image. In general this method seems intriguing, here are some comments:----------------It’s not clear that some of the statements at the beginning of Sec 4.1 are actually true, re: positive/negative signs and how that changes (or does not change) the class. Mathematically, I don’t see why that would be the case? Moreover the contradictory evidence from MNIST vs. faces supports my intuition.----------------The authors use the PASS score through the paper, but only given an intuition + citation for it. I think it’s worth explaining what it actually does, in a sentence or two.----------------The PASS score seems to have some, but not complete, correlation with L_2, L_\\{infty} or visual estimation of how “good” the adversarial examples are. I am not sure what the take-home message from all these numbers is.----------------“In general, LOTS cannot produce high quality adversarial examples at the lower layers” (sec 5.2) seems false for MNIST, no?----------------I would have liked this work to include more quantitative results (e.g., extract adversarial examples at different layers, add them to the training set, train networks, compare on test set), in addition to the visualizations present. That to me is the main drawback of the paper, in addition to basically no comparisons with other methods (it’s hard to judge the merits of this work in vacuum).-------------------------------------EDIT after rebuttal: thanks to the authors for addressing the experimental validation concerns. I think this makes the paper more interesting, so revising my score accordingly. ']",0.705078125,0.294921875,1,0.92724609375,0.07275390625,1,0.1947021484375,0.80517578125,0,0.88671875,0.11334228515625,1,0.1715087890625,0.82861328125,0,0.441162109375,0.55859375,0
61,61,61,61,61,61,https://openreview.net/forum?id=SkYbF1slg,"My comments are as follows: 


-(1) First of all, this paper is 21 pages without appendix, and too long as a conference proceeding..(2.12), H(X) should disappear?.Therefore, it is not easy for readers to follow the paper.

This paper presents an information theoretic framework for unsupervised learning..The framework relies on infomax principle, whose goal is to maximize the mutual information between input and output..The authors propose a two-step algorithm for learning in this setting.","['This paper presents an information theoretic framework for unsupervised learning. The framework relies on infomax principle, whose goal is to maximize the mutual information between input and output. The authors propose a two-step algorithm for learning in this setting. First, by leveraging an asymptotic approximation to the mutual information, the global objective is decoupled into two subgoals whose solutions can be expressed in closed form. Next, these serve as the initial guess for the global solution, and are refined by the gradient descent algorithm.----------------While the story of the paper and the derivations seem sound, the clarity and presentation of the material could improve. For example, instead of listing step by step derivation of each equation, it would be nice to first give a high-level presentation of the result and maybe explain briefly the derivation strategy. The very detailed aspects of derivations, which could obscure the underlying message of the result could perhaps be postponed to later sections or even moved to an appendix.----------------A few questions that the authors may want to clarify:--------1. Page 4, last paragraph: ""from above we know that maximizing I(X;R) will result in maximizing I(Y;R) and I(X,Y^U)"". While I see the former holds due to equality in 2.20, the latter is related via a bound in 2.21. Due to the possible gap between I(X;R) and I(X,Y^U), can your claim that maximizing of the former indeed maximizes the latter be true?--------2. Paragraph above section 2.2.2: it is stated that, dropout used to prevent overfitting may in fact be regarded as an attempt to reduce the rank of the weight matrix. No further tip is provided why this should be the case. Could you elaborate on that?--------3. At the end of page 9: ""we will discuss how to get optimal solution of C for two specific cases"". If I understand correctly, you actually are not guaranteed to get the optimal solution of C in either case, and the best you can guarantee is reaching a local optimum. This is due to the nonconvexity of the constraint 2.80 (quadratic equality). If optimality cannot be guaranteed, please correct the wording accordingly.', 'This paper proposes a hierarchical infomax method. My comments are as follows: ----------------(1) First of all, this paper is 21 pages without appendix, and too long as a conference proceeding. Therefore, it is not easy for readers to follow the paper. The authors should make this paper as compact as possible while maintaining the important message. ----------------(2) One of the main contribution in this paper is to find a good initialization point by maximizing I(X;R). However, it is unclear why maximizing I(X;\\breve{Y}) is good for maximizing I(X;R) because Proposition 2.1 shows that I(X;\\breve{Y}) is an “upper” bound of I(X;R) (When it is difficult to directly maximize a function, people often maximize some tractable “lower” bound of it).----------------Minor comments:--------(1) If (2.11) is approximation of (2.8), “\\approx” should be used. ----------------(2) Why K_1 instead of N in Eq.(2.11)?----------------(3) In Eq.(2.12), H(X) should disappear?----------------(4) Can you divide Section 3 into subsections?', ""This is an 18 page paper plus appendix which presents a mathematical derivation for infomax for an actual neural population with noise.  The original Bell & Sejnowski infomax framework only considered the no noise case.  Results are shown for natural image patches and the mnist dataset, which qualitatively resemble results obtained with other methods.----------------This seems like an interesting and potentially more general approach to unsupervised learning.  However the paper is quite long and it was difficult for me to follow all the twists and turns.  For example the introduction of the hierarchical model was confusing and it took several iterations to understand where this was going.  'Hierarchical' is probably not the right terminology here because it's not like a deep net hierarchy, it's just decomposing the tuning curve function into different parts.  I would recommend that the authors try to condense the paper so that the central message and important steps are conveyed in short order, and then put the more complete mathematical development into a supplementary document.----------------Also, the authors should look at the work of Karklin & Simoncelli 2011 which is highly related.  They also use an infomax framework for a noisy neural population to derive on and off cells in the retina, and they show the conditions under which orientation selectivity emerges.""]",0.7197265625,0.2802734375,1,0.7783203125,0.2216796875,1,0.15087890625,0.84912109375,0,0.875,0.12481689453125,1,0.08428955078125,0.91552734375,0,0.3408203125,0.6591796875,0
62,62,62,62,62,62,https://openreview.net/forum?id=SkgSXUKxx,"The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net..---There seem to be two stories in the paper: feature penalty as a soft batch norm version, and low-shot learning; why is feature penalty specifically adapted to low-shot learning and not a more classical supervised task?.It makes various (interesting) claims about a number of things, but they seem more or less disparate, and only loosely related to low-shot learning.

---- ""Table 13.2"" should be ""Table 2""..---Experiments are given on ""low-shot"" settting..-Experiments show small improvements on a synthetic XOR test set.","['Summary--------===--------This paper extends and analyzes the gradient regularizer of Hariharan and--------Girshick 2016. In that paper a regularizer was proposed which penalizes--------gradient magnitudes and it was shown to aid low-shot learning performance.--------This work shows that the previous regularizer is equivalent to a direct penalty--------on the magnitude of feature values weighted differently per example.----------------The analysis goes to to provide two examples where a feature penalty--------favors a better representation. The first example addresses the XOR--------problem, constructing a network where a feature penalty encourages--------a representation where XOR is linearly separable.--------The second example analyzes a 2 layer linear network, showing improved stability--------of a 2nd order optimizer when the feature penalty is added.--------One last bit of analysis shows how this regularizer can be interpreted as--------a Gaussian prior on both features and weights. Since the prior can be--------interpreted as having a soft whitening effect, the feature regularizer--------is like a soft version of Batch Normalization.----------------Experiments show small improvements on a synthetic XOR test set.--------On the Omniglot dataset feature regularization is better than most baselines,--------but is worse than Moment Matching Networks. An experiment on ImageNet similar--------to Hariharan and Girshick 2016 also shows effective low-shot learning.------------------------Strengths--------===----------------* The core proposal is a simple modification of Hariharan and Girshick 2016.----------------* The idea of feature regularization is analyzed from multiple angles--------both theoretically and empirically.----------------* The connection with Batch Normalization could have broader impact.------------------------Weaknesses--------===----------------* In section 2 the gradient regularizer of Hariharan and Girshick is introduced.--------While introducing the concept, some concern is expressed about the motivation:--------""And it is not very clear why small gradients on every sample produces--------good generalization experimentally."" This seems to be the central issue to me.--------The paper details some related analysis, it does not offer a clear answer to--------this problem.------------------------* The purpose and generality of section 2.1 is not clear.----------------The analysis provides a specific case (XOR with a non-standard architecture)--------where feature regularization intuitively helps learn a better representation.--------However, the intended take-away is not clear.----------------The take-away may be that since a feature penalty helps in this case it--------should help in other cases. I am hesitant to buy that argument because of the--------specific architecture used in this section. The result seems to rely on the--------choice of an x^2 non-linearity, which is not often encountered in recent neural--------net literature.----------------The point might also be to highlight the difference between a weight--------penalty and a feature penalty because the two seem to encourage--------different values of b in this case. However, there is no comparison to--------a weight penalty on b in section 2.1.------------------------* As far as I can tell, eq. 3 depends on either assuming an L2 or cross-entropy--------loss. A more general class of losses for which eq. 3 holds is not provided. This--------should be made clear before eq. 3 is presented.------------------------* The Omniglot and ImageNet experiments are performed with Batch Normalization,--------yet the paper points out that feature regularization may be similar in effect--------to Batch Norm. Since the ResNet CNN baseline includes Batch Norm and there are--------clear improvements over that baseline, the proposed regularizer has a clear--------additional positive effect. However, results should be provided without--------Batch Norm so a 1-1 comparison between the two methods can be performed.------------------------* The ImageNet experiment should be more like Hariharan and Girshick.--------In particular, the same split of classes should be used (provided in--------the appendix) and performance should be measured using n > 1 novel examples--------per class (using k nearest neighbors).------------------------Minor:----------------* A brief comparison to Matching Networks is provided in section 3.2, but the--------performance of Matching Networks should also be reported in Table 1.----------------* From the approach section: ""Intuitively when close to convergence, about half--------of the data-cases recommend to update a parameter to go left, while--------the other half recommend to go right.""----------------Could the intuition be clarified? There are many directions in high--------dimensional space and many ways to divide them into two groups.----------------* Is the SGM penalty of Hariharan and Girshick implemented for this paper--------or using their code? Either is acceptable, but clarification would be appreciated.----------------* Should the first equal sign in eq. 13 be proportional to, not equal to?----------------* The work is dense in nature, but I think the presentation could be improved.--------In particular, more detailed derivations could be provided in an appendix--------and some details could be removed from the main version in order to increase--------focus on the results (e.g., the derviation in section 2.2.1).------------------------Overall Evaluation--------===----------------This paper provides an interesting set of analyses, but their value is not clear.--------There is no clear reason why a gradient or feature regularizer should improve--------low-shot learning performance. Despite that, experiments support that conclusion,--------the analysis is interesting by itself, and the analysis may help lead to a--------clearer explanation.----------------The work is a somewhat novel extension and analysis of Hariharan and Girshick 2016.--------Some points are not completely clear, as mentioned above.', 'This paper proposes analysis of regularization, weight Froebius-norm and feature L2 norm, showing that it is equivalent to another proposed regularization, gradient magnitude loss. They then argue that: 1) it is helpful to low-shot learning, 2) it is numerically stable, 3) it is a soft version of Batch Normalization. Finally, they demonstrate experimentally that such a regularization improves performance on low-shot tasks.----------------First, this is a nice analysis of some simple models, and proposes interesting insights in some optimization issues. Unfortunately, the authors do not demonstrate, nor argue in a convincing manner, that such an analysis extends to deep non-linear computation structures. I feel like the authors could write a full paper about ""results can be derived for φ(x) with convex differentiable non-linear activation functions such as ReLU"", both via analysis and experimentation to measure numerical stability.----------------Second, the authors again show an interesting correspondance to batch normalization, but IMO fail to experimentally show its relevance.----------------Finally, I understand the appeal of the proposed method from a numerical stability point of view, but am not convinced that it has any effect on low-shot learning in the high dimensional spaces that deep networks are used for.----------------I commend the authors for contributing to the mathematical understanding of our field, but I think they have yet to demonstrate the large scale effectiveness of what they propose. At the same time, I feel like this paper does not have a clear and strong message. It makes various (interesting) claims about a number of things, but they seem more or less disparate, and only loosely related to low-shot learning.----------------notes:--------- ""an expectation taken with respect to the empirical distribution generated by the training set"", generally the training set is viewed as a ""montecarlo"" sample of the underlying, unknown data distribution \\mathcal{D}.--------- ""we can see that our model learns meaningful representations"", it gets a 6.5% improvement on the baseline, but there is no analysis of the meaningfulness of the representations.--------- ""Table 13.2"" should be ""Table 2"".--------- please be mindful of formatting, some citations should be parenthesized and there are numerous extraneous and missing spacings between words and sentences.', 'The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net.--------Although the equations suggest a weighting per example, dropping this weight (alpha_i) works equally well.--------The proposed approach relates to Batch Norm and weight decay.--------Experiments are given on ""low-shot"" settting.--------There seem to be two stories in the paper: feature penalty as a soft batch norm version, and low-shot learning; why is feature penalty specifically adapted to low-shot learning and not a more classical supervised task?--------Regarding your result on Omniglot, 91.5, I believe it is still about 2% worse than the Matching Networks, which you refer to but don\'t put in Table 1. Why?--------Overall, the idea is simple but feels like preliminary: while it is supposed to be a ""soft BN"", BN itself gets better performance than feature penalty, and both together give even better results. Is something still missing in the explanation?------------------ edits after revised version:----------------Thank you for adding more information to the paper. I feel it is still too long but hopefully you can reduce it to 9 pages as promised. However, I\'m still not convinced the paper is ready to be accepted, mainly for the following reasons:--------- on Omniglot, the paper is still significantly far from the current state of the art.--------- the new experiments do not really confirm/infirm the relationship with BN.--------- you added an explanation of why FP works for low-shot setting, by showing it controls the VC dimension and hence is good to control overfitting with a small number of training examples, but this discussion is basic and does not really shed more light than the obvious.--------I\'m pushing up your score from 4 to 5 for the improved version, but I still think it is below acceptance level.']",0.77685546875,0.2230224609375,1,0.8369140625,0.1632080078125,1,0.264404296875,0.73583984375,0,0.857421875,0.1427001953125,1,0.0711669921875,0.9287109375,0,0.26953125,0.73046875,0
63,63,63,63,63,63,https://openreview.net/forum?id=SkkTMpjex,"The authors adapt/modify the K-FAC method in order to make it computationally tractable for optimizing deep neural networks..-Typos:
---- “updates that accounts for” — “updates that account for”
---- “Kronecker product of their inverse” — “Kronecker product of their inverses”
---- “where P is distribution over” — “where P is the distribution over”
---- “back-propagated loss derivativesas” — “back-propagated loss derivatives as”
---- “inverse of the Fisher” — “inverse of the Fisher Information matrix”
---- “which amounts of several matrix” — “which amounts to several matrix”
---- “The diagram illustrate the distributed” — “The diagram illustrates the distributed”
---- “Gradient workers computes” — “Gradient workers compute” 
---- “Stat workers computes” — “Stat workers compute” 
---- “occasionally and uses stale values” — “occasionally and using stale values” 
---- “The factors of rank-1 approximations” — “The factors of the rank-1 approximations”
---- “be the first singular value and its left and right singular vectors” — “be the first singular value and the left and right singular vectors … , respectively.”
---- “\Psi is captures” — “\Psi captures”
---- “multiplying the inverses of the each smaller matrices” — “multiplying the inverses of each of the smaller matrices”
---- “which is a nested applications of the reshape” — “which is a nested application of the reshape”
---- “provides a computational feasible alternative” — “provides a computationally feasible alternative”
---- “according the geometric mean” — “according to the geometric mean”
---- “analogous to shrink” — “analogous to shrinking”
---- “applied to existing model-specification code” — “applied to the existing model-specification code”
---- “: that the alternative parametrization” — “: the alternative parameterization”


-Minor Issues:
---- In paragraph 2 (Introduction) the authors mention several methods that approximate the curvature matrix..---3) Empirically illustrate the performance of the method, and show:
---- Asynchronous Fisher Block inversions do not adversely affect the performance of the method (CIFAR-10)
---- K-FAC is faster than Synchronous SGD (with and without BN, and with momentum) (ImageNet)
---- Doubly-factored K-FAC method does not deteriorate the performance of the method (ImageNet and ResNet)
---- Favorable scaling properties of K-FAC with mini-batch size


-Pros:
---- Paper presents interesting ideas on how to make computationally demanding aspects of K-FAC tractable.

The authors illustrate the performance of the method on the CIFAR-10 and ImageNet datasets using several models and compare with synchronous SGD..The method distributes the computation of the gradients and the other quantities required by the K-FAC method (2nd order statistics and Fisher Block inversion)..---- Experiments are well thought out and highlight the key advantages of the method over Synchronous SGD (with and without BN).","['In this paper, the authors present a partially asynchronous variant of the K-FAC method. The authors adapt/modify the K-FAC method in order to make it computationally tractable for optimizing deep neural networks. The method distributes the computation of the gradients and the other quantities required by the K-FAC method (2nd order statistics and Fisher Block inversion). The gradients are computed in synchronous manner by the ‘gradient workers’ and the quantities required by the K-FAC method are computed asynchronously by the ‘stats workers’ and ‘additional workers’. The method can be viewed as an augmented distributed Synchronous SGD method with additional computational nodes that update the approximate Fisher matrix and computes its inverse. The authors illustrate the performance of the method on the CIFAR-10 and ImageNet datasets using several models and compare with synchronous SGD.----------------The main contributions of the paper are:--------1) Distributed variant of K-FAC that is efficient for optimizing deep neural networks. The authors mitigate the computational bottlenecks of the method (second order statistic computation and Fisher Block inverses) by asynchronous updating.--------2) The authors propose a “doubly-factored” Kronecker approximation for layers whose inputs are too large to be handled by the standard Kronecker-factored approximation. They also present (Appendix A) a cheaper Kronecker factored approximation for convolutional layers.--------3) Empirically illustrate the performance of the method, and show:--------- Asynchronous Fisher Block inversions do not adversely affect the performance of the method (CIFAR-10)--------- K-FAC is faster than Synchronous SGD (with and without BN, and with momentum) (ImageNet)--------- Doubly-factored K-FAC method does not deteriorate the performance of the method (ImageNet and ResNet)--------- Favorable scaling properties of K-FAC with mini-batch size----------------Pros:--------- Paper presents interesting ideas on how to make computationally demanding aspects of K-FAC tractable. --------- Experiments are well thought out and highlight the key advantages of the method over Synchronous SGD (with and without BN).----------------Cons: --------- “…it should be possible to scale our implementation to a larger distributed system with hundreds of workers.” The authors mention that this should be possible, but fail to mention the potential issues with respect to communication, load balancing and node (worker) failure. That being said, as a proof-of-concept, the method seems to perform well and this is a good starting point.--------- Mini-batch size scaling experiments: the authors do not provide validation curves, which may be interesting for such an experiment. Keskar et. al. 2016 (On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima) provide empirical evidence that large-batch methods do not generalize as well as small batch methods. As a result, even if the method has favorable scaling properties (in terms of mini-batch sizes), this may not be effective.----------------The paper is clearly written and easy to read, and the authors do a good job of communicating the motivation and main ideas of the method. There are a few minor typos and grammatical errors. ----------------Typos:--------- “updates that accounts for” — “updates that account for”--------- “Kronecker product of their inverse” — “Kronecker product of their inverses”--------- “where P is distribution over” — “where P is the distribution over”--------- “back-propagated loss derivativesas” — “back-propagated loss derivatives as”--------- “inverse of the Fisher” — “inverse of the Fisher Information matrix”--------- “which amounts of several matrix” — “which amounts to several matrix”--------- “The diagram illustrate the distributed” — “The diagram illustrates the distributed”--------- “Gradient workers computes” — “Gradient workers compute” --------- “Stat workers computes” — “Stat workers compute” --------- “occasionally and uses stale values” — “occasionally and using stale values” --------- “The factors of rank-1 approximations” — “The factors of the rank-1 approximations”--------- “be the first singular value and its left and right singular vectors” — “be the first singular value and the left and right singular vectors … , respectively.”--------- “\\Psi is captures” — “\\Psi captures”--------- “multiplying the inverses of the each smaller matrices” — “multiplying the inverses of each of the smaller matrices”--------- “which is a nested applications of the reshape” — “which is a nested application of the reshape”--------- “provides a computational feasible alternative” — “provides a computationally feasible alternative”--------- “according the geometric mean” — “according to the geometric mean”--------- “analogous to shrink” — “analogous to shrinking”--------- “applied to existing model-specification code” — “applied to the existing model-specification code”--------- “: that the alternative parametrization” — “: the alternative parameterization”----------------Minor Issues:--------- In paragraph 2 (Introduction) the authors mention several methods that approximate the curvature matrix. However, several methods that have been developed are not mentioned. For example:--------1) (AdaGrad) Adaptive Subgradient Methods for Online Learning and Stochastic Optimization (http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)--------2) Stochastic Quasi-Newton Methods for Nonconvex Stochastic Optimization (https://arxiv.org/abs/1607.01231)--------3) adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs (http://link.springer.com/chapter/10.1007/978-3-319-46128-1_1)--------4) A Self-Correcting Variable-Metric Algorithm for Stochastic Optimization (http://jmlr.org/proceedings/papers/v48/curtis16.html)--------5) L-SR1: A Second Order Optimization Method for Deep Learning (https://openreview.net/pdf?id=By1snw5gl)--------- Page 2, equation s = WA, is there a dimension issue in this expression?--------- x-axis for top plots in Figures 3,4,5,7 (Updates x XXX) appear to be a headings for the lower plots.--------- “James Martens. Deep Learning via Hessian-Free Optimization” appears twice in References section.', 'The paper proposes an asynchronous distributed K-FAC method for efficient optimization of --------deep networks. The authors introduce interesting ideas that many computationally demanding --------parts of the original K-FAC algorithm can be efficiently implemented in distributed fashion. The--------gradients and the second-order statistics are computed by distributed workers separately and --------aggregated at the parameter server along with the inversion of the approximate Fisher matrix --------computed by a separate CPU machine. The experiments are performed in CIFAR-10 and ImageNet--------classification problems using models such as AlexNet, ResNet, and GoogleReNet.----------------The paper includes many interesting ideas and techniques to derive an asynchronous distributed --------version from the original K-FAC. And the experiments also show good results on a few --------interesting cases. However, I think the empirical results are not thorough and convincing --------enough yet. Particularly, experiments on various and large number of GPU workers (in the same machine, --------or across multiple workers) are desired. For example, as pointed by the authors in the answer of a comment,--------Chen et.al. (Revisiting Distributed Synchronous SGD, 2015) used 100 workers to test their distributed deep --------learning algorithm. Even considering that the authors have a limitation in computing resource under the --------academic research setting, the maximum number of 4 or 8 GPUs seems too limited as the only test case of --------demonstrating the efficiency of a distributed learning algorithm.  ']",0.74072265625,0.259521484375,1,0.861328125,0.138671875,1,0.212890625,0.787109375,0,0.88134765625,0.11859130859375,1,0.326904296875,0.67333984375,0,0.427978515625,0.57177734375,0
64,64,64,64,64,64,https://openreview.net/forum?id=SkpSlKIel,"-- In Theorem 11, \bf x\in [0,1]^d?.-2- Is there an example of piecewise smooth function that requires at least poly(1/eps) hidden units with a shallow network?.-PROS 
---The paper presents an interesting combination of tools and arrives at a nice result on the exponential superiority of depth.

The main contribution of this paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network that uses O(log(1/eps)) layers and O(poly log(1/eps)) hidden units where the activation functions can be either ReLU or binary step or any combination of them..Extensions of the previous results to more general function classes, such as smooth or vector-valued functions..The technical content, including the proofs in the Appendix, look correct.","[""SUMMARY --------This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks with ReLU and threshold units. The main contribution of the paper is to show that approximating a strongly convex differentiable function is possible with much less units when using a network with one more hidden layer. ----------------PROS --------The paper presents an interesting combination of tools and arrives at a nice result on the exponential superiority of depth. ----------------CONS--------The main result appears to address only strongly convex univariate functions. ----------------SPECIFIC COMMENTS ----------------- Thanks for the comments on L. Still it would be a good idea to clarify this point as far as possible in the main part. Also, I would suggest to advertise the main result more prominently. --------I still have not read the revision and maybe you have already addressed some of these points there. ----------------- The problem statement is close to that from [Montufar, Pascanu, Cho, Bengio NIPS 2014], which specifically arrives at exponential gaps between deep and shallow ReLU networks, albeit from a different angle. I would suggest to include that paper it in the overview. ----------------- In Lemma 3, there is an i that should be x----------------- In Theorem 4, ``\\tilde f'' is missing the (x). ----------------- Theorem 11, the lower bound always increases with L ? ----------------- In Theorem 11, \\bf x\\in [0,1]^d? "", 'The main contribution of this paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network that uses O(log(1/eps)) layers and O(poly log(1/eps)) hidden units where the activation functions can be either ReLU or binary step or any combination of them. The paper is well written and clear. The arguments and proofs are easy to follow. I only have two questions:----------------1- It would be great to have similar results without binary step units. To what extent do you find the binary step unit central to the proof?----------------2- Is there an example of piecewise smooth function that requires at least poly(1/eps) hidden units with a shallow network?', 'This paper shows:----------------  1. Easy, constructive proofs to derive e-error upper-bounds on neural networks with O(log 1/e) layers and O(log 1/e) ReLU units.--------  2. Extensions of the previous results to more general function classes, such as smooth or vector-valued functions.--------  3. Lower bounds on the neural network size, as a function of its number of layers. The lower bound reveals the need of exponentially many more units to approximate functions using shallow architectures.----------------The paper is well written and easy to follow. The technical content, including the proofs in the Appendix, look correct. Although the proof techniques are simple (and are sometimes modifications of arguments by Gil, Telgarsky, or Dasgupta), they are brought together in a coherent manner to produce sharp results. Therefore, I am leaning toward acceptance.']",0.861328125,0.138427734375,1,0.8984375,0.1016845703125,1,0.278076171875,0.72216796875,0,0.876953125,0.12310791015625,1,0.352294921875,0.64794921875,0,0.431640625,0.568359375,0
65,65,65,65,65,65,https://openreview.net/forum?id=Skq89Scxx,"----Pros:


-- Simple and effective method to improve convergence
---- Good evaluation on well known database



----Cons:


-- Connection of introduction and topic of the paper is a bit unclear
---- Fig 2, 4 and 5 are hard to read..-I thank the authors for revising the paper based on my concerns..-Typos:
---- “flesh” -> “flush”

This heuristic to improve gradient descent in image classification is simple and effective, but this looks to me more like a workshop track paper..In particular, with architectures that combine a wide variety of layer types (embedding, RNN, CNN, gating), I found that ADAM-type techniques far outperform simple SGD with momentum, as they save searching for the right learning rate for each type of layer..But ADAM only works well combined with Poliak averaging, as it fluctuates a lot from one batch to another.","['This heuristic to improve gradient descent in image classification is simple and effective, but this looks to me more like a workshop track paper. Demonstration of the algorithm is limited to one task (CIFAR) and there is no theory to support it, so we do not know how it will generalize on other tasks----------------Working on DNNs for NLP, I find some observations in the paper opposite to my own experience. In particular, with architectures that combine a wide variety of layer types (embedding, RNN, CNN, gating), I found that ADAM-type techniques far outperform simple SGD with momentum, as they save searching for the right learning rate for each type of layer. But ADAM only works well combined with Poliak averaging, as it fluctuates a lot from one batch to another.----------------Revision:---------  the authors substantially improved the contents of the paper, including experiments on another set than Cifar---------  the workshop track has been modified to breakthrough work, so my recommendation for it is not longer appropriate--------I have therefore improved my rating', 'This an interesting investigation into learning rate schedules, bringing in the idea of restarts, often overlooked in deep learning. The paper does a thorough study on non-trivial datasets, and while the outcomes are not fully conclusive, the results are very good and the approach is novel enough to warrant publication. ----------------I thank the authors for revising the paper based on my concerns.----------------Typos:--------- “flesh” -> “flush”', ""This paper describes a way to speed up convergence through sudden increases of otherwise monotonically decreasing learning rates. Several techniques are presented in a clear way and parameterized method is proposed and evaluated on the CIFAR task. The concept is easy to understand and the authors chose state-of-the-art models to show the performance of their algorithm. The relevance of these results goes beyond image classification.------------------------Pros:----------------- Simple and effective method to improve convergence--------- Good evaluation on well known database------------------------Cons:----------------- Connection of introduction and topic of the paper is a bit unclear--------- Fig 2, 4 and 5 are hard to read. Lines are out of bounds and maybe only the best setting for T_0 and T_mult would be clearer. The baseline also doesn't seem to converge----------------Remarks:--------An loss surface for T_0 against T_mult would be very helpful. Also understanding the relationship of network depth and the performance of this method would add value to this analysis.""]",0.87158203125,0.1282958984375,1,0.9560546875,0.04388427734375,1,0.51611328125,0.48388671875,1,0.90869140625,0.0911865234375,1,0.13330078125,0.86669921875,0,0.415771484375,0.583984375,0
66,66,66,66,66,66,https://openreview.net/forum?id=Sks3zF9eg,"---This change goes against common sense and there would need to be strong evidence to show that it's a good idea in practice..-Cons: 
---Seems like more of a preliminary investigation of the potential benefits of sin, and more evidence (to support or in contrary) is needed to conclude anything significant -- the results on MNIST seem to indicate truncated sin is just as good, and while it is interesting that tanh maybe uses more of the saturated part, the two seem relatively interchangeable..-Pros:
---The closed form derivations of the loss surface were interesting to see, and the clarity of tone on the advantages *and* disadvantages was educational.

It's not clear that this activation function is good for a broad class of algorithmic tasks or just for the two they present..Maybe showing good results on more important tasks in addition to current toy tasks would make a stronger case?.Authors propose using periodic activation functions (sin) instead of tanh for gradient descent training of neural networks.","['Summary:--------In this paper, the authors explore the advantages/disadvantages of using a sin activation function.--------They first demonstrate that even with simple tasks, using sin activations can result in complex to optimize loss functions.--------They then compare networks trained with different activations on the MNIST dataset, and discover that the periodicity of the sin activation is not necessary for learning the task well.--------They then try different algorithmic tasks, where the periodicity of the functions is helpful.----------------Pros:--------The closed form derivations of the loss surface were interesting to see, and the clarity of tone on the advantages *and* disadvantages was educational.----------------Cons: --------Seems like more of a preliminary investigation of the potential benefits of sin, and more evidence (to support or in contrary) is needed to conclude anything significant -- the results on MNIST seem to indicate truncated sin is just as good, and while it is interesting that tanh maybe uses more of the saturated part, the two seem relatively interchangeable. The toy algorithmic tasks are hard to conclude something concrete from.', ""Authors propose using periodic activation functions (sin) instead of tanh for gradient descent training of neural networks.--------This change goes against common sense and there would need to be strong evidence to show that it's a good idea in practice. --------The experiments show slight improvement (98.0 -> 98.1) for some MNIST configurations. They show strong improvement (almost 100% higher accuracy after 1500 iterations) on a toy algorithmic task. It's not clear that this activation function is good for a broad class of algorithmic tasks or just for the two they present. Hence evidence shown is insufficient to be convincing that this is a good idea for practical tasks."", 'An interesting study of using Sine as activation function showing successful training of models using Sine. However the scope of tasks this is applied to is a bit too limited to be convincing. Maybe showing good results on more important tasks in addition to current toy tasks would make a stronger case?']",0.87646484375,0.12359619140625,1,0.892578125,0.107421875,1,0.31640625,0.68359375,0,0.91943359375,0.0804443359375,1,0.2210693359375,0.77880859375,0,0.410888671875,0.58935546875,0
67,67,67,67,67,67,https://openreview.net/forum?id=Skvgqgqxe,"The paper is clear and well written..-I like the idea of learning tree representations of text which are useful for a downstream task..-Minor comments:
---In the second paragraph of the introduction, one might argue that bag-of-words is also a predominant approach to represent sentences.

I have not much to add to my pre-review comments..---It's a very well written paper with an interesting idea..---Lots of people currently want to combine RL with NLP.","[""I have not much to add to my pre-review comments.--------It's a very well written paper with an interesting idea.--------Lots of people currently want to combine RL with NLP. It is very en vogue.--------Nobody has gotten that to work yet in any really groundbreaking or influential way that results in actually superior performance on any highly relevant or competitive NLP task.--------Most people struggle with the fact that NLP requires very efficient methods on very large datasets and RL is super slow.--------Hence, I believe this direction hasn't shown much promise yet and it's not yet clear it ever will due to the slowness of RL.--------But many directions need to be explored and maybe eventually they will reach a point where they become relevant.----------------It is interesting to learn the obviously inherent grammatical structure in language though sadly again, the trees here do not yet capture much of what our intuitions are.----------------Regardless, it's an interesting exploration, worthy of being discussed at the conference."", ""The paper proposes to use reinforcement learning to learn how to compose the words in a sentence, i.e. parse tree, that can be helpful for the downstream tasks. To do that, the shift-reduce framework is employed and RL is used to learn the policy of the two actions SHIFT and REDUCE. The experiments on four datasets (SST, SICK, IMDB, and SNLI) show that the proposed approach outperformed the approach using predefined tree structures (e.g. left-to-right, right-to-left). ----------------The paper is well written and has two good points. Firstly, the idea of using RL to learn parse trees using downstream tasks is very interesting and novel. And employing the shift-reduce framework is a very smart choice because the set of actions is minimal (shift and reduce). Secondly, what shown in the paper somewhat confirms the need of parse trees. This is indeed interesting because of the current debate on whether syntax is helpful.----------------I have the following comments:--------- it seems that the authors weren't aware of some recent work using RL to learn structures for composition, e.g. Andreas et al (2016).--------- because different composition functions (e.g. LSTM, GRU, or classical recursive neural net) have different inductive biases, I was wondering if the tree structures found by the model would be independent from the composition function choice.--------- because RNNs in theory are equivalent to Turing machines, I was wondering if restricting the expressiveness of the model (e.g. reducing the dimension) can help the model focus on discovering more helpful tree structures.----------------Ref:--------Andreas et al. Learning to Compose Neural Networks for Question Answering. NAACL 2016"", 'In this paper, the authors propose a new method to learn hierarchical representations of sentences, based on reinforcement learning. They propose to learn a neural shift-reduce parser, such that the induced tree structures lead to good performance on a downstream task. They use reinforcement learning (more specifically, the policy gradient method REINFORCE) to learn their model. The reward of the algorithm is the evaluation metric of the downstream task. The authors compare two settings, (1) no structure information is given (hence, the only supervision comes from the downstream task) and (2) actions from an external parser is used as supervision to train the policy network, in addition to the supervision from the downstream task. The proposed approach is evaluated on four tasks: sentiment analysis, semantic relatedness, textual entailment and sentence generation.----------------I like the idea of learning tree representations of text which are useful for a downstream task. The paper is clear and well written. However, I am not convinced by the experimental results presented in the paper. Indeed, on most tasks, the proposed model is far from state-of-the-art models:-------- - sentiment analysis, 86.5 v.s. 89.7 (accuracy);-------- - semantic relatedness, 0.32 v.s. 0.25 (MSE);-------- - textual entailment, 80.5 v.s. 84.6 (accuracy).--------From the results presented in the paper, it is hard to know if these results are due to the model, or because of the reinforcement learning algorithm.----------------PROS:-------- - interesting idea: learning structures of sentences adapted for a downstream task.-------- - well written paper.--------CONS:-------- - weak experimental results (do not really support the claim of the authors).----------------Minor comments:--------In the second paragraph of the introduction, one might argue that bag-of-words is also a predominant approach to represent sentences.--------Paragraph titles (e.g. in section 3.2) should have a period at the end.----------------------------------------------------------------------------------------------------------------------------------------------UPDATE----------------I am still not convinced by the results presented in the paper, and in particular by the fact that one must combine the words in a different way than left-to-right to obtain state of the art results.--------However, I do agree that this is an interesting research direction, and that the results presented in the paper are promising. I am thus updating my score from 5 to 6.']",0.8408203125,0.1591796875,1,0.904296875,0.09588623046875,1,0.1812744140625,0.81884765625,0,0.86279296875,0.13720703125,1,0.045684814453125,0.9541015625,0,0.2340087890625,0.76611328125,0
68,68,68,68,68,68,https://openreview.net/forum?id=SkwSJ99ex,"Mid-way, we discover that 1) it's just a convolution 2) it's actually a different kind of convolution depending on whether one fuses serial or parallel pooling layers..2365-2369)..For example, the mean variance normalization layer and batch normalization layer can all be absorbed without retraining or losing accuracy.

This is very misleading and adding noise to the field..https://arxiv.org/abs/1605.02346)..---2.","['One of the main idea of this paper is to replace pooling layers with convolutions of stride 2 and retraining the model. Authors merge this into a new layer and brand it as a new type of layer. This is very misleading and adding noise to the field. And using strided convolutions rather than pooling is not actually novel (e.g. https://arxiv.org/abs/1605.02346).--------While the speed-up obtained are good, the lack of novelty and the rebranding attempt make this paper not a good fit for ICLR.', ""This paper proposes to reduce model size and evaluation time of deep CNN models on mobile devices by converting multiple layers into single layer and then retraining the converted model. The paper showed that the computation time can be reduced by 3x to 5x with only 0.4% accuracy loss on a specific model.----------------Reducing model sizes and speeding up model evaluation are important in many applications. I have several concerns:----------------1. There are many techniques that can reduce model sizes. For example, it has been shown by several groups that using the teacher-student approach, people can achieve the same and sometimes even better accuracy than the teacher (big model) using a much smaller model. However, this paper does not compare any one of them.--------2. The technique proposed in this paper is limited in its applicability since it's designed specifically for the models discussed in the paper. --------3. Replacing several layers with single layer is a relatively standard procedure. For example, the mean variance normalization layer and batch normalization layer can all be absorbed without retraining or losing accuracy.----------------BTW, the DNN low-rank approximation technique was first proposed in speech recognition. e.g., ----------------Xue, J., Li, J. and Gong, Y., 2013, August. Restructuring of deep neural network acoustic models with singular value decomposition. In INTERSPEECH (pp. 2365-2369)."", ""This paper looks at the idea of fusing multiple layers (typically a convolution and a LRN or pooling layer) into a single convolution via retraining of just that layer, and shows that simpler, faster models can be constructed that way at minimal loss in accuracy. This idea is fine. Several issues:--------- The paper introduces the concept of a 'Deeprebirth layer', and for a while it seems like it's going to be some new architecture. Mid-way, we discover that 1) it's just a convolution 2) it's actually a different kind of convolution depending on whether one fuses serial or parallel pooling layers. I understand the desire to give a name to the technique, but in this case naming the layer itself, when it's actually multiple things, non of which are new architecturally, confuses the argument a lot.--------- There are ways to perform this kind of operator fusion without retraining, and some deep learning framework such as Theano and the upcoming TensorFlow XLA implement them. It would have been nice to have a baseline that implements it, especially since most of the additional energy cost from non-fused operators comes from the extra intermediate memory writes that operator fusion removes.--------- Batchnorm can be folded into convolution layers without retraining by scaling the weights. Were they folded into the baseline figures reported in Table 7?--------- At the time of writing, the authors have not provided the details that would make this research reproducible, in particular how the depth of the fused layers relates to the depth of the original layers in each of the experiments.--------- Retraining: how much time (epochs) does the retraining take? Did you consider using any form of distillation?--------Interesting set of experiments. This paper needs a lot of improvements to be suitable for publication.--------- Open-sourcing: having the implementation be open-source always enhances the usefulness of such paper. Not a requirement obviously.""]",0.59814453125,0.401611328125,1,0.921875,0.07794189453125,1,0.11505126953125,0.884765625,0,0.7685546875,0.2313232421875,1,0.08453369140625,0.91552734375,0,0.2154541015625,0.78466796875,0
69,69,69,69,69,69,https://openreview.net/forum?id=SkxKPDv5xl,"Similarly, Figure 3 shows the vanilla RNN outperforming the Wavenet reimplementation in human evaluation on the Blizzard dataset..This is not discussed at all, I think it would be useful to comment on why this could be happening..---Another novelty here is that they use a quantitative likelihood-based measure to assess them model, in addition to the AB human comparisons used in the wavenet work.

Hopefully, this will be addressed in the final version..---As the authors admit, their wavenet implementation is probably not as good as the original one, which makes the comparisons questionable..-Despite the cons and given that more modeling details are provided, I think this paper will be a valuable contribution.","[""The paper proposed a novel SampleRNN to directly model waveform signals and achieved better performance both in terms of objective test NLL and subjective A/B tests. ----------------As mentioned in the discussions, the current status of the paper lack plenty of details in describing their model. Hopefully, this will be addressed in the final version.----------------The authors attempted to compare with wavenet model, but they didn't manage to get a model better than the baseline LSTM-RNN, which makes all the comparisons to wavenets less convincing. Hence, instead of wasting time and space comparing to wavenet, detailing the proposed model would be better. "", 'The paper introduces SampleRNN, a hierarchical recurrent neural network model of raw audio. The model is trained end-to-end and evaluated using log-likelihood and by human judgement of unconditional samples, on three different datasets covering speech and music. This evaluation shows the proposed model to compare favourably to the baselines.----------------It is shown that the subsequence length used for truncated BPTT affects performance significantly, but interestingly, a subsequence length of 512 samples (~32 ms) is sufficient to get good results, even though the features of the data that are modelled span much longer timescales. This is an interesting and somewhat unintuitive result that I think warrants a bit more discussion.----------------The authors have attempted to reimplement WaveNet, an alternative model of raw audio that is fully convolutional. They were unable to reproduce the exact model architecture from the original paper, but have attempted to build an instance of the model with a receptive field of about 250ms that could be trained in a reasonable time using their computational resources, which is commendable.----------------The architecture of the Wavenet model is described in detail, but it found it challenging to find the same details for the proposed SampleRNN architecture (e.g. which value of ""r"" is used for the different tiers, how many units per layer, ...). I think a comparison in terms of computational cost, training time and number of parameters would also be very informative.----------------Surprisingly, Table 1 shows a vanilla RNN (LSTM) substantially outperforming this model in terms of likelihood, which is quite suspicious as LSTMs tend to have effective receptive fields of a few hundred timesteps at best. One would expect the much larger receptive field of the Wavenet model to be reflected in the likelihood scores to some extent. Similarly, Figure 3 shows the vanilla RNN outperforming the Wavenet reimplementation in human evaluation on the Blizzard dataset. This raises questions about the implementation of the latter. Some discussion about this result and whether the authors expected it or not would be very welcome.----------------Table 1 and Figure 4 also show the 2-tier SampleRNN outperforming the 3-tier model in terms of likelihood and human rating respectively, which is very counterintuitive as one would expect longer-range temporal correlations to be even more relevant for music than for speech. This is not discussed at all, I think it would be useful to comment on why this could be happening.----------------Overall, this an interesting attempt to tackle modelling very long sequences with long-range temporal correlations and the results are quite convincing, even if the same can\'t always be said of the comparison with the baselines. It would be interesting to see how the model performs for conditional generation, seeing as it can be more easily be objectively compared to models like Wavenet in that domain.--------------------------------Other remarks:----------------- upsampling the output of the models is done with r separate linear projections. This choice of upsampling method is not motivated. Why not just use linear interpolation or nearest neighbour upsampling? What is the advantage of learning this operation? Don\'t the r linear projections end up learning largely the same thing, give or take some noise?----------------- The third paragraph of Section 2.1.1 indicates that 8-bit linear PCM was used. This is in contrast to Wavenet, for which an 8-bit mu-law encoding was used, and this supposedly improves the audio fidelity of the samples. Did you try this as well?----------------- Section 2.1 mentions the discretisation of the input and the use of a softmax to model this discretised input, without any reference to prior work that made the same observation. A reference is given in 2.1.1, but it should probably be moved up a bit to avoid giving the impression that this is a novel observation.', ""Pros:--------The authors are presenting an RNN-based alternative to wavenet, for generating audio a sample at a time.--------RNNs are a natural candidate for this task so this is an interesting alternative. Furthermore the authors claim to make significant improvement in the quality of the produces samples.--------Another novelty here is that they use a quantitative likelihood-based measure to assess them model, in addition to the AB human comparisons used in the wavenet work.----------------Cons:--------The paper is lacking equations that detail the model. This can be remedied in the camera-ready version.--------The paper is lacking detailed explanations of the modeling choices:--------- It's not clear why an MLP is used in the bottom layer instead of (another) RNN.--------- It's not clear why r linear projections are used for up-sampling, instead of feeding the same state to all r samples, or use a more powerful type of transformation. --------As the authors admit, their wavenet implementation is probably not as good as the original one, which makes the comparisons questionable. ----------------Despite the cons and given that more modeling details are provided, I think this paper will be a valuable contribution. ""]",0.880859375,0.11907958984375,1,0.95166015625,0.048126220703125,1,0.48046875,0.51953125,0,0.89697265625,0.1031494140625,1,0.232177734375,0.76806640625,0,0.31884765625,0.68115234375,0
70,70,70,70,70,70,https://openreview.net/forum?id=SyVVJ85lg,"-The ability to predict network running time is very useful, and the paper shows that even a simple model does it reasonably, which is a strength..---Furthermore, their code is open-source and the live demo is looking good..-More experiments were added, so I'm updating my score.

This paper introduces an analytical performance model to estimate the training and evaluation time of a given network for different software, hardware and communication strategies..The authors included many freedoms in the variables while calculating the run-time of a network such as the number of workers, bandwidth, platform, and parallelization strategy..It would also be a different story if the authors had been able to use this framework to make novel architectural decisions that improved training scalability in some way, and incorporated such new insights in the paper.","['This paper introduces an analytical performance model to estimate the training and evaluation time of a given network for different software, hardware and communication strategies. --------The paper is very clear.  The authors included many freedoms in the variables while calculating the run-time of a network such as the number of workers, bandwidth, platform, and parallelization strategy. Their results are consistent with the reported results from literature.--------Furthermore, their code is open-source and the live demo is looking good. --------The authors mentioned in their comment that they will allow users to upload customized networks and model splits in the coming releases of the interface, then the tool can become very useful.--------It would be interesting to see some newer network architectures with skip connections such as ResNet, and DenseNet.', ""In PALEO the authors propose a simple model of execution of deep neural networks. It turns out that even this simple model allows to quite accurately predict the computation time for image recognition networks both in single-machine and distributed settings.----------------The ability to predict network running time is very useful, and the paper shows that even a simple model does it reasonably, which is a strength. But the tests are only performed on a few networks of very similar type (AlexNet, Inception, NiN) and only in a few settings. Much broader experiments, including a variety of models (RNNs, fully connected, adversarial, etc.) in a variety of settings (different batch sizes, layer sizes, node placement on devices, etc.) would probably reveal weaknesses of the proposed very simplified model. This is why this reviewer considers this paper borderline -- it's a first step, but a very basic one and without sufficiently large experimental underpinning.----------------More experiments were added, so I'm updating my score."", ""This paper is technically sound. It highlights well the strengths and weaknesses of the proposed simplified model.----------------In terms of impact, its novelty is limited, in the sense that the authors did seemingly the right thing and obtained the expected outcomes. The idea of modeling deep learning computation is not in itself particularly novel. As a companion paper to an open source release of the model, it would meet my bar of acceptance in the same vein as a paper describing a novel dataset, which might not provide groundbreaking insights, yet be generally useful to the community.----------------In the absence of released code, even if the authors promise to release it soon, I am more ambivalent, since that's where all the value lies. It would also be a different story if the authors had been able to use this framework to make novel architectural decisions that improved training scalability in some way, and incorporated such new insights in the paper.----------------UPDATED: code is now available. Revised review accordingly.""]",0.94775390625,0.05242919921875,1,0.92431640625,0.075439453125,1,0.317626953125,0.6826171875,0,0.9345703125,0.06561279296875,1,0.269287109375,0.73046875,0,0.471923828125,0.52783203125,0
71,71,71,71,71,71,https://openreview.net/forum?id=SygvTcYee,"-There also seem to be a few ideas worth comparing, at least:
---- Circular vs. parameter server configurations
---- Decoupled sub-problems vs. parallel SGD


-Parallel SGD also has the benefit that it's extremely easy to implement on top of NN toolboxes, so this has to work a lot better to be practically useful..-Related Work: In the part on convex ERM and methods, I would recommend citing general communication efficient frameworks, COCOA (Ma et al.).This means passing around the parameter vector for each hidden unit.

Then each machine updates it's coordinates based on the complete model for a slice of the data..This would mean, for a feed-forward network, producing the intermediate activations of each layer for each data point..Assuming you have one sub-problem for every hidden unit, then it seems like:


-1.","['The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) (Carreira-Perpinan and Wang, 2012). This method decomposes the optimization into training individual layers and updating the auxiliary coordinates. The paper focuses on binary autoencoders and proposes to partition the data onto several machines allowing the parameters to move between machines. Relatively good speedup factors are reported especially on larger datasets and a theoretical model of performance is presented that matches with the experiments.----------------My main concern is that even though the method is presented as a general framework for nested functions, experiments focus on a restricted family of models (i.e. binary autoencoders with linear or kernel encoders and linear decoders) with only two components. While the speedup factors are encouraging, it is hard to get a sense of their importance as the binary autoencoder model considered is not well studied by other researchers and is not widely used. I encourage the authors to apply this framework to more generic architectures and problems.----------------Questions:--------1- Does this framework apply to some form of generic multi-layer neural network? If so, some experimental results are useful.--------2- What is the implication of applying this framework to more than two components (an encoder and a decoder) and non-linear components?--------3- It is desired to see a plot of performance as a function of time for different setups to demonstrate the speedup after convergence. It seems the paper only focuses on the speedup factors per iteration. For example, increasing the mini-batch size may improve the speed per iteration but may hurt the convergence speed.--------4- Did you consider a scenario where the dataset is too big that storing the data and auxiliary variables on multiple machines simultaneously is not possible?----------------The paper cites an ArXiv manuscript with the same title by the authors multiple times. Please make the paper self-contained and include any supplementary material in the appendix.----------------I believe without applying this framework to a more generic architecture beyond binary autoencoders, this paper does not appeal to a wide audience at ICLR, hence weak reject.', 'UPDATE:--------I looked at the arxiv version of the paper. It is much longer and appears more rigorous. Fig 3 there is indeed more insightful.--------However, I am reviewing the submission and my overall assessment does not change. This is not a minor incremental contribution, and if you want to compress it into a conference submission of this type, I would recommend choosing message you want to convey, and focus on that. As you say, ""...ICLR submission focus on the ParMAC algorithm..."", I would focus on this properly - and remove or move to appendix all extensions and theoretical remarks, and have an extra page on explaining the algorithm. Additionally, make sure to clearly explain the relation of the arxiv paper, in particular that the submission was a compressed version.----------------ORIGINAL REVIEW:--------The submission proposes ParMAC, based on MAC (Method of Auxiliary Coordinates), formulating a distributed variant of the idea.----------------Related Work: In the part on convex ERM and methods, I would recommend citing general communication efficient frameworks, COCOA (Ma et al.) and AIDE (Reddi et al.). I believe these works are most related to the practical objectives authors of this paper set, while number of the papers cited are less relevant.----------------Section 2, explaining MAC, is quite clearly written, but I do not find part on MAC and EM particularly useful.----------------Section 3 is much less clearly written. I have trouble following notation, particularly in the speedups part, as different symbols were introduced at different places. Perhaps a quick summary or paragraph on notation in the introduction would be helpful. In paragraph 2, you write as if reader knew how data/anything is distributed, but this was not mentioned yet; it is specified later. It is not clear what is meant by ""submodel"". Perhaps a more precise example pointing back to eqs (1) & (2) would be useful. As far as I understand from what is written, there are P independent sets of submodels, that traverse the machines in circular fashion. I don\'t understand how are they initialized (identically?), and more importantly I don\'t understand what would be a single output of the algorithm (averaging? does not seem to make sense). Since this is not addressed, I suppose I get it wrong, leaving me to guess what was actually meant. --------The fact that I am not able to understand what is actually happening, I see as major issue.----------------I don\'t like the later paragraphs on extensions, model for speedup, convergence and topologies. I don\'t understand whether these are novel contributions or not, as the authors refer to other work for details. If these are novel, the explanation is not sufficient, particularly speedup part, which contains undefined quantities, e.g. T(P) (or I can\'t find it). If this is not novel, It does not provide enough explanation to understand anything more, compared with a its version compressed to 1/4 of its size and referring to the other work. The statement that we can recover the original convergence guarantees seems strong and I don\'t see why it should be trivial to show (but author point to other work which I did not look at). In topologies part, claiming that something does ""true SGD"", without explaining what is ""true SGD"" seems very strange. Other statements in this section seem also very vague and unjustified/unexplained.----------------Experimental section seems to suggest that the method is interesting for binary autoencoders, but I don\'t see how would I conclude anything about any other models. ParMAC is also not compared to alternative methods, only with itself, focusing on scaling properties.----------------Conclusion contains statements that are too strong or misleading based on what I saw. In particular, ""we analysed its parallel speedup and convergence"" seems ungrounded. Further, the claim ""The convergence properties of MAC remain essentially unaltered in ParMAC"" is unsupported, regardless of the meaning of ""essentially unchanged"".----------------In summary, the method seems relevant for particular model class, binary autoencoders, but clarity of presentation is insufficient - I wouldn\'t be able to recreate the algorithm used in experiments - and the paper contains a number of questionable claims.', ""This paper proposes an extension of the MAC method in which subproblems are trained on a distributed cluster arranged in a circular configuration. The basic idea of MAC is to decouple the optimization between parameters and the outputs of sub-pieces of the model (auxiliary coordinates); optimization alternates between updating the coordinates given the parameters and optimizing the parameters given the outputs. In the circular configuration. Because each update is independent, they can be massively parallelized.----------------This paper would greatly benefit from more concrete examples of the sub-problems and how they decompose. For instance, can this be applied effectively for deep convolutional networks, recurrent models, etc? From a practical perspective, there's not much impact for this paper beyond showing that this particular decoupling scheme works better than others. ----------------There also seem to be a few ideas worth comparing, at least:--------- Circular vs. parameter server configurations--------- Decoupled sub-problems vs. parallel SGD----------------Parallel SGD also has the benefit that it's extremely easy to implement on top of NN toolboxes, so this has to work a lot better to be practically useful. ----------------Also, it's a bit hard to understand what exactly is being passed around from round to round, and what the trade-offs would be in a deep feed-forward network. Assuming you have one sub-problem for every hidden unit, then it seems like:----------------1. In the W step, different bits of the NN walk their way around the cluster, taking SGD steps w.r.t. the coordinates stored on each machine. This means passing around the parameter vector for each hidden unit.--------2. Then there's a synchronization step to gather the parameters from each submodel, requiring a traversal of the circular structure.--------3. Then each machine updates it's coordinates based on the complete model for a slice of the data. This would mean, for a feed-forward network, producing the intermediate activations of each layer for each data point.----------------So for something comparable to parallel SGD, you could do the following: put a mini-batch of size B on each machine with ParMAC, compared to running such mini-batches in parallel. Completing steps 1-2-3 above would then be roughly equivalent to one synchronized PS type implementation step (distribute model to workers, get P gradients back, update model.)---------------- It would be really helpful to see how this compares in practice. It's hard for me to understand intuitively why the proposed method is theoretically any better than parallel SGD (except for the issue of non-smooth function optimization); the decoupling also can fundamentally change the problem since you're not doing back-propagation directly anymore, so that seems like it would conflate things as well and it's not necessarily going to just work for other types of architectures."", 'This paper proposes a novel approach ParMAC, a parallel and distributed framework of MAC (the Method of Auxiliary Coordinates) to learn nested and non-convex models which is based on the composition of multiple processing layers (i.e., deep nets). The basic idea of MAC to optimise the nested objective function, which is traditionally learned using methods based on the chain-rule gradients but inconvenient and is hard to parallelise, is to break nested functional relationships judiciously by introducing new variables ( the auxiliary coordinates) as equality constraints, and then to optimise a penalised function using alternating optimisation over the original parameters (W step) and over the coordinates (Z step).  The minimisation (W step) updates the parameters by splitting the nested model into independent submodels and training them using existing algorithms, and the coordination (Z step) ensures that corresponding inputs and outputs of submodels eventually match.  In this paper, the basic assumptions of ParMAC are that with large datasets in distributed systems, it is imperative to minimise data movement over the network because of the communication time generally far exceeds the computation time in modern architectures. Thus, the authors propose the ParMAC to translate the parallelism inherent in MAC into a distributed system by data parallelism and model parallelism. They also analyse its parallel speedup and convergence, and demonstrated it with MPI-based implementation to optimise binary autoencoders. The proposed ParMAC is tested on 3 colour image retrieval datasets. ----------------The organization of the paper is well written, and the presentation is clear. My questions are included in the following:--------- The MAC framework solves the original problem approximately. If people use the sigmoid function to smooth the stepwise function, the naive optimization methods can be easier applied. What is the difference between these two? Or why do we want to use a new approach to solve it?--------- The authors do not compare their ParMAC model with other distributed approaches for the same nested function optimization problem.']",0.52685546875,0.47314453125,1,0.87841796875,0.12139892578125,1,0.10491943359375,0.89501953125,0,0.822265625,0.1778564453125,1,0.0621337890625,0.93798828125,0,0.261962890625,0.73779296875,0
72,72,72,72,72,72,https://openreview.net/forum?id=r10FA8Kxg,"---They also observe (as others have before) that student models can be shallower than the teacher model from which they are trained for comparable performance..It would be interesting to see some results on a more challenging problem such as ImageNet..-Originality:
---- This is mainly an experimental paper, but the question it asks is interesting and worth investigation.

The experiments are performed on he CIFAR 10 dataset where deep convolutional teacher networks are used to train shallow student networks using L2 regression on logit outputs..-Strong  points..The results show that similar accuracy on the same parameter budget can be only obtained when multiple layers of convolution are used.","['Description.--------This paper describes experiments testing whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy. The experiments are performed on he CIFAR 10 dataset where deep convolutional teacher networks are used to train shallow student networks using L2 regression on logit outputs.  The results show that similar accuracy on the same parameter budget can be only obtained when multiple layers of convolution are used. ----------------Strong  points.--------- The experiments are carefully done with thorough selection of hyperparameters. --------- The paper shows interesting results that go partially against conclusions from the previous work in this area (Ba and Caruana 2014).--------- The paper is well and clearly written.----------------Weak points:--------- CIFAR is still somewhat toy dataset with only 10 classes. It would be interesting to see some results on a more challenging problem such as ImageNet. Would the results for a large number of classes be similar?----------------Originality:--------- This is mainly an experimental paper, but the question it asks is interesting and worth investigation. The experimental results are solid and provide new insights.----------------Quality:--------- The experiments are well done.----------------Clarity:--------- The paper is well written and clear.----------------Significance:--------- The results go against some of the conclusions from previous work, so should be published and discussed.----------------Overall:--------Experimental paper with interesting results. Well written. Solid experiments. ', 'This paper aims to investigate the question if shallow non-convolutional networks can be as affective as deep convolutional ones for image classification, given that both architectures use the same number of parameters. --------To this end the authors conducted a series of experiments on the CIFAR10 dataset.--------They find that there is a significant performance gap between the two approaches, in favour of deep CNNs. --------The experiments are well designed and involve a distillation training approach, and the results are presented in a comprehensive manner.--------They also observe (as others have before) that student models can be shallower than the teacher model from which they are trained for comparable performance.----------------My take on these results is that they suggest that using (deep) conv nets is more effective, since this model class encodes a form of a-prori or domain knowledge that images exhibit a certain degree of translation invariance in the way they should be processed for high-level recognition tasks. The results are therefore perhaps not quite surprising, but not completely obvious either.----------------An interesting point on which the authors comment only very briefly is that among the non-convolutional architectures the ones using 2 or 3 hidden layers outperform those with 1, 4 or 5 hidden layers. Do you have an interpretation / hypothesis of why this is the case? It  would be interesting to discuss the point a bit more in the paper.----------------It was not quite clear to me why were the experiments were limited to use  30M parameters at most. None of the experiments in Figure 1 seem to be saturated. Although the performance gap between CNN and MLP is large, I think it would be worthwhile to push the experiment further for the final version of the paper.----------------The authors state in the last paragraph that they expect shallow nets to be relatively worse in an ImageNet classification experiment. --------Could the authors argue why they think this to be the case? --------One could argue that the much larger training dataset size could compensate for shallow and/or non-convolutional choices of the architecture. --------Since MLPs are universal function approximators, one could understand architecture choices as expressions of certain priors over the function space, and in a large-data regimes such priors could be expected to be of lesser importance.--------This issue could for example be examined on ImageNet when varying the amount of training data.--------Also, the much higher resolution of ImageNet images might have a non-trivial impact on the CNN-MLP comparison as compared to the results established on the CIFAR10 dataset.----------------Experiments on a second data set would also help to corroborate the findings, demonstrating to what extent such findings are variable across datasets.']",0.849609375,0.150390625,1,0.95361328125,0.04620361328125,1,0.15087890625,0.84912109375,0,0.927734375,0.0721435546875,1,0.144287109375,0.85546875,0,0.401611328125,0.59814453125,0
73,73,73,73,73,73,https://openreview.net/forum?id=r17RD2oxe,"I’m not sure what we can conclude from the paper beyond the fact that CNNs are able to group categories together based on visual similarities, and deeper networks are able to do this better than more shallow networks (Fig 2)..""Experiments show that the proposed method using deep representation is very competitive to human beings in building the tree of life based on the visual similarity of the species."".Our visual world is not a hierarchy.

-There are two claimed contributions: (1) Constructs a biology evolutionary tree, and (2) Gives insight into the representations produced by deep networks..-Regarding (2), the technical depth of the exploration is not sufficient for ICLR..-Regarding (1), while the motivation of the work is grounded in biology, in practice the method is based only on visual similarity.","['The paper presents a simple method for constructing a visual hierarchy of ImageNet classes based on a CNN trained on discriminate between the classes. It investigates two metrics for measuring inter-class similarity: (1) softmax probability outputs, i.e., the class confusion matrix, and (2) L2 distance between fc7 features, along with three methods for constructing the hierarchy given the distance matrix: (1) approximation central point, (2) minimal spanning tree, and (3) multidimensional scaling of Borg&Groenen 2005.----------------There are two claimed contributions: (1) Constructs a biology evolutionary tree, and (2) Gives insight into the representations produced by deep networks. ----------------Regarding (1), while the motivation of the work is grounded in biology, in practice the method is based only on visual similarity. The constructed trees thus can’t be expected to reflect the evolutionary hierarchy, and in fact there are no quantitative experiments that demonstrate that they do. ----------------Regarding (2), the technical depth of the exploration is not sufficient for ICLR. I’m not sure what we can conclude from the paper beyond the fact that CNNs are able to group categories together based on visual similarities, and deeper networks are able to do this better than more shallow networks (Fig 2).----------------In summary, this paper is unfortunately not ready for publication at this time. ', ""This paper introduces a hierarchical clustering method using learned CNN features to build 'the tree of life'. The assumption is that the feature similarity indicates the distance in the tree. The authors tried three different ways to construct the tree: 1) approximation central point 2) minimum spanning tree and 3) multidimensional scaling based method. Out of them, MDS works the best. It is a nice application of using deep features. However, I lean toward rejecting the paper because the following reasons:----------------1) All experiments are conducted in very small scale. The experiments include 6 fish species, 11 canine species, 8 vehicle classes. There are no quantitative results, only by visualizing the generated tree versus the wordNet tree. Moreover, the assumption of using wordNet is not quite valid. WordNet is not designed for biology purpose and it might not reflect the true evolutionary relationship between species. --------2) Limited technical novelty. Most parts of the pipeline are standard, e.g. use pretrained model for feature extraction, use previous methods to construct hierarchical clustering. I think the technical contribution of this paper is very limited. "", 'I like this paper in that it is a creative application of computer vision to Biology. Or, at least, that would be a good narrative but I\'m not confident biologists would actually care about the ""Tree of Life"" built from this method. There\'s not really any biology in this paper, either in methodology or evaluation. It boils down to a hierarchical clustering of visual categories with ground truth assumed to be the WordNet hierarchy (which may or may not be the biological ground truth inheritance relationships between species, if that is even possible to define -- it probably isn\'t for dog species which interbreed and it definitely isn\'t for vehicles) or the actual biological inheritance tree or what humans would do in the same task. If we\'re just worried about visual relationships and not inheritance relationships then a graph is the right structure, not a tree. A tree is needlessly lossy and imposes weird relationships (e.g. ImageNet has a photo of a ""toy rabbit"" and by tree distance it is maximally distant from ""rabbit"" because the toy is in the devices top level hierarchy and the real rabbit is in the animal branch. Are those two images really as semantically unrelated as is possible?). Our visual world is not a hierarchy. Our biological world can reasonably be defined as one. One could define the task of trying to recover the biological inheritance tree from visual inputs, although we know that would be tough to do because of situations like convergent evolution. Still, one could evaluate how well various visual features can recover the hierarchical relationship of biological organisms. This paper doesn\'t quite do that. And even if it did, it would still feel like a bit of a solution in search of a problem. The paper says that this type of exercise can help us understand deep features, but I\'m not sure sure how much it reveals. I guess it\'s a fair question to ask if a particular feature produces meaningful class-to-class distances, but it\'s not clear that the biological tree of life or the wordnet hierarchy is the right ground truth for that (I\'d argue it\'s not).----------------Finally, the paper mentions human baselines in a few places but I\'m not really seeing it. ""Experiments show that the proposed method using deep representation is very competitive to human beings in building the tree of life based on the visual similarity of the species."" and then later ""The reconstructed quality is as good as what human beings could reconstruct based on the visual similarity."" That\'s the extent of the experiment? A qualitative result and the declaration that it\'s as good as humans could do? ']",0.8857421875,0.11419677734375,1,0.65966796875,0.34033203125,1,0.319580078125,0.68017578125,0,0.9072265625,0.09271240234375,1,0.09442138671875,0.90576171875,0,0.283935546875,0.7158203125,0
74,74,74,74,74,74,https://openreview.net/forum?id=r1Aab85gg,"I have few more comments:


-1..The paper focuses on bilingual word representation learning with the following setting:


-1..There is no direct word to word alignments available between the source and target language.

This paper discusses aligning word vectors across language when those embeddings have been learned independently in monolingual settings..There are reasonable scenarios in which such a strategy could come in helpful, so I feel this paper addresses an interesting problem..The paper is mostly well executed but somewhat lacks in evaluation.","['This paper discusses aligning word vectors across language when those embeddings have been learned independently in monolingual settings. There are reasonable scenarios in which such a strategy could come in helpful, so I feel this paper addresses an interesting problem. The paper is mostly well executed but somewhat lacks in evaluation. It would have been nice if a stronger downstream task had been attempted.----------------The inverted Softmax idea is very nice.----------------A few minor issues that ought to be addressed in a published version of this paper:----------------1) There is no mention of Haghighi et al (2008) ""Learning Bilingual Lexicons from Monolingual Corpora."", which strikes me as a key piece of prior work regarding the use of CCA in learning bilingual alignment. This paper and links to the work here ought to be discussed.--------2) Likewise, Hermann & Blunsom (2013) ""Multilingual distributed representations without word alignment."" is probably the correct paper to cite for learning multilingual word embeddings from multilingual aligned data.--------3) It would have been nicer if experiments had been performed with more divergent language pairs rather than just European/Romance languages--------4) A lot of the argumentation around the orthogonality requirements feels related to the idea of using a Mahalanobis distance / covar matrix to learn such mappings. This might be worth including in the discussion--------5) I don\'t have a better suggestion, but is there an alternative to using the term ""translation (performance/etc.)"" when discussing word alignment across languages? Translation implies something more complex than this in my mind.--------6) The Mikolov citation in the abstract is messed up', 'The paper focuses on bilingual word representation learning with the following setting:----------------1. Bilingual representation is learnt in an offline manner i.e., we already have monolingual representations for the source and target language and we are learning a common mapping for these two representations.--------2. There is no direct word to word alignments available between the source and target language.----------------This is a practically useful setting to consider and authors have done a good job of unifying the existing solutions for this problem by providing theoretical justifications. Even though the authors do not propose a new method for offline bilingual representation learning, the paper is significant for the following contributions:----------------1. Theory for offline bilingual representation learning.--------2. Inverted softmax.--------3. Using cognate words for languages that share similar scripts.--------4. Showing that this method also works at sentence level (to some extent).----------------Authors have addressed all my pre-review questions and I am ok with their response. I have few more comments:----------------1. Header for table 3 which says “word frequency” is misleading. “word frequency” could mean that rare words occur in row-1 while I guess authors meant to say that rare words occur in row-5.--------2. I see that authors have removed precision @5 and @10 from table-6. Is it because of the space constraints or the results have different trend? I would like to see these results in the appendix.--------3. In table-6 what is the difference between row-3 and row-4? Is the only difference NN vs. inverted softmax? Or there are other differences? Please elaborate.--------4. Another suggestion is to try running an additional experiment where one can use both expert dictionary and cognate dictionary. Comparing all 3 methods in this setting should give more valuable insights about the usefulness of cognate dictionary.', 'This paper extends preceding works to create a mapping between the word embedding space of two languages. The word embeddings had been independently trained on monolingual data only, and various forms of bilingual information is used to learn the mapping. This mapping is then used to measure the precision of translations.----------------In this paper, the authors propose two changes: ""CCA"" and ""inverted softmax"".  Looking at Table 1, CCA is only better than Dina et al in 1 out of 6 cases (It/En @1).  Most of the improvements are in fact obtained by the introduction of the inverted softmax normalization.----------------Overall, I wonder which aspect of this paper is really new. You mention:-------- - Faruqui & Dyer 2014 already used CCA and dimensionality reduction-------- - Xing et al 2015 argued already that Mikolov\'s linear matrix should be orthogonal----------------Could you make clear in what aspect your work is different from Faruqui & Dyer 2014 (other the fact that you applied the method to measure translation precision) ?----------------Using cognates instead of a bilingual directory is a nice trick. Please explain how you obtained this list of cognates ? Obviously, this only works for languages with the same alphabet (for instance Greek and Russian are excluded)----------------Also, it seems to me that in linguistics the term ""cognate"" refers to words which have a common etymological origin - they don\'t necessarily have the same written form (e.g. night, nuit, noche, Nacht). Maybe, you should use a different term ? Those words are probably proper names in news texts.']",0.92724609375,0.07275390625,1,0.39794921875,0.60205078125,0,0.36962890625,0.63037109375,0,0.83544921875,0.16455078125,1,0.08880615234375,0.9111328125,0,0.295166015625,0.70458984375,0
75,75,75,75,75,75,https://openreview.net/forum?id=r1Chut9xl,"I would however have like to see how the obtained embedding would perform with respect to other more common embeddings in solving a supervised task..-To discuss the contributions I will quickly review the generative story in the paper: first a K-dimensional latent representation is sampled from a multivariate Gaussian, then an MLP (with parameters \theta) predicts unnormalised potentials over a vocabulary of V words, the potentials are exponentiated and normalised to make the parameters of a multinomial from where word observations are repeatedly sampled to make a document..The second idea is also sensible, but is conceptually not novel.

The paper claims improved inference for density estimation of sparse data (here text documents) using deep generative Gaussian models (variational auto-encoders), and a method for deriving word embeddings from the model's generative parameters that allows for a degree of interpretability similar to that of Bayesian generative topic models..Here intractable inference is replaced by the VAE formulation where an inference network (with parameters \phi) independently predicts for each document the mean and variance of a normal distribution (amenable to reparameterised gradient computation)..In optimising generative parameters (\theta) and variational parameters (\phi), the authors turn to a treatment which is reminiscent of the original SVI procedure.","['The paper claims improved inference for density estimation of sparse data (here text documents) using deep generative Gaussian models (variational auto-encoders), and a method for deriving word embeddings from the model\'s generative parameters that allows for a degree of interpretability similar to that of Bayesian generative topic models.----------------To discuss the contributions I will quickly review the generative story in the paper: first a K-dimensional latent representation is sampled from a multivariate Gaussian, then an MLP (with parameters \\theta) predicts unnormalised potentials over a vocabulary of V words, the potentials are exponentiated and normalised to make the parameters of a multinomial from where word observations are repeatedly sampled to make a document. Here intractable inference is replaced by the VAE formulation where an inference network (with parameters \\phi) independently predicts for each document the mean and variance of a normal distribution (amenable to reparameterised gradient computation).----------------The first, and rather trivial, contribution is to use tf-idf features to inject first order statistics (a global information) into local observations. The authors claim that this is particularly helpful in the case of sparse data such as text.----------------The second contribution is more interesting. In optimising generative parameters (\\theta) and variational parameters (\\phi), the authors turn to a treatment which is reminiscent of the original SVI procedure. That is, they see the variational parameters \\phi as *global* variational parameters, and the predicted mean \\mu(x) and covariance \\Sigma(x) of each observation x are treated as *local* variational parameters. In the original VAE, local parameters are not directly optimised, instead they are indirectly optimised via optimisation of the global parameters utilised in their prediction (shared MLP parameters). Here, local parameters are optimised holding generative parameters fixed (line 3 of Algorithm 1). The optimised local parameters are then used in the gradient step of the generative parameters (line 4 of Algorithm 1). Finally, global variational parameters are also updated (line 5). --------Whereas indeed other authors have proposed to optimise local parameters, I think that deriving this procedure from the more familiar SVI makes the contribution less of a trick and easier to relate to.----------------Some things aren\'t entirely clear to me. I think it would have been nice if the authors had shown the functional form of the gradient used in step 3 of Algorithm 1. The gradient step for global variational parameters (line 5 of Algorithm 1) uses the very first prediction of local parameters (thus ignoring the optimisation in step 3), this is unclear to me. Perhaps I am missing a fundamental reason why that has to be the case (either way, please clarify).--------The authors argue that this optimisation turns out helpful to modelling sparse data because there is evidence that the generative model p_\\theta(x|z) suffers from poor initialisation. Please, discuss why you expect the initialisation problem to be worse in the case of sparse data.----------------The final contribution is a neat procedure to derive word embeddings from the generative model parameters. These embeddings are then used to interpret what the model has learnt. Interestingly, these word embeddings are context-sensitive once that the latent variable models an entire document.----------------About Figures 2a and 2b: the caption says that solid lines indicate validation perplexity for M=1 (no optimisation of local parameters) and dashed lines indicate M=100 (100 iterations of optimisation of local parameters), but the legends of the figures suggest a different reading. If I interpret the figures based on the caption, then it seems that indeed deeper networks exposed to more data benefit from optimisation of local parameters. Are the authors pretty sure that in Figure 2b models with M=1 have reached a plateau (so that longer training would not allow them to catch up with M=100 curves)? As the authors explain in the caption, x-axis is not comparable on running time, thus the question.----------------The analysis of singular values seems like an interesting way to investigate how the model is using its capacity.  However, I can barely interpret Figures 2c and 2d, I think the authors could have walked readers through them.----------------As for the word embedding I am missing an evaluation on a predictive task. Also, while illustrative, Table 2b is barely reproducible. The text reads ""we create a document comprising a subset of words in the the context’s Wikipedia page."" which is rather vague. I wonder whether this construct needs to be carefully designed in order to get Table 2b.----------------In sum, I have a feeling that the inference technique and the embedding technique are both useful, but perhaps they should have been presented separately so that each could have been explored in greater depth.', 'First I would like to apologize for the delay in reviewing.----------------Summary : In this paper a variational inference is adapted to deep generative models, showing improvement for non-negative sparse dataset. The authors offer as well a method to interpret the data through the model parameters.----------------The writing is generally clear. The methods seem correct. The introspection approach appears to be original. I found very interesting the experiment on the polysemic word embedding. I would however have like to see how the obtained embedding would perform with respect to other more common embeddings in solving a supervised task.----------------Minor :--------Eq. 2: too many closing parentheses', 'This paper introduces three tricks for training deep latent variable models on sparse discrete data:--------1) tf-idf weighting--------2) Iteratively optimizing variational parameters after initializing them with an inference network--------3) A technique for improving the interpretability of the deep model----------------The first idea is sensible but rather trivial as a contribution. The second idea is also sensible, but is conceptually not novel. What is new is the finding that it works well for the dataset used in this paper.----------------The third idea is interesting, and seems to give qualitatively reasonable results. The quantitative semantic similarity results don’t seem that convincing, but I am not very familiar with the relevant literature and therefore cannot make a confident judgement on this issue.', ""This paper presents a small trick to improve the model quality of variational autoencoders (further optimizing the ELBO while initializing it from the predictions of the q network, instead of just using those directly) and the idea of using Jacobian vectors to replace simple embeddings when interpreting variational autoencoders.----------------The idea of the Jacobian as a natural replacement for embeddings is interesting, as it does seem to cleanly generalize the notion of embeddings from linear models. It'd be interesting to see comparisons with other work seeking to provide context-specific embeddings, either by clustering or by smarter techniques (like Neelakantan et al, Efficient non-parametric estimation of multiple embeddings per word in vector space, or Chen et al A Unified Model for Word Sense Representation and Disambiguation). With the evidence provided in the experimental section of the paper it's hard to be convinced that the Jacobian of VAE-generated embeddings is substantially better at being context-sensitive than prior work.----------------Similarly, the idea of further optimizing the ELBO is interesting but not fully explored. It's unclear, for example, what is the tradeoff between the complexity of the q network and steps further optimizing the ELBO, in terms of compute versus accuracy.----------------Overall the ideas in this paper are good but I'd like to see them a little more fleshed out.""]",0.841796875,0.158447265625,1,0.9296875,0.070068359375,1,0.44921875,0.55078125,0,0.90576171875,0.0943603515625,1,0.16748046875,0.83251953125,0,0.490234375,0.509765625,0
76,76,76,76,76,76,https://openreview.net/forum?id=r1S083cgx,"Prediction of dynamic parameters?.-So I do not think such a paper is appropriate for a conference like ICLR..There is no comparision with other methods.

This paper has no machine learning algorithmic contribution: it just uses the the same combination  of LSTM and bivariate mixture density network as Graves, and the detailed explanation in the appendix even misses one key essential point: how are the Gaussian parameters obtained as a transformation of the output of the LSTM..The paper presents a method for sequence generation with a known method applied to feature extracted from another existing method..The part describing the handwriting tasks and the data transformation is well written and interesting to read, it could be valuable work for a conference more focused on handwriting recognition, but I am no expert in the field.","['This paper has no machine learning algorithmic contribution: it just uses the the same combination  of LSTM and bivariate mixture density network as Graves, and the detailed explanation in the appendix even misses one key essential point: how are the Gaussian parameters obtained as a transformation of the output of the LSTM.--------There are also no numerical evaluation suggesting that the algorithm is some form of improvement over the state-of-the-art.----------------So I do not think such a paper is appropriate for a conference like ICLR. The part describing the handwriting tasks and the data transformation is well written and interesting to read, it could be valuable work for a conference more focused on handwriting recognition, but I am no expert in the field.', 'The paper presents a method for sequence generation with a known method applied to feature extracted from another existing method. The paper is heavily oriented towards to chosen technologies and lacks in literature on sequence generation. In principle, rich literature on motion prediction for various applications could be relevant here. Recent models exist for sequence prediction (from primed inputs) for various applications, e.g. for skeleton data. These models learn complex motion w/o any pre-processing. ----------------Evaluation is a big concern. There is no quantitative evaluation. There is no comparision with other methods.----------------I still wonder whether the intermediate representation (developed by Plamondon et al.) is useful in this context of a fully trained sequence generation model and whether the model could pick up the necessary transformations itself. This should be evaluated.----------------Details:----------------There are several typos and word omissions, which can be found by carefully rereading the paper.----------------At the beginning of section 3, it is still unclear what the application is. Prediction of dynamic parameters? What for? Section 3 should give a better motivation of the work.----------------Concerning the following paragraph----------------""While such methods are superior for handwriting analysis and biometric purposes, we opt for a less precise method (Berio & Leymarie, 2015) that is less sensitive to sampling quality and is aimed at generating virtual target sequences that remain perceptually similar to the original trace. --------""--------This method has not been explained. A paper should be self-contained.----------------The authors mentioned that the ""V2V-model is conditioned on (...)""; but not enough details are given. ----------------Generally speaking, more efforts could be made to make the paper more self-contained.', 'This paper takes a model based on that of Graves and retrofits it with a representation derived from the work of Plamondon. --------part of the goal of deep learning has been to avoid the use of hand-crafted features and have the network learn from raw feature representations, so this paper is somewhat against the grain. ----------------The paper relies on some qualitative examples as demonstration of the system, and doesn\'t seem to provide a strong motivation for there being any progress here. --------The paper does not provide true text-conditional handwriting synthesis as shown in Graves\' original work. ----------------Be more consistent about your bibliography (e.g. variants of Plamondon\'s own name, use of ""et al."" in the bibliography etc.) ']",0.94970703125,0.050201416015625,1,0.85498046875,0.14501953125,1,0.1600341796875,0.83984375,0,0.88671875,0.113525390625,1,0.0965576171875,0.9033203125,0,0.37451171875,0.62548828125,0
77,77,77,77,77,77,https://openreview.net/forum?id=r1YNw6sxg,"-- Typos:
---p.2: ""learning is a relative[ly] recent addition""
---p.2: ""be applied [to] directly learn""


-4) Conclusion


-In spite of the aforementioned limits of the experiments, this paper is interesting and solid, in part thanks to the excellent reply to the pre-review questions and the subsequent improved revision..This leads me to believe the authors are more than capable of following to a significant extent the aforementioned suggestions for improvement, thus leading to an even better paper..The authors combine bilinear models of one-step dynamics of visual feature maps at multiple scales with a reinforcement learning algorithm to learn a servoing policy.

The paper proposes a novel approach for learning visual servoing based on Q-iteration..The main contributions of the paper are:


-1..Bilinear dynamics model for predicting next frame (features) based on action and current frame
---2.","['The paper proposes a novel approach for learning visual servoing based on Q-iteration. The main contributions of the paper are:----------------1. Bilinear dynamics model for predicting next frame (features) based on action and current frame--------2. Formulation of servoing with a Q-function that learns weights for different feature channels--------3. An elegant method for optimizing the Bellman error to learn the Q-function----------------Pros:--------+ The paper does a good job of exploring different ways to connect the action (u_t) and frame representation (y_t) to predict next frame features (y_{t+1}). They argue in favour of a locally connected bilinear model which strikes the balance between computation and expressive ability. ----------------Cons:--------- While, sec. 4 makes good arguments for different choices, I would have liked to see more experimental results comparing the 3 approaches: fully connected, convolutional and locally connected dynamics.----------------Pros: --------+ The idea of weighting different channels to capture the importance of obejcts in different channels seems more effective than treating errors across all channels equally. This is also validated experimentally, where unweighted performance suffers consistently.--------+ Solving the Bellman error is a difficult problem in Q-learning approaches. The current paper presents a solid optimization scheme based on the key-observation that scaling Q-function parameters does not affect the best policy chosen. This enables a more elegant FQI approach as opposed to typical optimization schemes which (c_t + \\gamma min_u Q_{t+1}) fixed. ----------------Cons:--------- However, I would have liked to see the difference between FQI and such an iterative approach which holds the second term in Eq. 5 fixed.----------------Experimental results:--------- Overall, I find the experimental results unsatisfying given the small scale and toy simulations. However, the lack of benchmarks in this domain needs to be recognized.--------- Also, as pointed out in pre-review section, the idea of modifying the VGG needs to be experimentally validated. In its current form, it is not clear whether the modified VGG would perform better than the original version.----------------Overall, the contribution of the paper is solid in terms of technical novelty and problem formulations. However, the paper could use stronger experiments as suggested to earlier to bolster its claims.', 'This paper investigates the benefits of visual servoing using a learned--------visual representation. The authors  propose to first learn an action-conditional--------bilinear model of the visual features (obtained from a pre-trained VGG net) from--------which a policy can be derived using a linearization of the dynamics. A multi-scale,--------multi-channel and locally-connected variant of the bilinear model is presented.--------Since the bilinear model only predicts the dynamics one step ahead, the paper--------proposes a weighted objective which incorporates the long-term values of the--------current policy. The evaluation problem is addressed using a fitted-value approach.----------------The paper is well written, mathematically solid, and conceptually exhaustive.--------The experiments also demonstrate the benefits of using a value-weighted objective--------and is an important contribution of this paper. This paper also seems to be the--------first to outline a trust-region fitted-q iteration algorithm. The use of--------pre-trained visual features is also shown to help, empirically, for generalization.----------------Overall, I recommend this paper as it would benefit many researchers in robotics.--------However, in the context of this conference, I find the contribution specifically on--------the ""representation"" problem to be limited. It shows that a pre-trained VGG--------representation is useful, but does not consider learning it end-to-end. This is not--------to say that it should be end-to-end, but proportionally speaking, the paper--------spends more time on the control problem than the representation learning one.--------Also, the policy representation is fixed and the values are approximated--------in linear form using problem-specific features. This doesn\'t make the paper--------less valuable, but perhaps less aligned with what I think ICLR should be about.', '1) Summary----------------This paper proposes to tackle visual servoing (specifically target following) using spatial feature maps from convolutional networks pre-trained on general image classification tasks. The authors combine bilinear models of one-step dynamics of visual feature maps at multiple scales with a reinforcement learning algorithm to learn a servoing policy. This policy is learned by minimizing a regularized weighted average of distances to features predicted by the aforementioned model of visual dynamics.----------------2) Contributions----------------+ Controlled experiments in simulation quantifying the usefulness of pre-trained deep features for visual servoing.--------+ Clear performance benefits with respect to many sensible baselines, including ones using ground truth bounding boxes.--------+ Principled learning of multi-scale visual feature weights with an efficient trust-region fitted Q-iteration algorithm to handle the problem of distractors.--------+ Good sample efficiency thanks to the choice of Q-function approximator and the model-based one-step visual feature dynamics.--------+ Open source virtual city environment to benchmark visual servoing.----------------3) Suggestions for improvement----------------- More complex benchmark:--------Although the environment is not just a toy synthetic one, the experiments would benefit greatly from more complex visual conditions (clutter, distractors, appearance and motion variety, environment richness and diversity, etc). At least, the realism and diversity of object appearances could be vastly improved by using a larger number of 3D car models, including more realistic and diverse ones that can be obtained from Google SketchUp for instance, and populating the environment with more distractor cars (in traffic or parked). This is important as the main desired quality of the approach is robustness to visual variations.----------------- End-to-end and representation learning:--------Although the improvements are already significant in the current synthetic experiments, it would be interesting to measure the impact of end-to-end training (i.e. also fine-tuning the convnet), as it is possibly needed for better generalization in more challenging visual conditions. It would also allow to measure the benefit of deep representation learning for visual servoing, which would be relevant to ICLR (there is no representation learning so far, although the method can be straightforwardly adapted as the authors mention briefly).----------------- Reproducibility:--------The formalism and algorithms are clearly explained, but there is a slightly overwhelming mass of practical tricks and implementation details described with varying levels of details throughout the paper and appendix. Grouping, simplifying, or reorganizing the exposition of these implementation details would help, but a better way would probably consist in only summarizing the most important ones in section and link to an open source implementation of the method for completeness.----------------- Typos:--------p.2: ""learning is a relative[ly] recent addition""--------p.2: ""be applied [to] directly learn""----------------4) Conclusion----------------In spite of the aforementioned limits of the experiments, this paper is interesting and solid, in part thanks to the excellent reply to the pre-review questions and the subsequent improved revision. This leads me to believe the authors are more than capable of following to a significant extent the aforementioned suggestions for improvement, thus leading to an even better paper.']",0.5224609375,0.4775390625,1,0.6318359375,0.368408203125,1,0.1680908203125,0.83203125,0,0.85546875,0.14453125,1,0.189208984375,0.81103515625,0,0.468017578125,0.53173828125,0
78,78,78,78,78,78,https://openreview.net/forum?id=r1osyr_xg,"-Regarding the second question, it is hard to draw many conclusions from analogy tasks alone, especially when effects unrelated to good/bad paraphrasing such as corpus size/content, window size, vocabulary size, etc., can have an outsize effect on performance..(See for example 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should probably be cited.).-The main argumentation leading to the model selection is intuitive, and I believe that the inclusion of good paraphrases and the elimination of bad paraphrases during training should in principle improve word representation quality.

The main idea and model are presented convincingly and seem plausible..The evaluation does not convincingly determine whether the model is a significant improvement over simpler methods (particularly those that do not require the paraphrase database!)..The paper would be stronger if model choices were explained more convincingly or - better yet - alternatives were explored.","['This paper proposes a method for estimating the context sensitivity of paraphrases and uses that to inform a word embedding learning model. The main idea and model are presented convincingly and seem plausible. The main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration. The evaluation does not convincingly determine whether the model is a significant improvement over simpler methods (particularly those that do not require the paraphrase database!). Likewise, the model section did not convince me that this was the most obvious model formulation to try. The paper would be stronger if model choices were explained more convincingly or - better yet - alternatives were explored.----------------On balance I lean towards rejecting the paper and encouraging the authors to submit a revised and improved version at a near point in the future.----------------Detailed/minor points below:----------------1) While the paper is grammatically mostly correct, it would benefit from revision with the help of a native English speaker. In its current form long sections are very difficult to understand due to the unconventional sentence structure.--------2) The tables need better and more descriptive labels.--------3) The results are somewhat inconclusive. Particularly in the analogy task in Table 4 it is surprising that CBOW does better on the semantic aspect of the task than your embeddings which are specifically tailored to be good at this?--------4) Why was ""Enriched CBOW"" not included in the analogy task?--------5) In the related work section several papers are mentioned that learn embeddings from a combination of lexica and corpora, yet it is repeatedly said that this was the first work of such a kind / that there hasn\'t been enough work on this. That feels a little misleading.', ""This paper tries to leverage an external lexicon / knowledge base to improve corpus-based word representations by determining (in a fuzzy way) which potential paraphrase is the most appropriate in a particular context.----------------I think this paper is a bit lost in translation. The grammatical and storytelling styles made it really difficult for me to concentrate, and even unintelligible at times. One of the most important criteria in a conference paper is to communicate one's ideas clearly; unfortunately, I do not feel that this paper meets that standard.----------------In addition, the evaluation is rather lacking. There are many ways to evaluate word representations, and Google's analogy dataset has many issues (see, for example, Linzen's paper from RepEval 2016, as well as Drozd et al., COLING 2016).----------------Finally, this work does not provide any qualitative result or motivation. Why does this method work better? Where does it fail? What have we learned about word representations / lexicons / corpus-based methods in general?"", ""This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology. Sometimes polysemy is context-dependent, but prior approaches have neglected this fact when incorporating external paraphrase information during learning. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates, down-weighting those candidates that depend strongly on context. This function is inferred from bilingual translation agreement.----------------The main argumentation leading to the model selection is intuitive, and I believe that the inclusion of good paraphrases and the elimination of bad paraphrases during training should in principle improve word representation quality. However, the main questions are how well the proposed method achieves this goal, and, even if it achieves it well, whether it makes much difference in practical terms.----------------Regarding the first question, I am not entirely convinced that the parameterization of the control function f(x_ij) is optimal. It would have been nice to see some experiments investigating different choices, in particular some baselines where the effect of f is diminished (so that it reduces to f=1 in the limit) would have been interesting. I also feel like there would be a lot to gain from having f be a function of the nearby word embeddings, though this would obvious incur a significant slowdown. (See for example 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should probably be cited.) As it stands, the experimental results do not clearly distinguish the fuzzy paraphrase approach from prior work, i.e. tables 3 and 4 do not show major trends one way or the other.----------------Regarding the second question, it is hard to draw many conclusions from analogy tasks alone, especially when effects unrelated to good/bad paraphrasing such as corpus size/content, window size, vocabulary size, etc., can have an outsize effect on performance. ----------------Overall, I think this is a good paper presenting a sensible idea, but I am not convinced by the experiments that the specific approach is achieving its goal. With some improved experiments and analysis, I would wholeheartedly recommend this paper for acceptance; as it stands, I am on the fence.""]",0.947265625,0.052581787109375,1,0.88134765625,0.11846923828125,1,0.55517578125,0.444580078125,1,0.92431640625,0.075927734375,1,0.259521484375,0.740234375,0,0.392578125,0.607421875,0
79,79,79,79,79,79,https://openreview.net/forum?id=r1rhWnZkg,"---- Sect 2, first sentence: “every pairs” -> “every pair”



----Summary:
---While the paper provides a new best performance and an interesting interpretation of Hadamard product, to be a strong paper, either a more theoretical analysis of the properties of this approximation is required or a corresponding experimental evaluation..---This formulation is evaluated on the visual question answering (VQA) task together with several other model variants..-Strength:
---1.

The small difference in performance (0.44% om Table 1) could easily be attributed to these differences..-Weaknesses/Suggestions:


-1..-3.","['Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.----------------Strengths:----------------1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).----------------3. The various design choices made in model development have been experimentally verified. ----------------Weaknesses/Suggestions:----------------1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).----------------2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? ----------------3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.----------------4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.----------------5. In the caption for Table 1, fix the following: “have not” -> “have no” ----------------Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling.', 'Results on the VQA task are good for this simple model, the ablation study of table 1 gives some insights as to what is important. ----------------Missing are some explanations about the language embedding and the importance in deciding embedding dimension and final output dimension, equivalent to deciding the projected dimension in the compact bilinear model. Since the main contribution of the--------paper seems to be slightly better performance with fairly large reduction in parameters vs. compact bilinear something should be said about choice of those hyper parameters. If you increase embedded and output dimensions to equalize parameters to the compact bilinear model are further gains possible?  How is the question encoded? Is word order preserved in this encoding, the compact bilinear model compared to in table 1 mentions glove, the proposed model is using this as well? The meaning of visual attention in this model along with the number of glimpses should be tied to the sentence embedding, so now we are looking at particular spatial components when that part of the sentence is encoded, then we stack according to your equation 9?', 'This work proposes to approximate the bilinear pooling (outer product) with a formulation which uses the Hadamard Product (element-wise product). --------This formulation is evaluated on the visual question answering (VQA) task together with several other model variants.----------------Strength:--------1. The paper discusses how the Hadamard product can be used to approximate the full outer product.--------2. The paper provides an extensive experimental evaluation of other model aspect for VQA.--------3. The full model archives a slight improvement over prior state-of-the-art on the challenging and large scale VQA challenge.----------------Weaknesses:--------1. Novelty: The paper presents only a new “interpretation” of the Hadamard product which has previously been widely used for pooling, including for VQA.--------2. Experimental evaluation:--------2.1. An experimental direct comparison with MCB missing. Although the evaluated model is similar to Fukui et al. several other changes have been made, including question encoding (GRU vs. LSTM), normalization (tanh vs. L2 vs. none). The small difference in performance (0.44% om Table 1) could easily be attributed to these differences.--------2.2. An experimental comparison to the full outer product (e.g. for a lower dimension) is missing. It remains unclear how good the proposed approximation for the full outer product is. While a comparison to MCB is presented this seems insufficient as MCB is a very different model.--------2.3. One of the most important hyper parameters for the Hadamard Product seems to be the dimension of the lower dimensional embedding d. What effect does changing this have?--------2.4. Comparison with other pooling strategies, e.g. elementwise sum instead of elementwise product.--------3. No theoretical analysis or properties of the approximation are presented.--------4. The paper seems to be general at the beginning, but the claim of the benefit of the Hadamard product is only shown experimentally on the VQA dataset.--------5. Related work: The comparison to the related works in the appendix should at least be mentioned in the main paper, even if the details are the supplemental.------------------------Minor--------- It is not clear why the Lu et al, 2015 is cited rather than the published paper from Antol et al.--------- Sect 2, first sentence: “every pairs” -> “every pair”------------------------Summary:--------While the paper provides a new best performance and an interesting interpretation of Hadamard product, to be a strong paper, either a more theoretical analysis of the properties of this approximation is required or a corresponding experimental evaluation. It is a bit unfortunate that most of the experimental evaluation is not about the main claim of the paper (the Hadamard product) but of unrelated aspects which are important to achieve high performance in the VQA challenge.----------------To be more convincing I would like to see the following experiments --------- Comparison with Outer product in the identical model--------- Comparison with MCB in the identical model--------- Comparison with elementwise sum instead of elementwise product--------- One of the most important hyper parameters for the Hadamard Product seems to be the dimension of the lower dimensional embedding d. What effect does changing this have?']",0.406494140625,0.59375,0,0.68896484375,0.31103515625,1,0.326904296875,0.6728515625,0,0.73193359375,0.268310546875,1,0.0948486328125,0.9052734375,0,0.2327880859375,0.76708984375,0
80,80,80,80,80,80,https://openreview.net/forum?id=r1te3Fqel,"-Finally, although the method is shown to outperform the baseline method reported in the original paper introducing the SQuAD dataset, it currently seems to be 12th (out of 15 systems) on the leaderboard (https://rajpurkar.github.io/SQuAD-explorer/)..there is nothing architecturally novel about it, as is for instance the case with the Gated Attentive Reader)..-Therefore, given the lack of model novelty (based on my understanding), and the lack of strong results (based on the leaderboard), I don't feel the paper is ready in its current form to be accepted to the conference.

It is similar in spirit to several other recent models, with the main exception that it is able to predict answers of different lengths, as opposed to single words/tokens/entities..However, both approaches considered -- using a POS pattern trie tree to filter out word sequences with POS tags matching those of answers in the training set, and brute-force enumeration of all phrases up to length N -- seem somewhat orthogonal to the idea of ""learning end-to-end "" an answer chunk extraction model..-Note: The GRU citation should be (Cho et al., 2014), not (Bengio et al., 2015).","['SYNOPSIS: The paper proposes a new neural network-based model for reading comprehension (reading a passage of text and answering questions based on the passage). It is similar in spirit to several other recent models, with the main exception that it is able to predict answers of different lengths, as opposed to single words/tokens/entities. The authors compare their model on the Stanford Question Answering Dataset (SQuAD), and show improvements over the baselines, while apparently lagging quite far behind the current state of the art reported on the SQuAD leaderboard.----------------THOUGHTS: The main novelty of the method is to be able to identify phrases of different lengths as possible answers to the question. However, both approaches considered -- using a POS pattern trie tree to filter out word sequences with POS tags matching those of answers in the training set, and brute-force enumeration of all phrases up to length N -- seem somewhat orthogonal to the idea of ""learning end-to-end "" an answer chunk extraction model. Furthermore, as other reviews have pointed out, it seems that the linguistic features actually contribute a lot to the final accuracy (Table 3). One could argue that these are easy to obtain using standard taggers, but it takes away even more from the idea of an ""end-to-end trained"" system.----------------The paper is generally well written, but there are several crucial sections in parts describing the model where it was really hard for me to follow the descriptions. In particular, the attention mechanism seems fairly standard to me in a seq2seq sense (i.e. there is nothing architecturally novel about it, as is for instance the case with the Gated Attentive Reader). I may be missing something, but even after the clarification round I still don\'t understand how it is novel compared to standard attention used in for instance seq2seq models.----------------Finally, although the method is shown to outperform the baseline method reported in the original paper introducing the SQuAD dataset, it currently seems to be 12th (out of 15 systems) on the leaderboard (https://rajpurkar.github.io/SQuAD-explorer/). Of course, it may be that further training and hyperparameter optimizations may improve these results.----------------Therefore, given the lack of model novelty (based on my understanding), and the lack of strong results (based on the leaderboard), I don\'t feel the paper is ready in its current form to be accepted to the conference.----------------Note: The GRU citation should be (Cho et al., 2014), not (Bengio et al., 2015).', 'The paper proposed an end-to-end machine learning model called dynamic reader for the machine reading comprehension task. Compared to earlier systems, the proposed model is able to extract and rank a set of answer candidates from a given document.----------------There are many recent models focusing on building good question answering systems by extracting phrases from a given article. It seems that there are two different aspects that are unique in this work:----------------1. The use of convolution model, and--------2. Dynamic chunking----------------Convolution network is often only used for modeling character-based word embeddings so I am curious about its effectiveness on representing phrases. Therefore, I wish there could be more analysis on how effective it is, as the authors do not compare the convolution framework to other alternative approaches such as LSTM. The comparisons are important, as the authors uses uni-gram, bi-gram and tri-gram information in the convolution network, and it is not clear to me that if tri-gram information is still needed for LSTM models.----------------The dynamic chunking is a good idea, and a very similar idea is proposed in some of the recent papers such as [Kenton et al, 16], which also targets at the same dataset. However, I would like to see more analysis on the dynamic chunking. Why this approach is a good approach for representing answer chunks? Given the representation of the chunk is constructed by the first and the end word representations generated by a convolution network, I am not sure about the ability of this representation to capture the long answer phrases.----------------The authors do not use character base embedding but use some of the previous trained NLP models. It would be interesting if the authors could show what are the advantages and disadvantages of using linguistic features compared to character embeddings.----------------In short, there are several good ideas proposed in the paper, but the lack of proper analysis make it difficult to judge how important the proposed techniques are.', 'SUMMARY.--------The paper propose a reading-comprehension question answering system for the recent QA task where answers of a question can be either single tokens or spans in the given text passage.--------The model first encodes the passage and the query using a recurrent neural network.--------With an attention mechanism the model calculates the importance of each word on the passage with respect to each word in the question.--------The encoded words in the passage are concatenated with the attention; the resulting vector is re-encoded with a further RNN.--------Three convolutional neural networks with different filter size (1,2,3-gram) are used to further capture local features.--------Candidate answers are selected either matching POS patterns of answers in the training set or choosing all possible text span until a certain length.--------Each candidate answer has three representations, one for each n-gram representation. The compatibility of these representation with the question representation is then calculated.--------The scores are combined linearly and used for calculating the probability of the candidate answer being the right answer for the question.----------------The method is tested on the SQUAD dataset and outperforms the proposed baselines.------------------------------------------OVERALL JUDGMENT--------The method presented in this paper is interesting but not very motivated in some points.--------For example, it is not explained why in the attention mechanism it is beneficial to concatenate the original passage encoding with the attention-weighted ones.--------The contributions of the paper are moderately novel proposing mainly the attention mechanism and the convolutional re-encoding.--------In fact, combining questions and passages and score their compatibility has became a fairly standard procedure in all QA models.------------------------------------------DETAILED COMMENTS----------------Equation (13) i should be s, not s^l.----------------I still do not understand the sentence "" the best function is to concatenate the hidden stat of the fist word in a chunk in forward RNN and that of the last word in backward RNN"". The RNN is over what all the words in the chunk? in the passage? --------The answer the authors gave in the response does not clarify this point.']",0.8330078125,0.1669921875,1,0.8984375,0.10162353515625,1,0.430419921875,0.56982421875,0,0.92138671875,0.0784912109375,1,0.1400146484375,0.85986328125,0,0.3642578125,0.6357421875,0
81,81,81,81,81,81,https://openreview.net/forum?id=r1xUYDYgg,"cuDNN on standard (e.g..-Benchmarking on high-end compute server hardware is an interesting point of reference, but the questions that come to mind for me when reading this paper are


-(1) How would this fit into a live-video processing application on a mobile device
---(2) What kind of a “cluster” would this present to someone trying to do distributed deep learning in the wild by drawing on idle graphics cards: how much memory do they have, how might we handle data for training on such computers, what is the compute speed vs. communication latency and bandwidth..The performance of sukiyaki2 vs AMD's Caffe port is impressive.

Validity:
---The presented work seems technically valid..Code for matrix library sushi2 and DL library sukiyaki2 are on github, including live demos that run in your browser..-https://mil-tokyo.github.io/sukiyaki2/examples/mnist/ was fun, but seemed very slow (5 mnist images per second).","[""Validity:--------The presented work seems technically valid. Code for matrix library sushi2 and DL library sukiyaki2 are on github, including live demos that run in your browser.----------------https://mil-tokyo.github.io/sukiyaki2/examples/mnist/ was fun, but seemed very slow (5 mnist images per second). The demo page would be more interesting if it showed what model was being trained, which implementation was being used (pure js or webcl?), which hardware was being used for the computation, and how that compared with other people who logged into the page. As it is, the demo is kind of unclear as to what is happening.----------------Relevance:----------------The grand vision of a DLTraining@Home is exciting. While much work remains, having a solid WebCL foundation seems valuable. The big advantage of javascript is that it runs everywhere, especially on idle desktops and laptops around the world. However, these sorts of computers do not (with probability 1) have K80 or S9120 video cards. Instead, they have a wide variety of every consumer-grade card ever sold, which call for different blocking, tiling, and looping strategies in the computational kernels that underpin deep learning inference and training algorithms (hence, autotuning), which isn't discussed.----------------Sushi2 and Sukiyaki2 seem relatively young as projects. They are not widely followed on github, there is no tutorial-style documentation for Sukiyaki2, and the implementations of e.g. convolution do not seem to have seen much engineering work. Speed of evaluation seems to be one of the main focal points of the paper, but it’s not a major selling point to the ICLR audience because it seems about ¼ as fast as e.g. cuDNN on standard (e.g. AWS nodes) NVidia hardware. The performance of sukiyaki2 vs AMD's Caffe port is impressive.----------------Benchmarking on high-end compute server hardware is an interesting point of reference, but the questions that come to mind for me when reading this paper are----------------(1) How would this fit into a live-video processing application on a mobile device--------(2) What kind of a “cluster” would this present to someone trying to do distributed deep learning in the wild by drawing on idle graphics cards: how much memory do they have, how might we handle data for training on such computers, what is the compute speed vs. communication latency and bandwidth.----------------Answers to these questions are out of scope for this paper, but it would have been interesting to see at least some preliminary discussion.----------------Novelty:--------I’m not aware of a more mature WebCL-based HPC library.----------------Presentation:--------Table 1 is hard to read because it is actually two tables with different formatting, and the numbers (speeds?) aren’t labeled with units."", ""While it is interesting that this can be done, and it will be useful for some, it does seem like the audience is not really the mainstream ICLR audience, who will not be afraid to use a conventional ML toolkit. --------There is no new algorithm here, nor is there any UI/meta-design improvement to make it easier for non-experts to design and train neural network systems. ----------------I think there will be relatively little interest at ICLR in such a paper that doesn't really advance the state of the art. --------I have no significant objection to the presentation or methodology of the paper. "", 'This paper presents a JavaScript framework including WebCL components for training and deploying deep neural networks. The authors show that it is possible to reach competitive speeds with this technology, even higher speed than a compiled application with ViennaCL on AMD GPUs. While remaining a little more than factor three slower than compiled high performance software on NVIDIA GPUs, it offers compelling possibilities for easily deployable training and application settings for deep learning.----------------My main points of criticism are:--------1. In Tab. 4 different batch sizes are used. Even if this is due to technical limits for the Javascript library, it would only be fair to use the smaller batch sizes for the other frameworks as well (on the GPUs probably in favor of the presented framework).----------------2. In Fig. 6, why not include more information in the graphs? Especially, as stated in the question, why not include the node.js values? While I do see the possible application with one server and many ""low performance"" clients, the setting of having a few dedicated high performance servers is quite likely. Even if not, these are good values to compare with. For the sake of consistency, please include in both subfigures Firefox, Chrome, node.js.----------------Apart from these points, well-written, understandable and conclusive.']",0.86767578125,0.1324462890625,1,0.8330078125,0.1668701171875,1,0.2042236328125,0.7958984375,0,0.8388671875,0.1612548828125,1,0.0582275390625,0.94189453125,0,0.30322265625,0.69677734375,0
82,82,82,82,82,82,https://openreview.net/forum?id=r1y1aawlg,"It is introduced as the model that is used to refine an existing hypothesis, but it is not immediately clear that the training data for this model (at least in this section) are the gold standard translations- “training set” could be interpreted in variety of ways..that **iterative** improvement is beneficial -- has been satifactorily demonstrated it would be necessary to include stronger baselines - and in particular, to show that an iterative refinement scheme can really improve over a system closely matched to the attention-based model, both when used in isolation and when used in system combination with a PBMT system, and to demonstrate that the PBMT system is not simply acting as a regulariser for the attention-based model..“distributional” suggests that the representations are derived from how the words are distributed in the corpus, whereas you are learning these representations on this task which isn’t modeling their distribution except only very indirectly.

---- I don't understand footnote 1..This paper proposes a method for iteratively improving the output of an existing machine translation by identifying potential mistakes and proposing a substitution, in this case using an attention-based model..While this is an interesting area of research, I am not convinced by the proposed approach, and experimental evidence is lacking.","['This paper proposes a method for iteratively improving the output of an existing machine translation by identifying potential mistakes and proposing a substitution, in this case using an attention-based model.  It is motivated by the method in which (it is assumed) human translators operate.----------------The paper is interesting and imaginative.  However, in general terms, I am somewhat sceptical of this kind of approach -- whereby a machine learning method is used to identify and correct the predictions of another method, or itself -- because in the first case, if the new method is better, why not use it from the outset in place of the other method?  And in the second case, since the method has no new information compared to previously, why is it more likely to identify more past mistakes and correct them, than identify past correct terms and turn them into new errors?  That is unless there is a specific reason that an iterative approach can be shown to converge to a better solution when run over several epochs.----------------This paper does not convince me on these points.  Indeed, unsurprisingly, the authors note that ""the probability of correctly labelling a word as a mistake remains low (62%)"" - this admittedly beats a random-chance baseline, but is not compared to something more meaningful, such as simply contrasting the existing system with a more powerful convolutional model and labelling all discrepancies as mistakes.  The oracle experiments are rather meaningless - they just serve to confirm that improving a translation is very easy when the existing mistakes have been identified, but much harder when they are not. ----------------Although I do like the paper on the whole, to really convince me that main objective -- ie. that **iterative** improvement is beneficial -- has been satifactorily demonstrated it would be necessary to include stronger baselines - and in particular, to show that an iterative refinement scheme can really improve over a system closely matched to the attention-based model, both when used in isolation and when used in system combination with a PBMT system, and to demonstrate that the PBMT system is not simply acting as a regulariser for the attention-based model.----------------Minor comments:----------------I find the notation excessively fiddly at times - eg F^i = (F^{i,1}, F^{i,|F^i|}) - why use |F^i| here when F is a matrix, so surely the length of the slice is not dependent on i?----------------In the discussion in section 4 - it seems that this still creates a mismatch between the training and test conditions - could anything be done about this?------------------------------------------------------------------------ ', 'This paper proposes a model for iteratively refining translation hypotheses. This has several benefits, including enabling the translation model to condition not only on “left context”, but also on “right context”, and potentially enabling more rapid and/or accurate decoding. The motivation given is that often translators (and text generators generally) use a process of refinement in generating outputs.----------------This is an important idea that is not currently playing much of a role in neural net models, so this paper is a welcome contribution. However, while I think this is an important first step, I do feel that the lack of in depth analysis suggests this paper is not quite ready for a final publication version. For example, there are many possible connections to prior work in NLP, MT, and other parts of ML that could better contextualize this work (see specifics below). More substantively, the model in Section 3 could be interpreted as a globally normalized, undirected (~CRF) translation model trained using a pseudo-likelihood objective. In this analysis, the model squarely back in the context of traditional discriminative translation models which used “undirected” features, and the decoding algorithm then looks more like a standard greedy hill-climbing algorithm (albeit with an extra heuristic model for selecting which variable to update), which is also nothing unfamiliar.----------------My second criticism the limitations of the model are not well discussed. For example, the proposed editing procedure cannot obviously remove or insert a word from a translation. While I think this is a reasonable assumption than can be made for the sake of tractability, it is very unfortunate since missing or extra words (esp. function words) are a common problem in the baseline models that are being used. Second, the standard objections to absolute positional models (vs. relative positional models) seem particularly crucial to bring up in this work, especially since they might make some of the design decisions a bit more justifiable.----------------Overall, this is an initial step in an interesting direction, but it needs more thorough analysis to demonstrate its value. A more thorough analysis will also likely suggest some important model variants (for example: is a global translation model really the goal? or is a post-editing model that fixes outputs with more complex operations more ideal?)----------------Related work:--------I think that more could be done to put this work in the context of what has come before and what is currently going on in other parts of ML. The idea of iterative refinement has been proposed in other problems that have complex output spaces, for example the DRAW model of Gregor et al. and the conditional adversarial network models used to refine images proposed recently by Isola et al. In NLP, there have been several (stochastic) hill climbing approaches that have been proposed, such as the work on parsing by Zhang and Lei et al. (2014) who use random initial guesses and then do greedy hill climbing using a series of local refinements, the structured prediction cascades of Weiss and Taskar (2009) (not to mention general coarse-to-fine modeling strategies). Finally, in MT, Arun et al. (2009) who use a Gibbs sampler to refine an initial guess to do decoding with a more complex model. The use of an explicit error model is rather novel in the context of correction, but I would point out that although the proposed architecture is different, the discriminative word lexicon models of Mauser et al. (2009) and the neural version of the same by Ha et al. (2014) are similar in spirit. There have also been a number of papers on “automatic post editing”, including the shared task at WMT2016, and there are not only standard test sets and baselines, but also datasets that could actually be used to train a post editing model with human-generated data. Minimally using the techniques they described could be a useful foil for the models presented in this paper.----------------“the target sentence is also embedded in distributional space via a lookup table” I think “distributional space” is a bit unclear. Maybe “the target sentence is represented in terms of distributed word representations via a lookup table” or something like that. “distributional” suggests that the representations are derived from how the words are distributed in the corpus, whereas you are learning these representations on this task which isn’t modeling their distribution except only very indirectly.----------------Section 3 Model: In Section 3, the model computes the distribution over target word types at an absolute position i in the output sentence, given the target language context and the source language context. It is introduced as the model that is used to refine an existing hypothesis, but it is not immediately clear that the training data for this model (at least in this section) are the gold standard translations- “training set” could be interpreted in variety of ways. This becomes clearer when reading later in the paper, but it’s a bit less clear when reading from the beginning for the first time.----------------The use of a fixed sized window for representing the target word in context also seems to make something like a model 1 assumption since only the lexical features (and not any “alignment” or “positional” features) determine the attention. This should be clarified since it will make the assumptions of the model more transparent (and also suggest possible refinements to the model, e.g., including (representations) of i and j as components of S^j and T^i, which would allow model 2/3-like responses to be learned- although by leaving them out, the model might behave a bit more like a relative positional model than an absolute positional model, which is probably attractive).----------------Finally, some discussion for why a fixed window is used to represent the target sentence is worth including (since a global context is apparently used to represent the source sentence).----------------The relationship between this training objective and pseudo likelihood (PL; Besag, 1975) might be worth mentioning. Since I believe this is just a PL objective for a certain global model, this suggests alternative decoding algorithms, or certainly a different analysis of the proposed decoding objective.----------------The section 4 model conditions on the true context of a position in the true target, the current target guess, and the source. I don’t completely understand the rationale for this model since at test time only two of these variables are available, and the replacement of y_ref with y_g seems hard to justify.', 'Disclosure: I am not an expert in machine translation algorithms.----------------Summary: A human translator does not come up with the final translation right--------away. Instead, (s)he uses an iterative process, starting with a rough draft--------which is corrected little by little. The idea behind this paper is to--------implement a similar framework for an automated system. ----------------This paper is generally well written. ----------------It is my opinion however that drawings illustrating the architectures would help--------understanding how the different algorithms relate to one another.----------------I like a lot that you report on a preliminary experiment to give an--------intuition of how difficult the task is. You should highlight the links--------between the task of finding the errors in a guess translation and the task--------of iterative refinement. Could you use post-edited text to have a more--------solid ground-truth?----------------My main concern with this paper is that in the experimental section the --------iterative approach tries to improve upon only one type of machine translation. --------Which immediately prompts these questions:--------- why did they choose that approach to improve on?--------- what is the part of the improvement that comes from the choice of the--------  initial draft (maybe it was a very bad draft)? ----------------Here are some minor typos:--------- p.2: ... a lookup table that replace*S* each word... ?--------- p.3: I might be mistanken but it seems to me that j is used for two--------  different things. It is confusing.--------- p.3: ...takes as input these representation*S* and outputs... ?', 'This work proposes to iteratively improve a sentence that has been generated from another MT system (in this case, a phrase-based system). The authors use a neural net that takes in the source sentence and a window of (gold) words around the current target word, and predicts the current target word. During testing, the gold words are replaced with the generated words. While this is an interesting area of research, I am not convinced by the proposed approach, and experimental evidence is lacking.----------------Under the current framework, it is all but impossible for the model to do anything more than a rudimentary word replacement (e.g. it cannot change ""I went to the fridge even though I was not hungry"" to ""Although I was not hungry, I went to the fridge""). The fact that only 0.6 words are edited on average supports this. ----------------Specific comments:--------- It would be interesting to see what the improvements are if the baseline model is a neural system.--------- It seems strange (to me at least) that T^i and L(y^{-i|k}) only look at a window of 2k words. It means that when making the decision to change the i-th word, the model does not know what was generated outside of the window? --------- Relatedly, the idea of changing individual words based on local (i.e. word-level) scores seems counterintuitive. Given that we have the full generated sentence, don\'t we want a global score? Scoring at the sentence-level could also make room for non-greedy search strategies, which could potentially facilitate richer edits.--------- How does the approach compare to a model that simply re-ranks the k-best output?--------- Instead of editing, did you consider learning an encoder-decoder that takes in x, y_g, and generates y_ref? When decoding you can attend to both x and y_g.----------------Minor comments:--------- Iteratively improving a generated text was also explored in https://arxiv.org/pdf/1510.09202v1.pdf from a reinforcement learning angle.--------- I don\'t understand footnote 1.']",0.82763671875,0.172119140625,1,0.95068359375,0.049072265625,1,0.30224609375,0.69775390625,0,0.908203125,0.0919189453125,1,0.2127685546875,0.787109375,0,0.56298828125,0.436767578125,1
83,83,83,83,83,83,https://openreview.net/forum?id=rJ0JwFcex,"More discussion on this would be appreciated..At the moment, the results are not particularly reproducible..Not sure if I missed something.

In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future..Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer..So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt…).","[""The paper presents a method to synthesize string manipulation programs based on a set of input output pairs. The paper focuses on a restricted class of programs based on a simple context free grammar sufficient to solve string manipulation tasks from the FlashFill benchmark. A probabilistic generative model called Recursive-Reverse-Recursive Neural Network (R3NN) is presented that assigns a probability to each program's parse tree after a bottom-up and a top-down pass. Results are presented on a synthetic dataset and a Microsoft Excel benchmark called FlashFill.----------------The problem of program synthesis is important with a lot of recent interest from the deep learning community. The approach taken in the paper based on parse trees and recursive neural networks seems interesting and promising. However, the model seems too complicated and unclear at several places (details below). On the negative side, the experiments are particularly weak, and the paper does not seem ready for publication based on its experimental results. I was positive about the paper until I realized that the method obtains an accuracy of 38% on FlashFill benchmark when presented with only 5 input-output examples but the performance degrades to 29% when 10 input-output examples are used. This was surprising to the authors too, and they came up with some hypothesis to explain this phenomenon. To me, this is a big problem indicating either a bug in the code or a severe shortcoming of the model. Any model useful for program synthesis needs to be applicable to many input-output examples because most complicated programs require many examples to disambiguate the details of the program.----------------Given the shortcoming of the experiments, I am not convinced that the paper is ready for publication. Thus, I recommend weak reject. I encourage the authors to address the comments below and resubmit as the general idea seems promising.----------------More comments:----------------I am unclear about the model at several places:--------- How is the probability distribution normalized? Given the nature of bottom-up top-down evaluation of the potentials, should one enumerate over different completions of a program and the compare their exponentiated potentials? If so, does this restrict the applicability of the model to long programs as the enumeration of the completions gets prohibitively slow?--------- What if you only use 1 input-output pair for each program instead of 5? Do the results get better?--------- Section 5.1.2 is not clear to me. Can you elaborate by potentially including some examples? Does your input-output representation pre-supposes a fixed number of input-output examples across tasks (e.g. 5 or 10 for all of the tasks)?----------------Regarding the experiments,--------- Could you present some baseline results on FlashFill benchmark based on previous work?--------- Is your method only applicable to short programs? (based on the choice of 13 for the number of instructions)--------- Does a program considered correct when it is identical to a test program, or is it considered correct when it succeeds on a set of held-out input-output pairs?--------- When using 100 or more program samples, do you report the accuracy of the best program out of 100 (i.e. recall) or do you first filter the programs based on training input-output pairs and then evaluate a program that is selected?----------------Your paper is well beyond the recommended limit of 8 pages. please consider making it shorter."", 'This paper sets out to tackle the program synthesis problem: given a set of input/output pairs discover the program that generated them. The authors propose a bipartite model, with one component that is a generative model of tree-structured programs and the other component an input/output pair encoder for conditioning. They consider applying many variants of this basic model to a FlashFill DSL. The experiments explore a practical dataset and achieve fine numbers. The range of models considered, carefulness of the exposition, and basic experimental setup make this a valuable paper for an important area of research. I have a few questions, which I think would strengthen the paper, but think it\'s worth accepting as is.----------------Questions/Comments:----------------- The dataset is a good choice, because it is simple and easy to understand. What is the effect of the ""rule based strategy"" for computing well formed input strings?----------------- Clarify what ""backtracking search"" is? I assume it is the same as trying to generate the latent function? ----------------- In general describing the accuracy as you increase the sample size could be summarize simply by reporting the log-probability of the latent function. Perhaps it\'s worth reporting that? Not sure if I missed something.', 'This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel.  The approach is to model successive “extensions” of a program tree conditioned on some embedding of the input/output pairs.  Extension probabilities are computed as a function of leaf and production rule embeddings — one of the main contributions is the so-called “Recursive-Reverse-Recursive Neural Net” which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way).----------------There are many strong points about this paper.  In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future.  The R3NN idea is a good one and the authors motivate it quite well.  Moreover, the authors have explored many variants of this model to understand what works well and what does not.  Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read.  Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer.  And it’s unclear why the authors did not simply train on longer programs…  It also seems that the number of I/O pairs is fixed?  So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt…).  Overall however, I would certainly like to see this paper accepted at ICLR.----------------Other miscellaneous comments:--------* Too many e’s in the expansion probability expression — might be better just to write “Softmax”.--------* There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see).--------* The authors claim that using hyperbolic tangent activation functions is important — I’d be interested in some more discussion on this and why something like ReLU would not be good.--------* It’s unclear to me how batching was done in this setting since each program has a different tree topology.  More discussion on this would be appreciated.  Related to this, it would be good to add details on optimization algorithm (SGD?  Adagrad?  Adam?), learning rate schedules and how weights were initialized.  At the moment, the results are not particularly reproducible.--------* In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs?  Or was it some other reason?)--------* There is a missing related work by Piech et al (Learning Program Embeddings…) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs).']",0.8671875,0.1326904296875,1,0.96337890625,0.0364990234375,1,0.54541015625,0.45458984375,1,0.90478515625,0.0953369140625,1,0.106201171875,0.89404296875,0,0.27197265625,0.72802734375,0
84,84,84,84,84,84,https://openreview.net/forum?id=rJPcZ3txx,"-The second contribution is perhaps more interesting, as so far pruning methods have focused on saving memory, with very modest speed gains..-The first contribution is more about engineering, but the authors make the source code available which is greatly appreciated..-Other areas for improvement: The points in Figure 4 are hard to distinguish (e.g.

The paper details an implementation of sparse-full convolutions and a model to work out the potential speed-up of various sparsity levels for CNNs..The authors are very methodical in how they build the model and evaluate it very thoroughly..It is upto the area chairs to decide how well such a paper fits in at ICLR.","['The paper details an implementation of sparse-full convolutions and a model to work out the potential speed-up of various sparsity levels for CNNs.----------------The first contribution is more about engineering, but the authors make the source code available which is greatly appreciated.----------------The second contribution is perhaps more interesting, as so far pruning methods have focused on saving memory, with very modest speed gains. Imbuing knowledge of running speed into a pruning algorithm seems like the proper way to tackle this problem. The authors are very methodical in how they build the model and evaluate it very thoroughly.----------------It seems that the same idea could be used not just for pruning existing models, but also when building new architectures: selecting layers and their parameters as to achieve an optimal throughput rate. This could make for a nice direction for future work.----------------One point that is missing is some discussion of how transferable the performance model is to GPUs. This would make the technique easier to adopt broadly.----------------Other areas for improvement: The points in Figure 4 are hard to distinguish (e.g. small red circle vs. small red square), and overall the figure could be made bigger; specifying whether the ""base learning rate"" in Section 3 is the start or end rate of the annealing schedule; typos: ""punning"" (p.4), ""spares"" (p.5).', 'The authors provide a well engineered solution to exploiting sparsity in convolutional layers of a deep network by recasting it as sparse matrix-vector multiplication. This leads to very nice speedups and the analysis of when this is possible is also useful for practitioners. My main concern with this paper is that the ""research"" aspect of it seems rather minimal, and it\'s mostly about performance engineering and comparisons. It is upto the area chairs to decide how well such a paper fits in at ICLR.', 'This paper tackles the problem of compressing trained convnets with the goal of reducing memory overhead and speeding up the forward pass. As I understand it, the main contribution of this work is to develop fast convolution routines for sparse conv weights int he case of general sparsity (as compared with structured sparsity). They evaluate their method on both AlexNet and GoogLeNet as well as on various platforms. The authors make code available online. The paper is well written and does a good job of putting this work in the context of past model reduction techniques.----------------My main request of the authors would be to provide a concise summary of the speedup/memory gains achievable with this new work compared with previously published work. The authors do show the various sparsity level obtained with various methods of pruning but it is unclear to me how to translate the information given in the paper into an understanding of gains relative to other methods.']",0.80712890625,0.19287109375,1,0.806640625,0.193115234375,1,0.1351318359375,0.86474609375,0,0.896484375,0.10333251953125,1,0.244873046875,0.7548828125,0,0.396484375,0.603515625,0
85,85,85,85,85,85,https://openreview.net/forum?id=rJXTf9Bxg,"Furthermore, my understanding is that when evaluating discriminability the authors downsample and then bicubically upsample the image, which is much more like a blurring, very different from retraining all the models to work on low resolution in the first place..---(2) In section 3 the AC-GAN classification objective (omitting expectation for brevity) is given as L_S = log P(C=c|X_real) + log P(C=c|X_fake) and you say that both the discriminator and generator are trained to maximize this quantity..In its behaviour, MS-SSIM is not that dissimilar from Euclidean distance - although it is nonlinear and is bounded between -1 and 1.

2016..-To my knowledge training GANs on large, diverse images such as 128 x 128 ImageNet images is under-explored ([1] contains just a few samples in this setting)..Though the model is not very novel and a comparison to other class-conditional models is lacking, I feel the community will find the diagnostic tools and the thorough exploration of the ImageNet-trained model to be of interest.","['Apologies for the late review.----------------This submission proposes method for class-conditional generative image modeling using auxiliary classifiers. Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution. The discriminator has two outputs and two corresponding objectives: determine whether a sample is real or generated, and independently to predict the (real or sampled) class label corresponding to the sample.----------------Figure 2. nicely illustrates related methods - this particular method bears similarities to InfoGANs and Semi-supervised GANs. Compared to infogans, this method also encourages correspondence between the latent c and the real class labels for the real examples (whereas infogans are presented as fully unsupervised).----------------The authors attempt at evaluating the method quantitatively by looking at the discriminability and diversity of samples. It is found - not surprisingly - that higher resolution improves discriminability (because more information is present).----------------Discriminability: Figure 3 doesn’t have legends so it is a bit hard to understand what is going on. Furthermore, my understanding is that when evaluating discriminability the authors downsample and then bicubically upsample the image, which is much more like a blurring, very different from retraining all the models to work on low resolution in the first place.----------------Diversity: The authors try to quantitatively evaluate diversity of samples by measuring the average MS-SSIM between randomly selected pairs of points within each class. I think this method is significantly flawed and limited, for reasons mentioned in (Theis et al, 2015, A note on the evaluation…). In its behaviour, MS-SSIM is not that dissimilar from Euclidean distance - although it is nonlinear and is bounded between -1 and 1. Evaluating diversity/entropy of samples in high dimensions is very hard, especially if the distributions involved are non-trivial for example concentrated around manifolds. Consider for example a generative model which randomly samples just two images. Assuming that the MSSSIM between these two images is -1, this generative model can easily achieve an average MSSSIM score of 0, implying a conclusion that this model has more diversity than the training data itself. Conversely, SSIM is designed not to be sensitive to contrast and average pixel intensity, so if a model is diverse in this sense, that will be ignored by this measure.----------------Overall, the paper proposes a new way to incorporate class labels into training GAN-type models. As far as I know the particular algorithm is novel, but I consider it incremental compared to what has been done before. I think the proposed evaluation metrics are flawed, especially when evaluating the diversity of the samples for the aforementioned reasons.', 'This is a clear, easy to read, highly relevant paper that improves GAN training for images and explores evaluation criterion on GANs. The main contributions are as follows:--------- Adding an auxiliary classifier head to a GAN discriminator and training a classification objective in addition to the real/fake objective improves performance. Generator is conditioned on 1-hot encoding of class and is trained to generate the specified class.--------- Training different models on different subsets of imagenet classes improves performance.--------- They motivate evaluating GAN images by using a perceptual similarity metric (MS-SSIM) on pairs of samples to quantify diversity in the samples (and detect mode collapse)--------- They show this metric correlates with a discriminability metric (classification accuracy of pre-trained imagenet model on generated samples) .----------------The overall novelty of this approach is somewhat lacking in that previous methods have proposed training a classifier head on the discriminator and the discriminability metric proposed is simply the inception score of [1] except with class information. However, I think there is still a contribution to be made my putting these tricks together and successfully demonstrating image synthesis gains. ----------------Questions for the authors: --------(1) Why do you think splitting the imagenet training into 100 different models improves performance?  Is the issue with the representation of the class? In other words, if an encoding more meaningful that 1-hot vector was used do you still think 100 models would be needed. Ideally we should hope that a generative model can leverage information from different classes to help with the generation of a particular class and also text-image synthesis models [2] have been quite successful when trained on diverse datasets (and these are conditioned on a semantically meaningful text encoding) which suggests to be that the issue is with the representation. --------(2) In section 3 the AC-GAN classification objective (omitting expectation for brevity) is given as L_S = log P(C=c|X_real) + log P(C=c|X_fake) and you say that both the discriminator and generator are trained to maximize this quantity. Obviously the generator would want to maximize  log P(C=c|X_fake) for its given conditioning class c. But can you explain why you would want the discriminator to also maximize the classification accuracy of generated samples? Why not do something similar to the CatGAN paper [3] and train the discriminator to be as uncertain as possible about the generated examples. Seems counterintuitive to me to have both the generator and discriminator trying to optimize the same classification objective rather than not be adversarial wrt to this loss as well as the real/fake loss. ----------------Overall, this paper makes a clear contribution to GAN research both in terms of image quality and evaluation metrics and I would recommend it for acceptance. ----------------[1] Salimans et al. Improved Techniques for Training GANs (https://arxiv.org/abs/1606.03498)--------[2] Reed et al. Generative Adversarial Text to Image Synthesis (https://arxiv.org/abs/1605.05396)--------[3] Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial networks (https://arxiv.org/abs/1511.06390)', 'This paper introduces a class-conditional GAN as a generative model for images. It introduces two main diagnostic tools for training GANs: one to assess whether a model is making full use of its output resolution and another to measure the diversity of generated samples. Experiments are conducted on the CIFAR-10 and ImageNet datasets.----------------Pros:--------+ The paper is clear and well-written.--------+ Experiments performed in the relatively under-explored 128 x 128 ImageNet setting.--------+ The proposed MS-SSIM diversity metric appears to be a useful tool for detecting convergence issues in class-conditional GAN models.----------------Cons:--------- AC-GAN model itself is of limited novelty relative to other GAN approaches that condition on class.--------- Diversity metric is of limited use for training non class-conditional GANs.--------- No experimental comparison of AC-GAN to other class-conditional models.----------------To my knowledge training GANs on large, diverse images such as 128 x 128 ImageNet images is under-explored ([1] contains just a few samples in this setting). Though the model is not very novel and a comparison to other class-conditional models is lacking, I feel the community will find the diagnostic tools and the thorough exploration of the ImageNet-trained model to be of interest.----------------* Section 4.2: MS-SSIM is traditionally defined for grayscale images only. How do you extend MS-SSIM to color images in your work? Were they computed channel-wise across R,G, and B?--------* Section 4.4: It is difficult to tell whether a single AC-GAN was trained for all of CIFAR-10 or one for each group. If single, why were the samples split into groups for computing Inception Score? And if multiple, the comparison to Salimans et al. is not a direct one. Also it would be helpful to include the real data Inception score as a point of comparison.--------* Appendix D: The caption of Figure 9 states that the same number of training steps was taken for each model. From this it seems possible that the models with more classes simply did not converge yet.----------------[1] Salimans, Tim, et al. ""Improved techniques for training GANs."" Advances in Neural Information Processing Systems. 2016.']",0.9013671875,0.0985107421875,1,0.92138671875,0.07879638671875,1,0.43603515625,0.56396484375,0,0.892578125,0.1075439453125,1,0.0819091796875,0.91796875,0,0.2841796875,0.7158203125,0
86,86,86,86,86,86,https://openreview.net/forum?id=rJiNwv9gg,"-Now, as the authors argue, there is some value to the fact that this scheme was learned automatically rather than by expert design---which means it has benefits beyond the compression of natural images (e.g., it could be used to automatically learning a compression scheme for signals for which we don't have as much domain knowledge)..2016b appears to be incomplete, and there is no comparison to Balle et al..For what kinds of textures does it do better and for what kinds does it do worse (the paper could show the best and worst 10 patches at different bit-rates) ?

This work proposes a new approach for image compression using auto encoders..The results are impressive, besting the state of the art in this field..-Pros:
---+ Very clear paper.","[""This work proposes a new approach for image compression using auto encoders. The results are impressive, besting the state of the art in this field.----------------Pros:--------+ Very clear paper. It should be possible to replicate these results should one be inclined to do so.--------+ The results, when compared to other work in this field are very promising. I need to emphasize, and I think the authors should have emphasized this fact as well: this is very new technology and it should not be surprising it's not better than the state of the art in image compression. It's definitely better than other neural network approaches to compression, though.----------------Cons:--------- The training procedure seems clunky. It requires multiple training stages, freezing weights, etc.--------- The motivation behind Figure 1 is a bit strange, as it's not clear what it's trying to illustrate, and may confuse readers (it talks about effects on JPEG, but the paper discusses a neural network architecture, not DCT quantization)"", 'This paper proposes an autoencoder approach to lossy image compression by minimizing the weighted sum of reconstruction error and code length. The architecture consists of a convolutional encoder and a sub-pixel convolutional decoder. Experiments compare PSNR, SSIM, and MS-SSIM performance against JPEG, JPEG-2000, and a recent RNN-based compression approach. A mean opinion score test was also conducted.----------------Pros:--------+ The paper is clear and well-written.--------+ The decoder architecture takes advantage of recent advances in convolutional approaches to image super-resolution.--------+ The proposed approaches to quantization and rate estimation are sensible and well-justified.----------------Cons:--------- The experimental baselines do not appear to be entirely complete.----------------The task of using autoencoders to perform compression is important and has a large practical impact. Though directly optimizing the rate-distortion tradeoff is not an entirely novel enterprise, there are enough differences (e.g. the quantization approach and sub-pixel convolutional decoder) to sufficiently distinguish this from earlier work. I am not an image compression expert but the approach and results both seem compelling. The main shortcoming is that the implementation of Toderici et al. 2016b appears to be incomplete, and there is no comparison to Balle et al. 2016. Overall, I feel that the fact that this architecture achieves competitive performance with JPEG-2000 while simultaneously setting the stage for future work that varies the encoder/decoder size and data domain means the community will find this work to be of significant interest.----------------I have no further specific comments at this time as they were answered sufficiently in the pre-review questions.', ""The paper proposes a neural approach to learning an image compression-decompression scheme as an auto-encoder. While the idea is certainly interesting and well-motivated, in practice, it turns out to achieve effectively identical rates to JPEG-2000.----------------Now, as the authors argue, there is some value to the fact that this scheme was learned automatically rather than by expert design---which means it has benefits beyond the compression of natural images (e.g., it could be used to automatically learning a compression scheme for signals for which we don't have as much domain knowledge). However, I still believe that this makes the paper unsuitable for publication in its current form because of the following reasons-------------------1. Firstly, the fact that the learned encoder is competitive---and not clearly better---than JPEG 2000 means that the focus of the paper should more be about the aspects in which the encoder is similar to, and the aspects in which it differs, from JPEG 2000. Is it learning similar filters or completely different ones ? For what kinds of textures does it do better and for what kinds does it do worse (the paper could show the best and worst 10 patches at different bit-rates) ?----------------2. Secondly, I think it's crucial that the paper demonstrate that the benefits come from a better coding scheme (as opposed to just a better decoder), as suggested in my initial pre-review question. How would a decoder trained on JPEG-2000 codes (and perhaps also on encoded random projections) do worse or better ?----------------3. Finally, I think the fact that it does as well/worse than JPEG-2000 significantly diminishes the case for using a 'deep' auto-encoder. JPEG-2000 essentially uses a wavelet transform, which is a basis that past studies have shown could be recovered using a simple sparse dictionary algorithm like K-SVD. This is why I feel that the method needs to clearly outperform JPEG-2000, or show comparisons to (or atleast discuss) a well-crafted traditional/generative model-based baseline.""]",0.611328125,0.388671875,1,0.9482421875,0.051910400390625,1,0.0943603515625,0.90576171875,0,0.8720703125,0.128173828125,1,0.0972900390625,0.90283203125,0,0.352294921875,0.64794921875,0
87,87,87,87,87,87,https://openreview.net/forum?id=rJqFGTslg,"The authors do a very good job of thinking it through, and taking to the next level by studying pruning across different layers too..This result is important to speed up and compress neural networks while being able to use standard fully-connected linear algebra routines..There are two easy critical missing baselines of 1) randomly pruning filters, 2) pruning filters with low activation pattern norms on training set.

The idea of ""pruning where it matters"" is great..-Extra points for clarity of the description and good pictures..Even more extra points for actually specifying what spaces are which layers are mapping into which (\mathbb symbol - two thumbs up!).","['The idea of ""pruning where it matters"" is great. The authors do a very good job of thinking it through, and taking to the next level by studying pruning across different layers too.----------------Extra points for clarity of the description and good pictures. Even more extra points for actually specifying what spaces are which layers are mapping into which (\\mathbb symbol - two thumbs up!).----------------The experiments are well done and the results are encouraging. Of course, more experiments would be even nicer, but is it ever not the case?----------------My question/issue - is the proposed pruning criterion proposed? Yes, pruning on the filter level is what in my opinion is the way to go, but I would be curious how the ""min sum of weights"" criterion compares to other approaches.--------How does it compare to other pruning criteria? Is it better than ""pruning at random""?----------------Overall, I liked the paper.', 'This paper prunes entire groups of filters in CNN so that they reduce computational cost and at the same time do not result in sparse connectivity. This result is important to speed up and compress neural networks while being able to use standard fully-connected linear algebra routines. --------The results are a 10% improvements in ResNet-like and ImageNet, which may be also achieved with better design of networks. New networks should have been also compared, but this we know it is time-consuming.--------A good paper with some useful results.', 'This paper proposes a simple method for pruning filters in two types of architecture to decrease the time for execution.----------------Pros:--------- Impressively retains accuracy on popular models on ImageNet and Cifar10----------------Cons:--------- There is no justification for for low L1 or L2 norm being a good selection criteria. There are two easy critical missing baselines of 1) randomly pruning filters, 2) pruning filters with low activation pattern norms on training set.--------- There is no direct comparison to the multitude of other pruning and speedup methods.--------- While FLOPs are reported, it is not clear what empirical speedup this method gives, which is what people interested in these methods care about. Wall-clock speedup is trivial to report, so the lack of wall-clock speedup is suspect.', ""This paper proposes a very simple idea (prune low-weight filters from ConvNets) in order to reduce FLOPs and memory consumption. The proposed method is experimented on with VGG-16 and ResNets on CIFAR10 and ImageNet.----------------Pros:--------- Creates *structured* sparsity, which automatically improves performance without changing the underlying convolution implementation--------- Very simple to implement----------------Cons:--------- No evaluation of how pruning impacts transfer learning----------------I'm generally positive about this work. While the main idea is almost trivial, I am not aware of any other papers that propose exactly the same idea and show a good set of experimental results. Therefore I'm inclined to accept it. The only major downside is that the paper does not evaluate the impact of filter pruning on transfer learning. For example, there is not much interest in the tasks of CIFAR10 or even ImageNet. Instead, the main interest in both academia and industry is the value of the learned representation for transferring to other tasks. One might expect filter pruning (or any other kind of pruning) to harm transfer learning. It's possible that the while the main task has about the same performance, transfer learning is strongly hurt. This paper has missed an opportunity to explore that direction.----------------Nit: Fig 2 title says VGG-16 in (b) and VGG_BN in (c). Are these the same models?""]",0.87109375,0.12890625,1,0.9345703125,0.06561279296875,1,0.173095703125,0.82666015625,0,0.9140625,0.086181640625,1,0.086669921875,0.91357421875,0,0.320068359375,0.6796875,0
88,88,88,88,88,88,https://openreview.net/forum?id=rJq_YBqxx,"Figure 2 suggests that it’s just one..I think the HGRU thing is over-complicated in terms of presentation..Achieving Open Vocabulary Neural Machine Translation
---with Hybrid Word-Character Models.

The paper is well-written, the results are competitive compared to other baselines in the literature..Authors provide very compelling results on various bilingual corpora for different language pairs..Can you cite those papers?","['* Summary: This paper proposes a neural machine translation model that translates the source and the target texts in an end to end manner from characters to characters. The model can learn morphology in the encoder and in the decoder the authors use a hierarchical decoder. Authors provide very compelling results on various bilingual corpora for different language pairs. The paper is well-written, the results are competitive compared to other baselines in the literature.------------------------* Review:--------     - I think the paper is very well written, I like the analysis presented in this paper. It is clean and precise. --------     - The idea of using hierarchical decoders have been explored before, e.g. [1]. Can you cite those papers?--------     - This paper is mainly an application paper and it is mainly the application of several existing components on the character-level NMT tasks. In this sense, it is good that authors made their codes available online. However, the contributions from the general ML point of view is still limited.--------                   --------* Some Requests:-------- -Can you add the size of the models to the Table 1? --------- Can you add some of the failure cases of your model, where the model failed to translate correctly?----------------* An Overview of the Review:----------------Pros:--------    - The paper is well written--------    - Extensive analysis of the model on various language pairs--------    - Convincing experimental results.    --------    --------Cons:--------    - The model is complicated.--------    - Mainly an architecture engineering/application paper(bringing together various well-known techniques), not much novelty.--------    - The proposed model is potentially slower than the regular models since it needs to operate over the characters instead of the words and uses several RNNs.----------------[1] Serban IV, Sordoni A, Bengio Y, Courville A, Pineau J. Hierarchical neural network generative models for movie dialogues. arXiv preprint arXiv:1507.04808. 2015 Jul 17.', 'Update after reading the authors\' responses & the paper revision dated Dec 21:--------I have removed the comment ""insufficient comparison to past work"" in the title & update the score from 3 -> 5.--------The main reason for the score is on novelty. The proposal of HGRU & the use of the R matrix are basically just to achieve the effect of ""whether to continue from character-level states or using word-level states"". It seems that these solutions are specific to symbolic frameworks like Theano (which the authors used) and TensorFlow. This, however, is not a problem for languages like Matlab (which Luong & Manning used) or Torch.-------------------------------------This is a well-written paper with good analysis in which I especially like Figure 5. However I think there is little novelty in this work. The title is about learning morphology but there is nothing specifically enforced in the model to learn morphemes or subword units. For example, maybe some constraints can be put on the weights in w_i in Figure 1 to detect morpheme boundaries or some additional objective like MDL can be used (though it\'s not clear how these constraints can be incorporated cleanly). ----------------Moreover, I\'m very surprised that litte comparison (only a brief mention) was given to the work of (Luong & Manning, 2016) [1], which trains deep 8-layer word-character models and achieves much better results on English-Czech, e.g., 19.6 BLEU compared to 17.0 BLEU achieved in the paper. I think the HGRU thing is over-complicated in terms of presentation. If I read correctly, what HGRU does is basically either continue the character decoder or reset using word-level states at boundaries, which is what was done in [1]. Luong & Manning (2016) even make it more efficient by not having to decode all target words at the morpheme level & it would be good to know the speed of the model proposed in this ICLR submission. What end up new in this paper are perhaps different analyses on what a character-based model learns & adding an additional RNN layer in the encoder.----------------One minor comment: annotate h_t in Figure 1.----------------[1] Minh-Thang Luong and Christopher D. Manning. 2016. Achieving Open Vocabulary Neural Machine Translation--------with Hybrid Word-Character Models. ACL. https://arxiv.org/pdf/1604.00788v2.pdf', 'The paper presents one of the first neural translation systems that operates purely at the character-level, another one being https://arxiv.org/abs/1610.03017 , which can be considered a concurrent work. The system is rather complicated and consists of a lot of recurrent networks.  The quantitative results are quite good and the qualitative results are quite encouraging.----------------First, a few words about the quality of presentation. Despite being an expert in the area, it is hard for me to be sure that I exactly understood what is being done. The Subsections 3.1 and 3.2 sketch two main features of the architecture at a rather high-level. For example, does the RNN sentence encoder receive one vector per word as input or more? Figure 2 suggests that it’s just one. The notation h_t is overloaded, used in both Subsection 3.1 and 3.2 with clearly different meaning. An Appendix that explains unambiguously how the model works would be in order. Also, the approach appears to be limited by its reliance on the availability of blanks between words, a trait which not all languages possess.----------------Second, the results seem to be quite good. However, no significant improvement over bpe2char systems is reported. Also, I would be curious to know how long it takes to train such a model,  because from the description it seems like the model would be very slow to train (400 steps of BiNNN). On a related note, normally an ablation test is a must for such papers, to show that the architectural enhancements applied were actually necessary. I can imagine that this would take a lot of GPU time for such a complex model.----------------On the bright side, Figure 3 presents some really interesting properties that of the embeddings that the model learnt. Likewise interesting is Figure 5.----------------To conclude, I think that this an interesting application paper, but the execution quality could be improved. I am ready to increase my score if an ablation test confirms that the considered encoder is better than a trivial baseline, that e.g. takes the last hidden state for each RNN. ']",0.779296875,0.2208251953125,1,0.97802734375,0.0221099853515625,1,0.319580078125,0.6806640625,0,0.88037109375,0.11968994140625,1,0.1424560546875,0.857421875,0,0.332275390625,0.66796875,0
89,89,89,89,89,89,https://openreview.net/forum?id=rJxDkvqee,"---Cons:
---  Missing proper ASR technique based baselines..This is especially true of the word similarity test..finally in this topic the discussion and experiments should look at homophones As if not obvious what the network would learn to handle these.

The motivation and tasks feel a bit synthetic as it requires acoustics spans for words that have already been segmented from continuous speech - - a major assumption..This paper proposes an approach to learning word vector representations for character sequences and acoustic spans jointly..The evaluation tasks feel a bit synthetic overall and in particular when evaluating character based comparisons it seems there should also be phoneme based comparisons.","['Pros:--------  Interesting training criterion.--------Cons:--------  Missing proper ASR technique based baselines.----------------Comments:--------  The dataset is quite small.--------  ROC curves for detection, and more measurements, e.g. EER would probably be helpful besides AP.--------  More detailed analysis of the results would be necessary, e.g. precision of words seen during training compared to the detection--------  performance of out-of-vocabulary words.--------  It would be interesting to show scatter plots for embedding vs. orthographic distances.', ""This paper proposes an approach to learning word vector representations for character sequences and acoustic spans jointly. The paper is clearly written and both the approach and experiments seem reasonable in terms of execution. The motivation and tasks feel a bit synthetic as it requires acoustics spans for words that have already been segmented from continuous speech - - a major assumption. The evaluation tasks feel a bit synthetic overall and in particular when evaluating character based comparisons it seems there should also be phoneme based comparisons.----------------There's a lot of discussion of character edit distance relative to acoustic span similarity. It seems very natural to also include phoneme string edit distance in this discussion and experiments. This is especially true of the word similarity test. Rather than only looking at levenshtein edit distance of characters you should evaluate edit distance of the phone strings relative to the acoustic embedding distances. Beyond the evaluation task the paper would be more interesting if you compared character embeddings with phone string embeddings. I believe the last function could remain identical it's just swapping out characters for phones as the symbol set.  finally in this topic the discussion and experiments should look at homophones As if not obvious what the network would learn to handle these.---------------- the vocabulary size and training data amount make this really a toy problem. although there are many pairs constructed most of those pairs will be easy distinctions. the experiments and conclusions would be far stronger with a larger vocabulary and word segment data set with subsampling all pairs perhaps biased towards more difficult or similar pairs.---------------- it seems this approach is unable to address the task of keyword spotting in longer spoken utterances. If that's the case please add some discussion as to why you are solving the problem of word embeddings given existing word segmentations. The motivating example of using this approach to retrieve words seems flawed if a recognizer must be used to segment words beforehand "", 'this proposes a multi-view learning approach for learning representations for acoustic sequences. they investigate the use of bidirectional LSTM with contrastive losses. experiments show improvement over the previous work.----------------although I have no expertise in speech processing, I am in favor of accepting this paper because of following contributions:--------- investigating the use of fairly known architecture on a new domain.--------- providing novel objectives specific to the domain--------- setting up new benchmarks designed for evaluating multi-view models----------------I hope authors open-source their implementation so that people can replicate results, compare their work, and improve on this work.']",0.87744140625,0.1224365234375,1,0.69677734375,0.30322265625,1,0.15380859375,0.84619140625,0,0.91455078125,0.08544921875,1,0.17724609375,0.82275390625,0,0.364990234375,0.634765625,0
90,90,90,90,90,90,https://openreview.net/forum?id=rk9eAFcxg,"The core of the approach is a combination of variational recurrent neural networks and adversarial domain adaptation (at the last time step)..-I don't have questions about the proposed model, the model is quite clear and seems to be a simple combination of VRNN and DANN..After reading them, I still think the paper is not novel enough so I'm leaving the rating untouched.

The work combines variational recurrent neural networks, and adversarial neural networks to handle domain adaptation for time series data..The proposed method, along with several competing algorithms are compared on two healthcare datasets constructed from MIMIC-III in domain adaptation settings..-The new contribution of the work is relatively small.","['The work combines variational recurrent neural networks, and adversarial neural networks to handle domain adaptation for time series data. The proposed method, along with several competing algorithms are compared on two healthcare datasets constructed from MIMIC-III in domain adaptation settings.----------------The new contribution of the work is relatively small. It extends VRNN with adversarial training for learning domain agnostic representations. From the experimental results, the proposed method clearly out-performs competing algorithms. However, it is not clear where the advantage is coming from. The only difference between the proposed method and R-DANN is using variational RNN vs RNN. Little insights were provided on how this could bring such a big difference in terms of performance and the drastic difference in the temporal dependencies captured by these two methods in Figure 4.  ----------------Detailed comments:--------1. Please provide more details on what is plotted in Figure 1. Is 1 (b) is the t-sne projection of representations learned by DANN or R-DANN? The text in section 4.4 suggests it’s the later case. It is surprising to see such a regular plot for VRADA.  What do you think are the two dominant latent factors encoded in figure 1 (c)? ----------------2. In Table 2, the two baselines have quite significant difference in performance testing on the entire target (including validation set) vs on the test set only. VRADA, on the other hand, performs almost identical in these two settings. Could you please offer some explanation on this?----------------3. Please explain figure 3 and 4 in more details. how to interpret the x-axis of figure 3, and the x and y axes of figure 4. Again the right two plots in figure 4 are extremely regular comparing to the ones on the left. ', ""Update: I thank the authors for their comments! After reading them, I still think the paper is not novel enough so I'm leaving the rating untouched.----------------This paper proposes a domain adaptation technique for time series. The core of the approach is a combination of variational recurrent neural networks and adversarial domain adaptation (at the last time step).----------------Pros:----------------1. The authors consider a very important application of domain adaptation.----------------2. The paper is well-written and relatively easy to read.----------------3. Solid empirical evaluation. The authors compare their method against several recent domain adaptation techniques on a number of datasets.----------------Cons:----------------1. The novelty of the approach is relatively low: it’s just a straightforward fusion of the existing techniques.----------------2. The paper lacks any motivation for use of the particular combination (VRNN and RevGrad). I still believe comparable results can be obtained by polishing R-DANN (e.g. carefully penalizing domain discrepancy at every step)----------------Additional comments:----------------1. I’m not convinced by the discussion presented in Section 4.4. I don’t think the visualization of firing patterns can be used to support the efficiency of the proposed method.----------------2. Figure 1(c) looks very suspicious. I can hardly believe t-SNE could produce this _very_ regular structure for non-degenerate (non-synthetic, real-world) data.----------------Overall, it’s a solid paper but I’m not sure if it is up to the ICLR standard."", ""This paper combines variational RNN (VRNN) and domain adversarial networks (DANN) for domain adaptation in the sequence modelling domain.  The VRNN is used to learn representations for sequential data, which is the hidden states of the last time step.  The DANN is used to make the representations domain invariant, therefore achieving cross domain adaptation.----------------Experiments are done on a number of data sets, and the proposed method (VRADA) outperforms baselines including DANN, VFAE and R-DANN on almost all of them.----------------I don't have questions about the proposed model, the model is quite clear and seems to be a simple combination of VRNN and DANN.  But a few questions came up during the pre-review question phase:----------------- As the authors have mentioned, DANN in general outperforms MMD based methods, however, the VFAE method which is based on MMD regularization on the representations seems to outperform DANN across the board.  That seems to indicate VRNN + MMD should also be a good combination.----------------- One baseline the authors showed in the experiments is R-DANN, which is an RNN version of DANN.  There are two differences between R-DANN and VRADA: (1) R-DANN uses deterministic RNN for representation learning, while VRADA uses variational RNN; (2) on target domain R-DANN only optimizes adversarial loss, while VRADA optimizes both adversarial loss and reconstruction loss for feature learning.  It would be good to analyze further where the performance gain comes from.""]",0.87353515625,0.126220703125,1,0.802734375,0.1973876953125,1,0.1619873046875,0.837890625,0,0.9033203125,0.096923828125,1,0.0650634765625,0.93505859375,0,0.359130859375,0.640625,0
91,91,91,91,91,91,https://openreview.net/forum?id=rkE8pVcle,"-Additional notes:
---I think the simulation, in the synthetic environment, for the first mistake a learner can make during dialogue: “the learner has problems understanding the surface form of the text of the dialogue partner, e.g., the phrasing of a question”, is particularly limited since only word misspellings are considered (and the models used don’t work at the character level), which is of course only a tiny fraction of ways an agent can misunderstand the context..-Other comments/questions: 

 What is the motivation behind using both vanilla-MemN2N AND Cont-MemN2N?.-I am a bit concerned that many of the tasks are too easy (e.g.

The motivation is that an intelligent agent can improve its performance by asking questions and getting corresponding feedback from users..The paper studies three different types of tasks where the agent can benefit from user feedback..Is using both resulting in any conclusions which are adding to the paper's contributions?","['The paper introduces a simulator and a set of synthetic question answering tasks where interaction with the ""teacher"" via asking questions is desired. The motivation is that an intelligent agent can improve its performance by asking questions and getting corresponding feedback from users. The paper studies this problem in an offline supervised and an online reinforcement learning settings. The results show that the models improve by asking questions.  ------------------ The idea is novel, and is relatively unexplored in the research community. The paper serves as a good first step in that direction.---------- The paper studies three different types of tasks where the agent can benefit from user feedback.---------- The paper is well written and provides a clear and detailed description of the tasks, models and experimental settings.----------------Other comments/questions: ---------- What is the motivation behind using both vanilla-MemN2N AND Cont-MemN2N? Is using both resulting in any conclusions which are adding to the paper\'s contributions?---------- In the Question Clarification setting, what is the distribution of misspelled words over question entity, answer entity, relation entity or none of these? If most of the misspelled words come from relation entities, it might be a much easier problem than it seems.---------- The first point on Page 10 ""The performance of TestModelAQ is worse than TestAQ but better than TestQA."" is not true for Task 2 from the numbers in Tables 2 and 4.---------- What happens if the conversational history is smaller or none? ---------- Figure 5, Task 6, why does the accuracy for good student drop when it stops asking questions? It already knows the relevant facts, so asking questions is not providing any additional information to the good student. ---------- Figure 5, Task 2, the poor student is able to achieve almost 70% of the questions correct even without asking questions. I would expect this number to be quite low. Any explanation behind this?---------- Figure 1, Task 2 AQ, last sentence should have a negative response ""(-)"" instead of positive as currently shown. ----------------Preliminary Evaluation: --------A good first step in the research direction of learning dialogue agents from unstructured user interaction. ', 'The goal of this paper is to analyze the behaviour of dialogue agents when they must answer factoid questions, but must query an oracle for additional information. This can be interpreted as a form of interaction between the dialogue agent and a ‘teacher’.----------------The problem under investigation is indeed very important. The authors create a synthetic environment in which to test their agent. The main strength of the paper is that the paper tests many different combinations of environments, where either some knowledge is missing (and the agent has to query for it), or there is some misspelling in the teacher’s question, and different ways the agent can ask for extra information. ----------------I am a bit concerned that many of the tasks are too easy (e.g. the AQ question paraphrase), and I am also concerned that the environment presented is very limited, and quite far (in terms of richness of linguistic structure) from how real humans would interact with chatbots. I think the paper would be better positioned as testing the basic reasoning capabilities of agents/ their ability to do question answering, rather than dialogue. However, I think the ‘ground-up’ approach that starts with simple environments is indeed worthy of analysis, and this paper makes an interesting contribution in that direction. Of course, the paper would be much more convincing with human experiments. ----------------Additional notes:--------I think the simulation, in the synthetic environment, for the first mistake a learner can make during dialogue: “the learner has problems understanding the surface form of the text of the dialogue partner, e.g., the phrasing of a question”, is particularly limited since only word misspellings are considered (and the models used don’t work at the character level), which is of course only a tiny fraction of ways an agent can misunderstand the context. I would be particularly interested to see some discussion of how the authors plan to scale this up to more realistic settings.----------------EDIT: I have updated my score to reflect the addition of the Mechanical Turk experiments', 'This paper introduces a simulator and a set of synthetic tasks for evaluating a dialogue agent\'s ability to learn from user feedback. For solving these tasks, the paper uses memory networks (Sukhbaatar et al., 2015) learned through previously proposed supervised learning and reinforcement learning methods. In this setup, it is demonstrated that the agent learning from feedback (e.g. through question asking or question clarification) performs better.----------------The motivation for the paper is excellent; dialogue agents which learn directly from unstructured human feedback (as opposed to reward signals alone) could be very useful in real-world applications. However, the paper falls short on the execution. All the numerous experiments presented are based on the synthetic dialogue simulator, which is highly artificial and different from real-world dialogues. The simulator is based on a simple factoid question-answering framework, which normally is not considered dialogue and which appears to be solvable with a few hand-crafted rules. The framework also assumes that the user\'s feedback is always correct and is given in one of a handful of forms (e.g. paraphrase of original question without typos) and that the agent can learn from examples of another agent asking questions or making clarifications, which simplifies the task even further.----------------Because of the artificial setting and limited scope of the experiments, it seems difficult to draw conclusions about how to learn from unstructured user feedback. To test the hypothesis that it is possible to learn from such user feedback, I would strongly recommend the authors to continue working on this project by carrying out experiments with real human users (even in the factoid question answering domain, if necessary). This would provide much stronger evidence that a dialogue agent can learn from such feedback.------------------------Other comments:--------- The abstract uses the phrase ""interactive dialogue agents"". What is meant by ""interactive"" dialogue agents? All dialogue agents interact with the user, so isn\'t it redundant to call them interactive?--------- A major limitation of the experiments is that the questions the agent can ask are specified a priori. If I understand correctly, in the supervised learning setting the agent is trained to imitate the questions of another rule-based agent. While in the RL setting, the paper states ""For each dialogue, the bot takes two sequential actions : to ask or not to ask a question (denoted as a_1 ); and guessing the final answer (denoted as a_2)"". This means the agent learns *when* to ask questions but not *what* questions to ask.--------- Related to the previous comment, in the sub-section ""ONLINE REINFORCEMENT LEARNING (RL)"" the paper states ""We also explored scenarios where the student learns the ability to decide when to ask a question and what to ask."". Please clarify this by removing the part ""what to ask"".--------- The paper presents an overwhelming amount of results. I understand the benefit of synthetic tasks is precisely the ability to measure many aspects of model performance, but in this case it confuses the reader to present so many results. For example, what was the reason for including the ""TrainAQ(+FP)"" and ""TrainMix"" training settings? How do these results help validate the original hypothesis? If they don\'t, they should be taken out or moved to the appendix.--------- Since the contribution of the paper lies in the tasks and evaluation, it might be better to move either the vanilla-MemN2N (Table 2) to the appendix or to move the Cont-MemN2N results (Table 3) to the appendix.------------------- UPDATE -------------------Following the discussion below and the additional experiments provided by the authors, I have increased my score to 8.']",0.669921875,0.330322265625,1,0.931640625,0.06842041015625,1,0.084716796875,0.9150390625,0,0.90771484375,0.0921630859375,1,0.1207275390625,0.87939453125,0,0.439208984375,0.560546875,0
92,92,92,92,92,92,https://openreview.net/forum?id=rkYmiD9lg,"The main idea is quite good, but insufficient attention was devoted to analyzing the aspects of the model that make it interesting and novel..The empirical experiments validate the proposed method..-Overall, I am on the fence regarding this paper.

A more detailed comparison with competing methods, like Factorization Machines, could also improve the paper..The paper presents an application of a tensor factorization to linear models, which allows to consider higher-order interactions between variables in classification (and regression) problems, and that maintains computational feasibility, being linear in the dimension..The factorization employed is based on the TT format, first proposed by Oseledests (2011).","['The paper presents an application of a tensor factorization to linear models, which allows to consider higher-order interactions between variables in classification (and regression) problems, and that maintains computational feasibility, being linear in the dimension. The factorization employed is based on the TT format, first proposed by Oseledests (2011). The authors also propose the adoption of a Riemannian optimization scheme to explicit consider the geometry of the tensor manifold, and thus speed up convergence.----------------The paper in general is well written, it presents an interesting application of the TT tensor format for linear models (together with an application of Riemannian optimization), which in my opinion is quite interesting since it has a wide range of possible applications in different algorithms in machine learning.----------------On the other side, I have some concerns are about the experimental part, which I consider not at the level of the rest of the paper, for instance in terms of number of experiments on real datasets, role of dropout in real datasets, comparison with other algorithms on real datasets. Moreover the authors do not take into account explicitly the problem of the choice of the rank to be used in the experiments. In general the experimental section seems a collection of preliminary experiments where different aspects have been tested by not in a organic way.----------------I think the paper is close to a weak acceptance / weak rejection, I don\'t rate it as a full acceptance paper, mainly due to the non-satisfactory experiment setting. In case of extra experiments confirming the goodness of the approach, I believe the paper could have much better scores.----------------Some minor comments:---------formula 2: Obvious comment: learning the parameters of the model in (1) can be done as in (2), but also in other ways, depending on the approach you are using.---------the fact that the rank is bounded by 2r, before formula 9, is explained in Lubich et al., 2015?---------after formula 10: why the N projections in total they cost O(dr^2(r+N)), it should be O(Ndr^2(r+1)), no? since each of the elements of the summation has rank 1, and the cost for each of them is O(dr^2(r+TT_rank(Z)^2)), where TT-rank(Z)=1. Am I wrong?---------section 6.2: can you explain why the random initialization freezes the convergence? This seems interesting but not motivated. Any guess?---------section 6.3: you adopt dropout: can you comment in particular on the advantages it gives in the context of the exponential machines? did you use it on real datasets?---------how do you choose r_0 in you experiments? with a validation set?---------in section 7: why you don\'t have x_1 x_2 among the variables?---------section 8: there is a typo in ""experiments""---------section 8.1: ""We simplicity, we binarized"" I think there\'s a problem with the English language in this sentence---------section 8.3: ""we report that dropout helps"".. this is quite general statement, only tested on a synthetic dataset---------section 8.5: can you provide more results for this dataset, for instance in terms of training and inference time? or test wrt other algorithms?', ""This paper introduces a polynomial linear model for supervised classification tasks. The model is based on a combination of the Tensor Train (TT) tensor decomposition method and a form of stochastic Riemannian  optimization. A few empirical experiments are performed that demonstrate the good performance of the proposed model relative to appropriate baselines.----------------From a theoretical standpoint, I think the approach is interesting and elegant. The main machinery underlying this work are the TT decomposition and the geometric structure of the manifold of tensors with fixed TT-rank, which have been established in prior work. The novelty of this paper is in the combination of this machinery to form an efficient polynomial linear model. As such, I would have hoped that the paper mainly focused on the efficacy of this combination and how it is superior to obvious alternatives. For example, I would have really appreciated seeing how FMs performed when optimized over the manifold of positive definite matrices, as another reviewer mentioned. Instead, there is a bit too much effort devoted to explaining prior work.----------------I think the empirical analysis could be substantially improved. I am particularly puzzled by the significant performance boost obtained from initializing with the ordinary logistic regression solution. I would have liked some further analysis of this effect, especially whether or not it is possible to obtain a similar performance boost with other models. Regarding the synthetic data, I think an important baseline would be against a vanilla feed forward neural network, which would help readers understand how complicated the interactions are and how difficult the dataset is to model. I agree with the previous reviewer regarding a variety of other possible improvements to the experimental section.----------------A few typos: 'Bernoulli distrbution', 'reproduce the experiemnts', 'generilize better'.----------------Overall, I am on the fence regarding this paper. The main idea is quite good, but insufficient attention was devoted to analyzing the aspects of the model that make it interesting and novel."", 'This paper proposes to use the tensor train (TT) decomposition to represent the full polynomial linear model. The TT form can reduce the computation complexity in both of inference and model training. A stochastic gradient over a Riemann Manifold has been proposed to solve the TT based formulation. The empirical experiments validate the proposed method.----------------The proposed approach is very interesting and novel for me. I would like to vote acceptance on this paper. My only suggestion is to include the computational complexity per iteration.', 'The paper describes how to use a tensor factorization method called Tensor Train for modeling the interactions between features for supervised classification tasks. Tensor Train approximates tensors of any dimensions using low rank products of matrices. The rank is used as a parameter for controlling the complexity of the approximation. Experiments are performed on different datasets for binary classification problems.----------------The core of the paper consists in demonstrating how the TT formalism developed by one of the authors could be adapted for modeling interactions between features. Another contribution is a gradient algorithm that exploits the geometrical structure of the factorization. These ideas are probably new in Machine learning. The algorithm itself is of reasonable complexity for the inference and could probably be adapted to large size problems although this is not the case for the experiments here.----------------The experimental section is not well structured, it is incomplete and could be improved. We miss a description of the datasets characteristics. The performance on the different datasets are not provided. Each dataset has been used for illustrating one aspect of the model, but you could also provide classification performance and a comparison with baselines for all the experiments. The experiments on the 2 UCI datasets show optimization performance on the training set, you could provide the same curves on test sets to show how the algorithm generalizes. The comparison to other approaches (section 8.4) is only performed on artificial data, which are designed with interacting features and are not representative of diverse situations. The same holds for the role of Dropout. The comparison on the Movielens dataset is incomplete. Besides, all the tests are performed on small size problems.----------------Overall, there are original contributions which could be worth a publication. The experiments are incomplete and not conclusive. A more detailed comparison with competing methods, like Factorization Machines, could also improve the paper. ']",0.8037109375,0.196533203125,1,0.93505859375,0.0648193359375,1,0.82470703125,0.1754150390625,1,0.912109375,0.088134765625,1,0.09466552734375,0.9052734375,0,0.5537109375,0.446533203125,1
93,93,93,93,93,93,https://openreview.net/forum?id=ry3iBFqgl,"After all, not all humans have the same level of carefulness or even the same level of reading comprehension..I think it’d be the best if the authors can try to explain the reason behind these differences, and if possible, perform a more careful measurement of human performance..-I’m also not sure whether the design choice of not presenting the news article when soliciting the questions was a good one.

An empirical study that fairly shows that NEWSQA is more challenging (or better in some other way) than SQuAD..It would seem that the shelf life of a dataset has decreased rapidly in recent literature..SQuAD dataset has been heavily pursued as soon as it hit online couple months ago, the best performance on their leaderboard now reaching to 82%.","['It would seem that the shelf life of a dataset has decreased rapidly in recent literature. SQuAD dataset has been heavily pursued as soon as it hit online couple months ago, the best performance on their leaderboard now reaching to 82%. This is rather surprising when taking into account the fact that the formal conference presentation of the dataset took place only a month ago at EMNLP’16, and that the reported machine performance (at the time of paper submission) was only at 51%. One reasonable speculation is that the dataset may have not been hard enough.----------------NewsQA, the paper in submission, aims to address this concern by presenting a dataset of a comparable scale created through different QA collection strategies. Most notably, the authors solicit questions without requiring answers from the same turkers, in order to promote more diverse and hard-to-answer questions. Another notable difference is that the questions are gathered without showing the content of the news articles, and the dataset makes use of a bigger subset of CNN/Daily corpus (12K / 90K), as opposed to a much smaller subset (500 / 90K) used by SQuAD.----------------In sum, I think NewsQA dataset presents an effort to construct a harder, large-scale reading comprehension challenge, a recently hot research topic for which we don’t yet have satisfying datasets. While not without its own weaknesses, I think this dataset presents potential values compared to what are available out there today.----------------That said, the paper does read like it was prepared in a hurry, as there are numerous small things that the authors could have done better. As a result, I do wonder about the quality of the dataset. For one, human performance of SQuAD measured by the authors (70.5 - 82%) is lower than that reported by SQuAD (80.3 - 90.5%). I think this sort of difference can easily happen depending on the level of carefulness the annotators can maintain. After all, not all humans have the same level of carefulness or even the same level of reading comprehension. I think it’d be the best if the authors can try to explain the reason behind these differences, and if possible, perform a more careful measurement of human performance. If anything, I don’t think it looks favorable for NewsQA if the human performance is only at the level of 74.9%, as it looks as if the difficulty of the dataset comes mainly from the potential noise from the QA collection process, which implies that the low model performance could result from not necessarily because of the difficulty of the comprehension and reasoning, but because of incorrect answers given by human annotators.----------------I’m also not sure whether the design choice of not presenting the news article when soliciting the questions was a good one. I can imagine that people might end up asking similar generic questions when not enough context has been presented. Perhaps taking a hybrid, what I would like to suggest is to present news articles where some sentences or phrases are randomly redacted, so that the question generators can have a bit more context while not having the full material in front of them.----------------Yet another way of encouraging the turkers from asking too trivial questions is to engage an automatic QA system on the fly — turkers must construct a QA pair for which an existing state-of-the-art system cannot answer correctly.', 'Paper Summary: --------This paper presents a new comprehension dataset called NewsQA dataset, containing 100,000 question-answer pairs from over 10,000 news articles from CNN. The dataset is collected through a four-stage process -- article filtering, question collection, answer collection and answer validation. Examples from the dataset are divided into different types based on answer types and reasoning required to answer questions. Human and machine performances on NewsQA are reported and compared with SQuAD.----------------Paper Strengths: ---------- I agree that models can benefit from diverse set of datasets. This dataset is collected from news articles, hence might pose different sets of problems from current popular datasets such as SQuAD.---------- The proposed dataset is sufficiently large for data hungry deep learning models to train. ---------- The inclusion of questions with null answers is a nice property to have.---------- A good amount of thought has gone into formulating the four-stage data collection process.---------- The proposed BARB model is performing as good as a published state-of-the-art model, while being much faster.    ----------------Paper Weaknesses: ---------- Human evaluation is weak. Two near-native English speakers\' performance on 100 examples each can hardly be a representative of the complete dataset. Also, what is the model performance on these 200 examples?---------- Not that it is necessary for this paper, but to clearly demonstrate that this dataset is harder than SQuAD, the authors should either calculate the human performance the same way as SQuAD or calculate human performances on both NewsQA and SQuAD in some other consistent manner on large enough subsets which are good representatives of the complete datasets. Dataset from other communities such as VQA dataset (Antol et al., ICCV 2015) also use the same method as SQuAD to compute human performance. ---------- Section 3.5 says that 86% of questions have answers agreed upon by atleast 2 workers. Why is this number inconsistent with the 4.5% of questions which have answers without agreement after validation (last line in Section 4.1)?---------- Is the same article shown to multiple Questioners? If yes, is it ensured that the Questioners asking questions about the same article are not asking the same/similar questions?---------- Authors mention that they keep the same hyperparameters as SQuAD. What are the accuracies if the hyperparameters are tuned using a validation set from NewsQA?---------- 500 examples which are labeled for reasoning types do not seem enough to represent the complete dataset. Also, what is the model performance on these 500 examples?---------- Which model\'s performance has been shown in Figure 1?---------- Are the two ""students"" graduate/undergraduate students or researchers?---------- Test set seems to be very small.---------- Suggestion: Answer validation step is nice, but maybe the dataset can be released in 2 versions -- one with all the answers collected in 3rd stage (without the validation step), and one in the current format with the validation step. ----------------Preliminary Evaluation: --------The proposed dataset is a large scale machine comprehension dataset collected from news articles, which in my suggestion, is diverse enough from existing datasets that state-of-the-art models can definitely benefit from it. With a better human evaluation, I think this paper will make a good poster. ', 'Summary: The paper proposes a novel machine comprehension dataset called NEWSQA. The dataset consists of over 100,000 question answer pairs based on over 10,000 news articles from CNN. The paper analyzes the different types of answers and the different types of reasoning required to answer questions in the dataset. The paper evaluates human performance and the performance of two baselines on the dataset and compares them with the performance on SQuAD dataset. ----------------Strengths:----------------1. The paper presents a large scale dataset for machine comprehension. ----------------2. The question collection method seems reasonable to collect exploratory questions. Having an answer validation step is desirable.----------------3. The paper proposes a novel (computationally more efficient) implementation of the match-LSTM model.----------------Weaknesses:----------------1. The human evaluation presented in the paper is not satisfactory because the human performance is reported on a very small subset (200 questions). So, it seems unlikely that these 200 questions will provide a reliable measure of the human performance on the entire dataset (which consists of thousands of questions).----------------2. NEWSQA dataset is very similar to SQuAD dataset in terms of the size of the dataset, the type of dataset -- natural language questions posed by crowdworkers, answers comprising of spans of text from related paragraphs. The paper presents two empirical ways to show that NEWSQA is more challenging than SQuAD -- 1) the gap between human and machine performance in NEWSQA is larger than that in SQuAD. However, since the human performance numbers are reported on very small subset, these trends might not carry over when human performance is computed on all of the dataset.--------2) the sentence-level accuracy on SQuAD is higher than that in NEWSQA. However, as the paper mentions, the differences in accuracies could likely be due to different lengths of documents in the two datasets. So, even this measure does not truly reflect that SQuAD is less challenging than NEWSQA.--------So, it is not clear if NEWSQA is truly more challenging than SQuAD.----------------3. Authors mention that BARB is computationally more efficient and faster compared to match-LSTM. However, the paper does not report how much faster BARB is compared to match-LSTM.----------------4. On page 7, under ""Boundary pointing"" paragraph, the paper should clarify what ""s"" in ""n_s"" refers to.----------------Review summary: While the dataset collection method seems interesting and promising, I would be more convinced after I see the following ----------1. Human performance on all (or significant percentage of the dataset).--------2. An empirical study that fairly shows that NEWSQA is more challenging (or better in some other way) than SQuAD.']",0.90625,0.09356689453125,1,0.9443359375,0.0557861328125,1,0.35693359375,0.64306640625,0,0.8876953125,0.11212158203125,1,0.06134033203125,0.9384765625,0,0.228271484375,0.77197265625,0
94,94,94,94,94,94,https://openreview.net/forum?id=ry_4vpixl,"we can't have complex attractors and so forth if we run the model forward without any inputs)..(providing large enough hidden dimensions and number of layers)
---- More generally, can one compare the expressiveness of the model with the equivalent model without the orthogonal matrices?.---- The experiments are a bit disappointing as the number of distinct input/output
---sequences were in fact very small and as noted by the authr, training
---becomes unstable (I didn't understand what ""success"" meant in this case).

My main objection with this work is that it operates under a hypothesis (that is becoming more and more popular in the literature) that all we need is to have gradients flow in order to solve long term dependency problems..The usual approach is then to enforce orthogonal matrices which (in absence of the nonlinearity) results in unitary jacobians, hence the gradients do not vanish and do not explode..It also becomes really hard to deal with noise (since you attempt to preserve every detail of the input, or rather every part of the input affects the output).","[""My main objection with this work is that it operates under a hypothesis (that is becoming more and more popular in the literature) that all we need is to have gradients flow in order to solve long term dependency problems. The usual approach is then to enforce orthogonal matrices which (in absence of the nonlinearity) results in unitary jacobians, hence the gradients do not vanish and do not explode. However this hypothesis is taken for granted (and we don't know it is true yet) and instead of synthetic data, we do not have any empirical evidence that is strong enough to convince us the hypothesis is true. ----------------My own issues with this way of thinking is: a) what about representational power; restricting to orthogonal matrices it means we can not represent the same family of functions as before (e.g. we can't have complex attractors and so forth if we run the model forward without any inputs). You can only get those if you have eigenvalues larger than 1. It also becomes really hard to deal with noise (since you attempt to preserve every detail of the input, or rather every part of the input affects the output). Ideally you would want to preserve only what you need for the task given limited capacity. But you can't learn to do that. My issue is that everyone is focused on solving this preserved issue without worrying of the side-effects. ----------------I would like one of these papers going for jacobians having eigenvalues of 1 show this helps in realistic scenarios, on complex datasets."", ""This paper discusses recurrent networks with an update rule of the form h_{t+1} = R_x R h_{t}, where R_x is an embedding of the input x into the space of orthogonal or unitary matrices, and R is a shared orthogonal or unitary matrix.    While this is an interesting model, it is by no means a *new* model:  the idea of using matrices to represent input objects (and multiplication to update state) is often used in the embedding-knowledge-bases or embedding-logic literature (e.g. Using matrices to model symbolic relationships by Ilya Sutskever and Geoffrey Hinton, or Holographic Embeddings of Knowledge Graphs by Maximillian Nickel et al.).  I don't think the experiments or analysis in this work add much to our understanding of it.    In particular, the experiments are especially weak, consisting only of a very simplified version of the copy task (which is already very much a toy).  I know several people who have played with this model in the setting of language modeling, and as the other reviewer notes, the inability of the model to forget is an actual annoyance.   ----------------I think it is incumbent on the authors to show how this model can be really useful on a nontrivial task; as it is we should not accept this paper.----------------Some questions:  is there any reason to use the shared R instead of absorbing it into all the R_x?  Can you find any nice ways of using the fact that the model is linear in h or linear in R_x ?"", 'This is a nice proposal, and could lead to more efficient training of--------recurrent nets. I would really love to see a bit more experimental evidence.--------I asked a few questions already but didn\'t get any answer so far.--------Here are a few other questions/concerns I have:----------------- Is the resulting model still a universal approximator? (providing large enough hidden dimensions and number of layers)--------- More generally, can one compare the expressiveness of the model with the equivalent model without the orthogonal matrices? with the same number of parameters for instance?--------- The experiments are a bit disappointing as the number of distinct input/output--------sequences were in fact very small and as noted by the authr, training--------becomes unstable (I didn\'t understand what ""success"" meant in this case).--------The authors point that the experiment section need to be expanded, but as--------far as I can tell they still haven\'t unfortunately.']",0.7705078125,0.2294921875,1,0.841796875,0.158447265625,1,0.2271728515625,0.77294921875,0,0.89697265625,0.10308837890625,1,0.10723876953125,0.892578125,0,0.328369140625,0.671875,0
95,95,95,95,95,95,https://openreview.net/forum?id=ryh9pmcee,"-Regarding the quantitative results in Table 2, it seems not appropriate to bold the EBGAN line when it seems to be statistically indistinguishable form the Rasmus et al (2015) results..---* From visual inspection alone it is difficult to conclude whether EB-GANs produce better samples than DC-GANs on the LSUN and CelebA datasets..-First, it does away with the entropy regularization term that Kim and Bengio (2016) introduced to ensure that the GAN discriminator converged to an energy function proportional to the log density of the data (at optimum).

This paper is a parallel work to Improving Generative Adversarial Networks with Denoising Feature Matching..The main solution of both papers is introducing autoencoder into discriminator to improve the stability and quality of GAN..Different to Denoising Feature Matching, EBGAN uses encoder-decoder instead of denoising only, and use hingle loss to replace original loss function.","['This paper is a parallel work to Improving Generative Adversarial Networks with Denoising Feature Matching. The main solution of both papers is introducing autoencoder into discriminator to improve the stability and quality of GAN. Different to Denoising Feature Matching, EBGAN uses encoder-decoder instead of denoising only, and use hingle loss to replace original loss function.----------------The theoretical results are good, and empirical result of high resolution image is unique among all recent GAN advantages.----------------I suggest to introduce Improving Generative Adversarial Networks with Denoising Feature Matching as related work.', 'This paper introduces an energy-based Generative Adversarial Network (GAN) and provides theoretical and empirical results modeling a number of image datasets (including large-scale versions of categories of ImageNet). As far as I know energy-based GANs (EBGAN) were introduced in Kim and Bengio (2016), but the proposed version makes a number of different design choices. ----------------First, it does away with the entropy regularization term that Kim and Bengio (2016) introduced to ensure that the GAN discriminator converged to an energy function proportional to the log density of the data (at optimum). This implies that the discriminator in the proposed scheme will become uniform at convergence as discussed in the theoretical section of the paper, however the introductory text seems to imply otherwise -- that one could recover a meaningful score function from the trained energy-function (discriminator). This should be clarified. ----------------Second, this version of the EBGAN setting includes two innovations: (1) the introduction of the hinge loss in the value function, and (2) the use of an auto-encoder parametrization for the energy function. These innovations are not empirically justified in any way - this is disappointing, as it would be really good to see empirical results supporting the arguments made in support of their introduction. ----------------The two significant contributions of this paper are the theoretical analysis of the energy-baesd GAN formalism (showing that the optimum corresponds to a Nash equilibrium) and the impressive empirical results on large images that set a new standard in what straight GAN-style models can achieve.----------------The theoretical results seem solid to me and make a nice contribution.----------------Regarding the quantitative results in Table 2, it seems not appropriate to bold the EBGAN line when it seems to be statistically indistinguishable form the Rasmus et al (2015) results. Though it is not mentioned here, the use of bold typically indicates the state-of-the-art. ----------------I think this paper could be be much stronger if the two novel contributions to the energy-based GAN setting were more thoroughly explored with ablation experiments. That being said, I think this paper has already become a contribution other are building on (including at least two other ICLR submissions) and so I think it should be accepted for publication at ICLR. ', 'This paper proposes a novel extension of generative adversarial networks that replaces the traditional binary classifier discriminator with one that assigns a scalar energy to each point in the generator\'s output domain. The discriminator minimizes a hinge loss while the generator attempts to generate samples with low energy under the discriminator. The authors show that a Nash equilibrium under these conditions yields a generator that matches the data distribution (assuming infinite capacity). Experiments are conducted with the discriminator taking the form of an autoencoder, optionally including a regularizer that penalizes generated samples having a high cosine similarity to other samples in the minibatch.----------------Pros:--------* The paper is well-written.--------* The topic will be of interest to many because it sets the stage for the exploration of a wider variety of discriminators than currently used for training GANs.--------* The theorems regarding optimality of the Nash equilibrium appear to be correct.--------* Thorough exploration of hyperparameters in the MNIST experiments.--------* Semi-supervised results show that contrastive samples from the generator improve classification performance.----------------Cons:--------* The relationship to other works that broaden the scope of the discriminator (e.g. [1]) or use a generative network to provide contrastive samples to an energy-based model ([2]) is not made clear in the paper.--------* From visual inspection alone it is difficult to conclude whether EB-GANs produce better samples than DC-GANs on the LSUN and CelebA datasets.--------* It is difficult to assess the effect of the PT regularizer beyond visual inspection as the Inception score results are computed with the vanilla EB-GAN.----------------Specific Comments--------* Sec 2.3: It is unclear to me why a reconstruction loss will necessarily produce very different gradient directions.--------* Sec 2.4: It is confusing that ""pulling-away"" is abbreviated as ""PT"".--------* Sec 4.1: It seems strange that the Inception model (trained on natural images) is being used to compute KL scores for MNIST. Using an MNIST-trained CNN to compute Inception-style scores seems to be more appropriate here.--------* Figure 3: There is little variation across the histograms, so this figure is not very enlightening.--------* Appendix A: In the proof of theorem 2, it is unclear to me why a Nash equilibrium of the system exists.----------------Typos / Minor Comments--------* Abstract: ""probabilistic GANs"" should probably be ""traditional"" or ""classical"" GANs.--------* Theorem 2: ""A Nash equilibrium ... exists""--------* Sec 3: Should be ""Several papers were presented""----------------Overall, I have some concerns with the related work and experimental evaluation sections, but I feel the model is novel enough and is well-justified by the optimality proofs and the quality of the generated samples.----------------[1] Springenberg, Jost Tobias. ""Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks."" arXiv preprint arXiv:1511.06390 (2015).--------[2] Kim, Taesup, and Yoshua Bengio. ""Deep Directed Generative Models with Energy-Based Probability Estimation."" arXiv preprint arXiv:1606.03439 (2016).']",0.8701171875,0.1300048828125,1,0.85595703125,0.1439208984375,1,0.3095703125,0.6904296875,0,0.90087890625,0.09893798828125,1,0.19384765625,0.80615234375,0,0.4052734375,0.5947265625,0
96,96,96,96,96,96,https://openreview.net/forum?id=ryhqQFKgl,"-Overall, while I think that I like this work, and while I am familiar with the JSB chorales, with probabilistic approaches, with n-grams, etc, I did find the paper quite hard to follow at various parts..(Incidentally, the issue is not the application as I think that music applications can be very appropriate, nor is the problem necessarily with the approach... see my next suggestion..)..-If the paper were revised substantially (though I cannot suggest details for how to do this within the appropriate page count), I would consider raising my score.

On one hand, this was very helpful for me to better understand the current paper..On the other hand, this was very needed for me to better understand the current paper..I get the sense that a long-form journal publication would actually give the authors the space necessary to fully explain these ideas, provide clearer running examples where needed, provide the necessary background for the appropriate readership, provide the necessary background on the previous system, perhaps demonstrating results on a second dataset to show generality of the approach, etc.","[""After the discussion below, I looked at previous work by the authors (MUS-ROVER) on which this paper was based. On one hand, this was very helpful for me to better understand the current paper. On the other hand, this was very needed for me to better understand the current paper.----------------Overall, while I think that I like this work, and while I am familiar with the JSB chorales, with probabilistic approaches, with n-grams, etc, I did find the paper quite hard to follow at various parts. The extensive use of notation did not help the clarity.----------------I think the ideas and approaches are good, and certainly worth publishing and worth pursuing. I am not sure that, in the paper's current form, ICLR is an appropriate venue. (Incidentally, the issue is not the application as I think that music applications can be very appropriate, nor is the problem necessarily with the approach... see my next suggestion..). I get the sense that a long-form journal publication would actually give the authors the space necessary to fully explain these ideas, provide clearer running examples where needed, provide the necessary background for the appropriate readership, provide the necessary background on the previous system, perhaps demonstrating results on a second dataset to show generality of the approach, etc. A short conference paper just seems to me to be too dense a format for giving this project the description it merits. If it were possible to focus on just one aspect of this system, then that might work, but I do not have good suggestions for exactly how to do that. ----------------If the paper were revised substantially (though I cannot suggest details for how to do this within the appropriate page count), I would consider raising my score. I do think that the effort would be better invested in turning this into a long (and clearer) journal submission.----------------[Addendum: based on discussions here & revisions, I have revised my score]"", 'Summary: --------The paper presents an advanced self-learning model that extracts compositional rules from Bach\'s chorales, which extends their previous work in: 1) the rule hierarchy in both conceptual and informational dimensions; 2) adaptive 2-D memory selection which assumes the features follow Dirichlet Distribution. Sonority (column of 4 MIDI numbers) acts as word in language model: unigram statistics have been used to learn the fundamental rules in music theory, while n-grams with higher order help characterize part writing. Sonorities have been clustered together based on feature functions through iterations. The partition induced by the features is recognized as a rule if it is sufficiently significant. As a result, two sample syllabi with different difficulty strides and ""satisfactory gaps"" have been generated in terms of sets of learned rules. ----------------1. Quality:-------- a) Strengths: In the paper, the exploration of hierarchies in two dimensions makes the learning process more cognitive and interpretable. The authors also demonstrate an effective memory selection to speed up the learning.---------------- b) Flaws: The paper only discussed N<=5, which might limit the learning and interpretation capacities of the proposed model, failing to capture long-distance dependence of music. (In the replies to questions, the authors mentioned they had experimented with max N=10, but I\'m not sure why related results were not included in the paper). Besides the elaborated interpretation of results, a survey seeking the opinions of students in a music department might make the evaluation of system performance more persuasive.----------------2. Clarity:-------- a) Pros: The paper clearly delivers an improved automatic theorist system which learns and represents music concepts as well as thoroughly interprets and compares the learned rules with music theory. Proper analogies and examples help the reader perceive the ideas more easily.---------------- b) Cons: Although detailed definitions can be found in the authors\' previous MUS-ROVER I papers, it would be great if they had described the optimization more clearly (in Figure 1. and related parts).  The ""(Conceptual-Hierarchy Filter)"" row in equations (3): the prime symbol should appear in the subscript.----------------3. Originality:--------The representation of music concepts and rules is still an open area, the paper investigate the topic in a novel way. It illustrates an alternative besides other interpretable feature learning methods such as autoencoders, GAN, etc. ----------------4. Significance:--------It is good to see some corresponding interpretations for the learned rules from music theory. The authors mentioned students in music could and should be involved in the self-learning loop to interact, which is very interesting. I hope their advantages can be combined in the practice of music theory teaching and learning.', ""This paper proposes an interesting framework (as a follow-up work of the author's previous paper) to learn compositional rules used to compose better music. The system consists of two components, a generative component (student) and a discriminative component (teacher). The generative component is a Probabilistic Graphical Models, generating the music following learned rules. The teacher compares the generated music with the empirical distribution of exemplar music (e.g, Bach’s chorales) and propose new rules for the student to learn so that it could improve.----------------The framework is different from GANs that the both the generative and discriminative components are interpretable. From the paper, it seems that the system can indeed learn sensible rules from the composed music and apply them in the next iteration, if trained in a curriculum manner. However, there is no comparison between the proposed system and its previous version, nor comparison between the proposed system and other simple baselines, e.g., an LSTM generative model. This might pose a concern here. ----------------I found this paper a bit hard to read, partly due to (1) lots of music terms (e.g, Tbl. 1 does not make sense to me) that hinders understanding of how the system performs, and (2) over-complicated math symbols and concept. For example, In Page 4, the concept of raw/high-level feature, Feature-Induced Partition and Conceptual Hierarchy, all means a non-overlapping hierarchical clustering on the 4-dimensional feature space. Also, there seems to be no hierarchy in Informational Hierarchy, but a list of rules. It would be much clearer if the authors write the paper in a plain way. ----------------Overall, the paper proposes a working system that seems to be interesting. But I am not confident enough to give strong conclusions.""]",0.82958984375,0.1705322265625,1,0.281494140625,0.71875,0,0.308349609375,0.69140625,0,0.92333984375,0.07666015625,1,0.06793212890625,0.93212890625,0,0.34033203125,0.65966796875,0
97,97,97,97,97,97,https://openreview.net/forum?id=ryxB0Rtxx,"Please provide some intuition..please clarify 


-2- The construction seems fine, but what is special about the resnet here in this construction?.-1- In Eq 3.4  seems the dimensions are not matching q_j in R^k and e_j in R^r.

This paper is well written and studied a fundamental problem in deep neural network..-One of key questions is how to extent the result in this paper to the more general nonlinear actuation function case..(3.1), U \in R ?","['This paper provides some theoretical guarantees for the identity parameterization by showing that 1) arbitrarily deep linear residual networks have no spurious local optima; and 2) residual networks with ReLu activations have universal finite-sample expressivity. This paper is well written and studied a fundamental problem in deep neural network. I am very positive on this paper overall and feel that this result is quite significant by essentially showing the stability of auto-encoder, given the fact that it is hard to provide concrete theoretical guarantees for deep neural networks.----------------One of key questions is how to extent the result in this paper to the more general nonlinear actuation function case. ----------------Minors: one line before Eq. (3.1), U \\in R ? \\times k', ""Paper Summary:----------------Authors investigate identity re-parametrization in the linear and the non linear case. ----------------Detailed comments:----------------— Linear Residual Network:----------------The paper shows that for a linear residual network any critical point is a global optimum. This problem is non convex it is interesting that this simple re-parametrization leads to such a result. ---------------- — Non linear Residual Network:----------------Authors propose a construction that maps the points to their labels via a resnet , using an initial random projection, followed by a residual block that clusters the data based on their label, and a last layer that maps the clusters to the label. ----------------1- In Eq 3.4  seems the dimensions are not matching q_j in R^k and e_j in R^r. please clarify ----------------2- The construction seems fine, but what is special about the resnet here in this construction? One can do a similar construction if we did not have the identity? can you discuss this point?--------In the linear case it is clear from a spectral point of view how the identity is helping the optimization. Please provide some intuition.  ----------------3-   Existence of a network in the residual  class that overfits does it give us any intuition on why residual network outperform other architectures? What does an existence result of such a network tell us about its representation power ? --------A simple linear model under the assumption that points can not be too close can overfit the data, and get fast convergence rate (see for instance tsybakov noise condition).----------------4- What does the construction tell us about the number of layers? ----------------5- clustering the activation independently from the label, is an old way to pretrain the network. One could use those centroids as weights for the next layer (this is also related to Nystrom approximation see for instance https://www.cse.ust.hk/~twinsen/nystrom.pdf ). Your clustering is very strongly connected to the label at each residual block.--------I don't think this is appealing or useful since no feature extraction is happening. Moreover the number of layers in this construction--------does not matter. Can you weaken the clustering to be independent to the label at least in the early layers? then one could you use your construction as an initialization in the training. ----------------— Experiments : ----------------- last layer is not trained means the layer before the linear layer preceding the softmax?----------------Minor comments:----------------Abstract:  how  the identity mapping motivated batch normalization?"", ""This paper investigates the identity parametrization also known as shortcuts where the output of each layer has the form h(x)+x instead of h(x). This has been shown to perform well in practice (eg. ResNet). The discussions and experiments in the paper are interesting. Here's a few comments on the paper:-----------------Section 2: Studying the linear networks is interesting by itself. However, it is not clear that how this could translate to any insight about non-linear networks. For example, you have proved that every critical point is global minimum. I think it is helpful to add some discussion about the relationship between linear and non-linear networks.-----------------Section 3: The construction is interesting but the expressive power of residual network is within a constant factor of general feedforward networks and I don't see why we need a different proof given all the results on finite sample expressivity of feedforward networks. I appreciate if you clarify this.-----------------Section 4: I like the experiments. The choice of random projection on the top layer is brilliant. However, since you have combined this choice with all-convolutional residual networks, it is hard for the reader to separate the affect of each of them. Therefore, I suggest reporting the numbers for all-convolutional residual networks with learned top layer and also ResNet with random projection on the top layer.----------------Minor comments:----------------1- I don't agree that Batch Normalization can be reduced to identity transformation and I don't know if bringing that in the abstract without proper discussion is a good idea.----------------2- Page 5 above assumption 3.1 : x^(i)=1 ==> ||x^(i)||_2=1---------------- ""]",0.436279296875,0.5634765625,0,0.8662109375,0.1336669921875,1,0.08624267578125,0.91357421875,0,0.83935546875,0.160400390625,1,0.05145263671875,0.94873046875,0,0.2098388671875,0.7900390625,0
