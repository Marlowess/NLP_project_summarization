import openai
import argparse
from pathlib import Path
import pandas as pd
import json
import random

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--summaries_a", type=Path, default="", required=True)
    parser.add_argument("--summaries_b", type=Path, default="", required=True)
    parser.add_argument("--model_a", type=Path, default="", required=True)
    parser.add_argument("--model_b", type=Path, default="", required=True)
    args = parser.parse_args()
    return args

def update_dataset_with_json(json_data, df=None):
    """
    Aggiunge i dati di un JSON formattato per la valutazione di riassunti a un dataset esistente,
    oppure crea un nuovo dataset se non esiste.
    
    Args:
        json_data (str): Una stringa JSON valida che contiene i punteggi.
        df (pd.DataFrame, opzionale): Un DataFrame esistente da aggiornare. Se None, viene creato uno nuovo.
        
    Returns:
        pd.DataFrame: Un DataFrame pandas aggiornato con i nuovi dati.
    """
    try:
        new_row = json.loads(json_data)

        new_df = pd.DataFrame([new_row])
        
        if df is None:
            df = new_df
        else:
            df = pd.concat([df, new_df], ignore_index=True)

        return df

    except json.JSONDecodeError as e:
        print("Invalid JSON format:", e)
        return df
    except KeyError as e:
        print("Missing expected key in JSON:", e)
        return df


def get_majority_row(df):
    def majority_or_tie(series):
        mode_values = series.mode()
        return mode_values[0] if len(mode_values) == 1 else 'TIE'
    
    majority_values = {col: majority_or_tie(df[col]) for col in df.columns}
    return pd.DataFrame([majority_values])


def randomize_summaries(summary_a, summary_b, model_a, model_b):
    """
    Randomizza l'ordine di due riassunti e restituisce i riassunti
    con un'etichetta aggiornata (A o B).
    """
    summaries = [(model_a, summary_a), (model_b, summary_b)]
    random.shuffle(summaries)
    randomized_summaries = {
        "summary_1": {"label": summaries[0][0], "text": summaries[0][1]},
        "summary_2": {"label": summaries[1][0], "text": summaries[1][1]}
    }
    return randomized_summaries

def randomize_reviews(reviews):
    random.shuffle(reviews)
    randomized_reviews = reviews
    return randomized_reviews



def evaluate_summary(reviews, generated_summary_a, generated_summary_b, model_a, model_b):
    """
    Evaluate a summary generated by different sources using GPT.
    """
    # randomize order for the prompt to correct position bias
    randomized_reviews = randomize_reviews(reviews)
    review_text = "\n\n".join([f"[Document {i+1}]\n{doc}" for i, doc in enumerate(randomized_reviews)])

    randomized = randomize_summaries(generated_summary_a, generated_summary_b, model_a, model_b)
    
    prompt_pairwise_discriminativeness = f"""
You are an expert in evaluating scientific document summaries. Your task is to evaluate which one of the generated summaries better captures both common and unique ideas of the documents.

Your evaluation should follow a structured Chain of Thought to ensure a logical and consistent assessment.

Chain of Thought for Evaluation
1. Extract Common Ideas: Identify the key ideas that appear in multiple source documents.
2. Compare Summaries on Common Ideas: Determine which summary more accurately and comprehensively represents these shared concepts.
3. Extract Unique Ideas: Identify insights or perspectives that appear in only one source document.
4. Compare Summaries on Unique Ideas: Determine which summary better captures these document-specific details without overemphasizing minor points.
5. Final Decision: Decide which summary is overall superior by counting how many common and unique ideas it captures.

Here are the source documents and the summaries based on the original source documents:

[Source documents]
{review_text}

[Generated summaries]
Summary of model named {randomized['summary_1']['label']}:
{randomized['summary_1']['text']}

Summary of model named {randomized['summary_2']['label']}:
{randomized['summary_2']['text']}

Answer picking:
1. Which summary captures the main common ideas present in the original documents? A strong summary should include all or a good number of major shared concepts while avoiding unnecessary repetition.
2. Which summary capture distinct, document-specific ideas that are not broadly shared across all sources? A strong summary should highlight valuable, unique perspectives from the original documents.
3. Which summary do you consider the best overall?

and then provide your evaluation exclusively in the following JSON format where X is the name of the model you picked:
{{
    "common_ideas": "X",
    "unique_ideas": "X",
    "best_overall": "X"
}}

Ensure the JSON is valid and does not include any additional text or comments.
"""
    
    response = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are an AI assistant."},
            {"role": "user", "content": prompt_pairwise_discriminativeness}
        ],
        max_tokens=500, # todo: manage better this variable
        temperature=0.7
    )

    return response.choices[0].message.content


def main():
    openai.api_key = "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"
    args = parse_args()
    model_a = args.model_a
    model_b = args.model_b
    summaries_a = pd.read_json(args.summaries_a)
    summaries_b = pd.read_json(args.summaries_b)
    summaries_by_documents_df = summaries_a.merge(summaries_b, on=['id'], suffixes=('_a', '_b'))
    evaluation_df = None
    for index, row in summaries_by_documents_df.iterrows():
        reviews = row['reviews_a']
        generated_summary_a = row['summary_a']
        generated_summary_b = row['summary_b']
        majority_df = None
        for i in range(5):
            evaluation = evaluate_summary(reviews, generated_summary_a, generated_summary_b, model_a, model_b)
            majority_df = update_dataset_with_json(evaluation, majority_df)
        majority_row = get_majority_row(majority_df)
        evaluation_df = pd.concat([evaluation_df, majority_row], ignore_index=True)
            
    
    evaluation_df.to_csv("data/evaluation/pairwise_evaluation_dataset.csv", index=False)

if __name__ == "__main__":
    main()
