import openai
import argparse
from pathlib import Path

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--summaries_by_documents", type=Path, default="", required=True)
    parser.add_argument("--eval_type", type=Path, default="", required=True)
    args = parser.parse_args()
    return args

def evaluate_summary(documents, generated_summary, eval_type):
    """
    Evaluate a summary generated by different sources using GPT.
    """
    document_text = "\n\n".join([f"[Document {i+1}]\n{doc}" for i, doc in enumerate(documents)])

    prompt_seahorse_like = f"""
You are an expert in evaluating document summaries. Below, you will receive:
1. Multiple input documents.
2. A generated summary.

Your task is to evaluate the summary based on the following criteria:
- Coverage: Does the summary include all the main points from the documents?
- Coherence: Is the summary well-organized and easy to understand?
- Conciseness: Is the summary brief but comprehensive?
- Faithfulness: Does the summary accurately represent the source documents without adding false information?
- Readability: Is the summary easy to read and grammatically correct?

Please assign a score from 1 to 10 for each criterion and provide a short explanation for each score.

[Documents]
{document_text}

[Generated Summary]
{generated_summary}

Please provide your evaluation in the following format:
Score for Coverage: X/10
Explanation: ...

Score for Coherence: X/10
Explanation: ...

Score for Conciseness: X/10
Explanation: ...

Score for Faithfulness: X/10
Explanation: ...

Score for Readability: X/10
Explanation: ...
"""
    
    prompt_discriminativeness = f"""
You are an expert in evaluating document summaries. Below, you will receive:
1. Multiple input documents.
2. A generated summary.

Your task is to evaluate whether the summary captures both common and unique ideas of the documents.

Please assign a score from 1 to 10 and provide a short explanation for each summary.

[Documents]
{document_text}

[Generated Summary]
{generated_summary}
"""
    
    if (eval_type == "discriminativeness"):
        prompt = prompt_discriminativeness
    else:
        prompt = prompt_seahorse_like
    
    response = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are an AI assistant."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=500, # todo: manage better this variable
        temperature=0.7
    )

    return response.choices[0].message.content

def main():
    openai.api_key = "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"
    args = parse_args()
    summaries_by_documents_df = args.summaries_by_documents
    eval_type = args.eval_type
    for summary_by_docs in summaries_by_documents_df:
        documents = summary_by_docs.documents
        generated_summary = summary_by_docs.summary
        evaluation = evaluate_summary(documents, generated_summary, eval_type)
        print("Evaluation Results:")
        print(evaluation)

if __name__ == "__main__":
    main()
