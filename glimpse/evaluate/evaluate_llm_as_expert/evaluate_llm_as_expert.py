import openai
import argparse
from pathlib import Path
import pandas as pd
import json

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--summaries_by_documents", type=Path, default="", required=True)
    parser.add_argument("--eval_type", type=Path, default="", required=True)
    args = parser.parse_args()
    return args

def update_dataset_with_json(json_data, df=None):
    """
    Aggiunge i dati di un JSON formattato per la valutazione di riassunti a un dataset esistente,
    oppure crea un nuovo dataset se non esiste.
    
    Args:
        json_data (str): Una stringa JSON valida che contiene i punteggi.
        df (pd.DataFrame, opzionale): Un DataFrame esistente da aggiornare. Se None, viene creato uno nuovo.
        
    Returns:
        pd.DataFrame: Un DataFrame pandas aggiornato con i nuovi dati.
    """
    try:
        data = json.loads(json_data)

        # Extract scores from JSON
        new_row = {criterion: details["score"] for criterion, details in data.items()}

        new_df = pd.DataFrame([new_row])
        
        if df is None:
            df = new_df
        else:
            df = pd.concat([df, new_df], ignore_index=True)

        return df

    except json.JSONDecodeError as e:
        print("Invalid JSON format:", e)
        return df
    except KeyError as e:
        print("Missing expected key in JSON:", e)
        return df

def evaluate_summary(reviews, generated_summary, eval_type):
    """
    Evaluate a summary generated by different sources using GPT.
    """
    review_text = "\n\n".join([f"[Document {i+1}]\n{doc}" for i, doc in enumerate(reviews)])

    prompt_seahorse_like = f"""
You are an expert in evaluating scientific document summaries. Below, you will receive:
- Source documents.
- A generated summary based on the original source documents.

Your task is to evaluate the summary based on the following criteria:
- comprehensible: The summary can be read and understood by the rater.
- repetition: The summary is free of unnecessarily repeated information.
- grammar: The summary is grammatically correct.
- main ideas: The summary captures the main idea(s) of the source documents.
- attribution: All the information in the summary is fully attributable to the source documents,
- conciseness: The summary concisely represents the information in the source documents.

Please assign a score for each criterion. The score should be in the range between 0 and 1 with a maximum of two decimal.

Here are the source documents and the summary based on the original source documents:

[Source documents]
{review_text}

[Generated Summary]
{generated_summary}

Provide your evaluation exclusively in the following JSON format where X is the numeric score:
{{
    "comprehensible": X,
    "repetition": X,
    "grammar": X,
    "main_ideas": X,
    "attribution": X,
    "conciseness": X
}}

Ensure the JSON is valid and does not include any additional text or comments.
"""
    
    prompt_discriminativeness = f"""
You are an expert in evaluating scientific document summaries. Below, you will receive:
1. Multiple input documents.
2. A generated summary.

Your task is to evaluate whether the summary captures both common and unique ideas of the documents.

Your evaluation should follow a structured Chain of Thought to ensure a logical and consistent assessment.

Chain of Thought for Evaluation
1. Extract Common Ideas: Identify the key ideas that appear in multiple source documents.
2. Compare Summaries on Common Ideas: Determine which summary more accurately and comprehensively represents these shared concepts.
3. Extract Unique Ideas: Identify insights or perspectives that appear in only one source document.
4. Compare Summaries on Unique Ideas: Determine which summary better captures these document-specific details without overemphasizing minor points.
5. Final Decision: Decide which summary is overall superior by counting how many common and unique ideas it captures.


Assign a score in the range between 0 and 1 with a maximum of two decimal to evaluate if the summary captures common ideas of the documents.
Assign a score in the range between 0 and 1 with a maximum of two decimal to evaluate if the summary captures unique ideas of the documents.

[Documents]
{review_text}

[Generated Summary]
{generated_summary}

Provide your evaluation exclusively in the following JSON format where X is the numeric score:
{{
    "common": X,
    "unique": X
}}

Ensure the JSON is valid and does not include any additional text or comments.
"""
    
    if (eval_type == "discriminativeness"):
        prompt = prompt_discriminativeness
    else:
        prompt = prompt_seahorse_like
    
    response = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are an AI assistant."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=500, # todo: manage better this variable
        temperature=0.7
    )

    return response.choices[0].message.content

def main():
    openai.api_key = "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"
    args = parse_args()
    summaries_by_documents_df = pd.read_json(args.summaries_by_documents)
    eval_type = args.eval_type
    evaluation_df = None
    for index, row in summaries_by_documents_df.iterrows():
        reviews = row['reviews']
        generated_summary = row['summary']
        evaluation = evaluate_summary(reviews, generated_summary, eval_type)
        evaluation_df = update_dataset_with_json(evaluation, evaluation_df)
        print(index + "/" + summaries_by_documents_df.shape[0])
    
    evaluation_df.to_csv("data/evaluation/evaluation_dataset.csv", index=False)

if __name__ == "__main__":
    main()
