import openai
import argparse
from pathlib import Path
import pandas as pd
import json
from dotenv import load_dotenv
import os

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--summaries_by_documents", type=Path, default="", required=True)
    parser.add_argument("--eval_type", type=str, default="discriminativeness", required=True)
    parser.add_argument("--model", type=str, required=True)
    args = parser.parse_args()
    return args

def update_dataset_with_json(json_data, df=None):
    """
    Aggiunge i dati di un JSON formattato per la valutazione di riassunti a un dataset esistente,
    oppure crea un nuovo dataset se non esiste.
    
    Args:
        json_data (str): Una stringa JSON valida che contiene i punteggi.
        df (pd.DataFrame, opzionale): Un DataFrame esistente da aggiornare. Se None, viene creato uno nuovo.
        
    Returns:
        pd.DataFrame: Un DataFrame pandas aggiornato con i nuovi dati.
    """
    try:
        new_row = json.loads(json_data)

        new_df = pd.DataFrame([new_row])
        
        if df is None:
            df = new_df
        else:
            df = pd.concat([df, new_df], ignore_index=True)

        return df

    except json.JSONDecodeError as e:
        print("Invalid JSON format:", e)
        return df
    except KeyError as e:
        print("Missing expected key in JSON:", e)
        return df

def evaluate_summary(reviews, generated_summary, eval_type):
    """
    Evaluate a summary generated by different sources using GPT.
    """
    review_text = "\n\n".join([f"[Document {i+1}]\n{doc}" for i, doc in enumerate(reviews)])

    prompt_seahorse_like = f"""
You are an expert in evaluating scientific document summaries. Below, you will receive:
- Source documents.
- A generated summary based on the original source documents.

Your task is to evaluate the summary based on the following criteria:
- comprehensible: The summary can be read and understood by the rater.
- repetition: The summary is free of unnecessarily repeated information.
- grammar: The summary is grammatically correct.
- main ideas: The summary captures the main idea(s) of the source documents.
- attribution: All the information in the summary is fully attributable to the source documents,
- conciseness: The summary concisely represents the information in the source documents.

Please assign a score for each criterion. The score should be in the range between 0 and 1 with a maximum of two decimal.

Here are the source documents and the summary based on the original source documents:

[Source documents]
{review_text}

[Generated Summary]
{generated_summary}

Provide your evaluation exclusively in the following JSON format where X is the numeric score:
{{
    "comprehensible": X,
    "repetition": X,
    "grammar": X,
    "main_ideas": X,
    "attribution": X,
    "conciseness": X
}}

Ensure the JSON is valid and does not include any additional text or comments.
"""
    
    prompt_discriminativeness = f"""
You are an expert in evaluating scientific document summaries. Below, you will receive:
1. Multiple input documents.
2. A generated summary.

Your task is to evaluate whether the summary captures both common and unique ideas of the documents.

Your evaluation should follow a structured Chain of Thought to ensure a logical and consistent assessment.

Chain of Thought for Evaluation
1. Extract common ideas: read the source documents and identify the key ideas that appear in them.
2. Assign a score to the summary on common ideas: read the summary and give it a score that represents how much accurately and comprehensively it represents these shared concepts.
3. Extract Unique Ideas: read the source documents and identify insights or perspectives that appear in only one source document.
4. Assign a score to the summary on unique ideas: read the summary and give it a score that represents how well it captures these document-specific details.


The scores should be in the range between 0 and 1 with a maximum of two decimal.

[Source documents]
{review_text}

[Generated Summary]
{generated_summary}

Provide your evaluation exclusively in the following JSON format where X is the numeric score:
{{
    "common": X,
    "unique": X
}}

Ensure the JSON is valid and does not include any additional text or comments.
"""
    
    if (eval_type == "discriminativeness"):
        prompt = prompt_discriminativeness
    else:
        prompt = prompt_seahorse_like
    
    response = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are an AI assistant."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=500, # todo: manage better this variable
        temperature=0.7
    )

    return response.choices[0].message.content

def main():
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    api_key_2 = os.getenv("OPENAI_API_KEY_2")
    print('a', api_key, api_key_2)
    openai.api_key = api_key
    args = parse_args()
    summaries_by_documents_df = pd.read_json(args.summaries_by_documents)
    eval_type = args.eval_type
    evaluation_df = None
    for index, row in summaries_by_documents_df.iterrows():
        reviews = row['reviews']
        generated_summary = row['summary']
        evaluation = evaluate_summary(reviews, generated_summary, eval_type)
        evaluation_df = update_dataset_with_json(evaluation, evaluation_df)
        print(f"{index + 1}/{summaries_by_documents_df.shape[0]}")
    
    evaluation_df.to_csv(f"data/evaluation/{args.model}_{args.eval_type}_evaluation_dataset.csv", index=False)

if __name__ == "__main__":
    main()
