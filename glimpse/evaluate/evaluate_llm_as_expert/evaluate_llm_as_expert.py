import openai
import argparse
from pathlib import Path
import pandas as pd
import json

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--summaries_by_documents", type=Path, default="", required=True)
    parser.add_argument("--eval_type", type=Path, default="", required=True)
    args = parser.parse_args()
    return args

def update_dataset_with_json(json_data, df=None):
    """
    Aggiunge i dati di un JSON formattato per la valutazione di riassunti a un dataset esistente,
    oppure crea un nuovo dataset se non esiste.
    
    Args:
        json_data (str): Una stringa JSON valida che contiene i punteggi.
        df (pd.DataFrame, opzionale): Un DataFrame esistente da aggiornare. Se None, viene creato uno nuovo.
        
    Returns:
        pd.DataFrame: Un DataFrame pandas aggiornato con i nuovi dati.
    """
    try:
        data = json.loads(json_data)

        # Extract scores from JSON
        new_row = {criterion: details["score"] for criterion, details in data.items()}

        new_df = pd.DataFrame([new_row])
        
        if df is None:
            df = new_df
        else:
            df = pd.concat([df, new_df], ignore_index=True)

        return df

    except json.JSONDecodeError as e:
        print("Invalid JSON format:", e)
        return df
    except KeyError as e:
        print("Missing expected key in JSON:", e)
        return df

def evaluate_summary(reviews, generated_summary, eval_type):
    """
    Evaluate a summary generated by different sources using GPT.
    """
    review_text = "\n\n".join([f"[Document {i+1}]\n{doc}" for i, doc in enumerate(reviews)])

    prompt_seahorse_like = f"""
You are an expert in evaluating scientific document summaries. Below, you will receive:
1. Multiple input documents.
2. A generated summary.

Your task is to evaluate the summary based on the following criteria:
- Coverage: Does the summary include all the main points from the documents?
- Coherence: Is the summary well-organized and easy to understand?
- Conciseness: Is the summary brief but comprehensive?
- Faithfulness: Does the summary accurately represent the source documents without adding false information?
- Readability: Is the summary easy to read and grammatically correct?

Please assign a score from 1 to 10 for each criterion.

[Documents]
{review_text}

[Generated Summary]
{generated_summary}

Provide your evaluation exclusively in the following JSON format:
{{
    "coverage": {{
        "score": X,
    }},
    "coherence": {{
        "score": X,
    }},
    "conciseness": {{
        "score": X,
    }},
    "faithfulness": {{
        "score": X,
    }},
    "readability": {{
        "score": X,
    }}
}}

Ensure the JSON is valid and does not include any additional text or comments.
"""
    
    prompt_discriminativeness = f"""
You are an expert in evaluating scientific document summaries. Below, you will receive:
1. Multiple input documents.
2. A generated summary.

Your task is to evaluate whether the summary captures both common and unique ideas of the documents.

Assign a score from 1 to 10 to evaluate if the summary captures common ideas of the documents.
Assign a score from 1 to 10 to evaluate if the summary captures unique ideas of the documents.
Assign a score from 1 to 10 for the confidence of your evaluation.

[Documents]
{review_text}

[Generated Summary]
{generated_summary}

Provide your evaluation exclusively in the following JSON format:
{{
    "common": {{
        "score": X,
        "confidence": X,
    }},
    "unique": {{
        "score": X,
        "confidence": X,
    }}
}}

Ensure the JSON is valid and does not include any additional text or comments.
"""
    
    if (eval_type == "discriminativeness"):
        prompt = prompt_discriminativeness
    else:
        prompt = prompt_seahorse_like
    
    response = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are an AI assistant."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=500, # todo: manage better this variable
        temperature=0.7
    )

    return response.choices[0].message.content

def main():
    openai.api_key = "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"
    args = parse_args()
    summaries_by_documents_df = pd.read_json(args.summaries_by_documents)
    eval_type = args.eval_type
    evaluation_df = None
    for index, row in summaries_by_documents_df.iterrows():
        reviews = row['reviews']
        generated_summary = row['summary']
        evaluation = evaluate_summary(reviews, generated_summary, eval_type)
        evaluation_df = update_dataset_with_json(evaluation, evaluation_df)
    
    evaluation_df.to_csv("data/evaluation/evaluation_dataset.csv", index=False)

if __name__ == "__main__":
    main()
